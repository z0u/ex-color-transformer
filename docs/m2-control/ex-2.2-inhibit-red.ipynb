{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191d5190",
   "metadata": {},
   "source": [
    "# Experiment 2.2: Specific concept intervention\n",
    "\n",
    "In the [1.x series](/README.md#m1-preliminary-experiments-with-color) of experiments (milestone 1), we validated our ideas for imposing structure on latent space. With only weak supervision, we guided a simple RGB autoencoder to use the color wheel for its latent representations. In this series, we'll try to inhibit and even delete certain concepts from the model.\n",
    "\n",
    "To start, let's take one of the earlier experiments and see what happens when we suppress activations that align with _red_.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "We've structured the latent space so red is located at $[1,0,0,0]$. If we suppress or redirect activations close to that vector, model performance on near-red colors should drop, while other colors remain mostly unaffected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf0edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup: Logging, Experiment (Modal)\n",
    "from __future__ import annotations\n",
    "import logging\n",
    "\n",
    "import modal\n",
    "\n",
    "from infra.requirements import freeze, project_packages\n",
    "from mini.experiment import Experiment\n",
    "from utils.logging import SimpleLoggingConfig\n",
    "\n",
    "logging_config = (\n",
    "    SimpleLoggingConfig()\n",
    "    .info('notebook', 'utils', 'mini', 'ex_color')\n",
    "    .error('matplotlib.axes')  # Silence warnings about set_aspect\n",
    ")\n",
    "logging_config.apply()\n",
    "\n",
    "# ID for tagging assets\n",
    "nbid = '2.2'\n",
    "# This is the logger for this notebook\n",
    "log = logging.getLogger(f'notebook.{nbid}')\n",
    "experiment_name = f'ex-color-{nbid}'\n",
    "\n",
    "run = Experiment(experiment_name)\n",
    "run.image = modal.Image.debian_slim().pip_install(*freeze(all=True)).add_local_python_source(*project_packages())\n",
    "run.before_each(logging_config.apply)\n",
    "None  # prevent auto-display of this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2028d",
   "metadata": {},
   "source": [
    "## Regularizers\n",
    "\n",
    "Like Ex 1.7:\n",
    "\n",
    "- **Anchor:** pins `red` to $(1,0,0,0)$\n",
    "- **Separate:** angular repulsion to reduce global clumping (applied within each batch)\n",
    "- **Planarity:** pulls vibrant hues to the $[0, 1]$ plane\n",
    "- **Unitarity:** pulls all embeddings to the surface of the unit hypersphere, i.e. it makes the embedding vectors have unit length. There are two terms: one that affects all colors equally, and another that just operates on the vibrant colors (because they seemed to need a little more help)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10c434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from ex_color.loss import Anchor, Separate, Planarity, Unitarity, RegularizerConfig\n",
    "from ex_color.model import ColorMLP\n",
    "from ex_color.training import TrainingModule\n",
    "\n",
    "RED = (1, 0, 0, 0)\n",
    "\n",
    "ALL_REGULARIZERS = [\n",
    "    RegularizerConfig(\n",
    "        name='reg-anchor',\n",
    "        compute_loss_term=Anchor(torch.tensor(RED, dtype=torch.float32)),\n",
    "        label_affinities={'red': 1.0},\n",
    "        layer_affinities=['encoder'],\n",
    "    ),\n",
    "    RegularizerConfig(\n",
    "        name='reg-separate',\n",
    "        compute_loss_term=Separate(power=10.0, shift=False),\n",
    "        label_affinities=None,\n",
    "        layer_affinities=['encoder'],\n",
    "    ),\n",
    "    RegularizerConfig(\n",
    "        name='reg-planar',\n",
    "        compute_loss_term=Planarity(),\n",
    "        label_affinities={'vibrant': 1.0},\n",
    "        layer_affinities=['encoder'],\n",
    "    ),\n",
    "    RegularizerConfig(\n",
    "        name='reg-unit-v',\n",
    "        compute_loss_term=Unitarity(),\n",
    "        label_affinities={'vibrant': 1.0},\n",
    "        layer_affinities=['encoder'],\n",
    "    ),\n",
    "    RegularizerConfig(\n",
    "        name='reg-unit',\n",
    "        compute_loss_term=Unitarity(),\n",
    "        label_affinities=None,\n",
    "        layer_affinities=['encoder'],\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22376072",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data is the same as last time:\n",
    "- Train: an HSV cube (of RGB values)\n",
    "- Test: an RGB cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de07fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "from ex_color.data.color_cube import ColorCube\n",
    "from ex_color.data.cube_sampler import vibrancy\n",
    "from ex_color.data.cyclic import arange_cyclic\n",
    "from ex_color.labelling import collate_with_generated_labels\n",
    "\n",
    "\n",
    "def prep_data() -> tuple[DataLoader, Tensor]:\n",
    "    \"\"\"\n",
    "    Prepare data for training.\n",
    "\n",
    "    Returns: (train, val)\n",
    "    \"\"\"\n",
    "    hsv_cube = ColorCube.from_hsv(\n",
    "        h=arange_cyclic(step_size=10 / 360),\n",
    "        s=np.linspace(0, 1, 10),\n",
    "        v=np.linspace(0, 1, 10),\n",
    "    )\n",
    "    hsv_tensor = torch.tensor(hsv_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "    vibrancy_tensor = torch.tensor(vibrancy(hsv_cube).flatten(), dtype=torch.float32)\n",
    "    hsv_dataset = TensorDataset(hsv_tensor, vibrancy_tensor)\n",
    "\n",
    "    labeller = partial(\n",
    "        collate_with_generated_labels,\n",
    "        soft=False,  # Use binary labels (stochastic) to simulate the labelling of internet text\n",
    "        scale={'red': 0.5, 'vibrant': 0.5},\n",
    "    )\n",
    "    # Desaturated and dark colors are over-represented in the cube, so we use a weighted sampler to balance them out\n",
    "    hsv_loader = DataLoader(\n",
    "        hsv_dataset,\n",
    "        batch_size=64,\n",
    "        num_workers=2,\n",
    "        sampler=WeightedRandomSampler(\n",
    "            weights=hsv_cube.bias.flatten().tolist(),\n",
    "            num_samples=len(hsv_dataset),\n",
    "            replacement=True,\n",
    "        ),\n",
    "        collate_fn=labeller,\n",
    "    )\n",
    "\n",
    "    rgb_cube = ColorCube.from_rgb(\n",
    "        r=np.linspace(0, 1, 8),\n",
    "        g=np.linspace(0, 1, 8),\n",
    "        b=np.linspace(0, 1, 8),\n",
    "    )\n",
    "    rgb_tensor = torch.tensor(rgb_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "    return hsv_loader, rgb_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fecb906",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "_Unlike_ earlier experiments, we've switched over to use PyTorch Lightning instead of our custom training loop. We also tried porting to Catalyst and Ignite, but we found that Lightning was the closest match to the shape that our training code had evolved into.\n",
    "\n",
    "Functionally, not much has changed at this point, but now we should be able to take advantage of things like [Lightning's distributed processing support](https://lightning.ai/docs/pytorch/stable/api_references.html#strategies).\n",
    "\n",
    "We have also switched to using Modal for remote compute, and Weights and Biases for experiment tracking. We also tried running our own Aim experiment tracker instance. It worked, but it was slow. We're not sure why; maybe we just hadn't configured the storage or networking properly. If you're curious, check out the [`aim` tag in the Git history](https://github.com/z0u/ex-color-transformer/tree/aim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c22398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4.6 no.2.2:  Training with: ['reg-anchor', 'reg-separate', 'reg-planar', 'reg-unit-v', 'reg-unit']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4.6 li.fa.ut.se:Seed set to 0\n",
      "I 4.7 ex.se:   PyTorch set to deterministic mode\n",
      "I 4.7 ex.se:   PyTorch set to deterministic mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/vscode/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/vscode/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mz0r\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mz0r\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "INFO: GPU available: False, used: False\n",
      "INFO: GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 5.3 li.py.ut.ra:GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 5.3 li.py.ut.ra:TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 5.3 li.py.ut.ra:HPU available: False, using: 0 HPUs\n",
      "max_steps: 2001, hsv_loader length: 57\n",
      "max_steps: 2001, hsv_loader length: 57\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250826_083236-exb5eboq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/z0r/ex-color-transformer/runs/exb5eboq' target=\"_blank\">ex-color-2.2</a></strong> to <a href='https://wandb.ai/z0r/ex-color-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/z0r/ex-color-transformer' target=\"_blank\">https://wandb.ai/z0r/ex-color-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/z0r/ex-color-transformer/runs/exb5eboq' target=\"_blank\">https://wandb.ai/z0r/ex-color-transformer/runs/exb5eboq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"width: 100%; padding: 5px 0; font-family: monospace\">\n",
       "  <div style=\"position: relative; height: calc(1em * 5/3); width: 100%; margin-bottom: 2em\">\n",
       "    <div style=\"position: absolute; top: 100%; height: 3.5px; left: 0; width: 100%; background: linear-gradient(to right, var(--h1) 0.0%, var(--h1) 2.0%, var(--h1) 4.1%, var(--h0) 6.1%, var(--h1) 8.2%, var(--h1) 10.2%, var(--h0) 12.2%, var(--h1) 14.3%, var(--h1) 16.3%, var(--h1) 18.4%, var(--h0) 20.4%, var(--h1) 22.4%, var(--h1) 24.5%, var(--h0) 26.5%, var(--h1) 28.6%, var(--h1) 30.6%, var(--h0) 32.7%, var(--h1) 34.7%, var(--h1) 36.7%, var(--h1) 38.8%, var(--h0) 40.8%, var(--h1) 42.9%, var(--h1) 44.9%, var(--h0) 46.9%, var(--h1) 49.0%, var(--h1) 51.0%, var(--h0) 53.1%, var(--h1) 55.1%, var(--h1) 57.1%, var(--h1) 59.2%, var(--h0) 61.2%, var(--h1) 63.3%, var(--h1) 65.3%, var(--h0) 67.3%, var(--h1) 69.4%, var(--h1) 71.4%, var(--h0) 73.5%, var(--h1) 75.5%, var(--h1) 77.6%, var(--h1) 79.6%, var(--h0) 81.6%, var(--h1) 83.7%, var(--h1) 85.7%, var(--h0) 87.8%, var(--h1) 89.8%, var(--h1) 91.8%, var(--h1) 93.9%, var(--h0) 95.9%, var(--h1) 98.0%, var(--h2) 100.0%);--h0:color(from currentColor srgb r g b / 0.00);--h1:color(from currentColor srgb r g b / 0.11);--h2:color(from currentColor srgb r g b / 0.44)\"></div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 2.8%; border-left: 0.5px solid currentColor\">0.0512</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 14.2%; border-left: 0.5px solid currentColor\">0.0278</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 25.6%; border-left: 0.5px solid currentColor\">0.0282</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 37.0%; border-left: 0.5px solid currentColor\">0.0142</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 48.4%; border-left: 0.5px solid currentColor\">0.0058</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 59.8%; border-left: 0.5px solid currentColor\">0.0123</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 71.2%; border-left: 0.5px solid currentColor\">0.0082</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 82.6%; border-left: 0.5px solid currentColor\">0.0081</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; right: 0.0%; border-right: 0.5px solid currentColor\">0.0129</div>\n",
       "    <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px)\">\n",
       "      <div style=\"border-left: 4px solid transparent; border-right: 4px solid transparent; border-bottom: 4px solid currentColor\"></div>\n",
       "    </div>\n",
       "    <div style=\"position: absolute; height: 100%; width: 100.0%; background-color: color(from currentColor srgb r g b / 0.1); border-bottom: 1px solid currentColor\"></div>\n",
       "    <div style=\"position: absolute; width: 100%; height: 100%; text-align: center; line-height: calc((1em * 5/3) / 0.9); font-size: 90%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5)\"><b>Training</b>: 100.0% [2001/2001] [<b>00:11</b>/<00:00, 177.48 it/s]</div>\n",
       "  </div>\n",
       "  <div style=\"display: grid; grid-template-columns: repeat(2, minmax(80px, 1fr)); gap: 5px 0px; width: 100%; margin: 1em 0; font-size: 0.85em\">\n",
       "    <div style=\"font-weight: bold; border-bottom: 0.5px solid currentColor; padding: 2px 10px; text-align: left; overflow: hidden; text-overflow: ellipsis; white-space: nowrap\">v_num</div>\n",
       "    <div style=\"font-weight: bold; border-bottom: 0.5px solid currentColor; padding: 2px 10px; text-align: left; overflow: hidden; text-overflow: ellipsis; white-space: nowrap\">train_loss</div>\n",
       "    <div style=\"padding: 2px 10px; text-align: left; overflow: hidden; text-overflow: ellipsis; white-space: nowrap\">eboq</div>\n",
       "    <div style=\"padding: 2px 10px; text-align: left; overflow: hidden; text-overflow: ellipsis; white-space: nowrap\">0.01293</div>\n",
       "  </div>\n",
       "</div>"
      ],
      "text/plain": [
       "Training: 100.0% [2001/2001]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting phase: Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_steps=2001` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 18.0 li.py.ut.ra:`Trainer.fit` stopped: `max_steps=2001` reached.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from ex_color.inference import InferenceModule\n",
    "from ex_color.intervention.intervention import InterventionConfig\n",
    "\n",
    "wandb_api_key = wandb.Api().api_key\n",
    "\n",
    "\n",
    "# @run.thither\n",
    "async def train(\n",
    "    dopesheet: Dopesheet,\n",
    "    regularizers: list[RegularizerConfig],\n",
    ") -> ColorMLP:\n",
    "    \"\"\"Train the model with the given dopesheet and variant.\"\"\"\n",
    "    import lightning as L\n",
    "    from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "    from ex_color.seed import set_deterministic_mode\n",
    "\n",
    "    from utils.progress.lightning import LightningProgress\n",
    "\n",
    "    log.info(f'Training with: {[r.name for r in regularizers]}')\n",
    "\n",
    "    seed = 0\n",
    "    set_deterministic_mode(seed)\n",
    "\n",
    "    hsv_loader, _ = prep_data()\n",
    "\n",
    "    model = ColorMLP(4)\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log.debug(f'Model initialized with {total_params:,} trainable parameters.')\n",
    "\n",
    "    training_module = TrainingModule(model, dopesheet, torch.nn.MSELoss(), regularizers)\n",
    "\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    logger = WandbLogger(experiment_name, project='ex-color-transformer')\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_steps=len(dopesheet),\n",
    "        callbacks=[\n",
    "            LightningProgress(),\n",
    "        ],\n",
    "        enable_checkpointing=False,\n",
    "        enable_model_summary=False,\n",
    "        # enable_progress_bar=True,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    print(f'max_steps: {len(dopesheet)}, hsv_loader length: {len(hsv_loader)}')\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(training_module, hsv_loader)\n",
    "    # This is only a small model, so it's OK to return it rather than storing and loading a checkpoint remotely\n",
    "    return model\n",
    "\n",
    "\n",
    "async with run():\n",
    "    model = await train(Dopesheet.from_csv(f'./ex-{nbid}-dopesheet.csv'), ALL_REGULARIZERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9688ac3",
   "metadata": {},
   "source": [
    "## Inference ('test-time')\n",
    "\n",
    "We wrap the model that we trained above in an `InferenceModule`, which knows how to apply our interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d2de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @run.thither\n",
    "async def infer(\n",
    "    model: ColorMLP,\n",
    "    interventions: list[InterventionConfig],\n",
    "    test_data: Tensor,\n",
    ") -> Tensor:\n",
    "    \"\"\"Run inference with the given model and interventions.\"\"\"\n",
    "    import lightning as L\n",
    "\n",
    "    inference_module = InferenceModule(model, interventions)\n",
    "    trainer = L.Trainer(\n",
    "        enable_checkpointing=False,\n",
    "        enable_model_summary=False,\n",
    "        enable_progress_bar=True,\n",
    "    )\n",
    "    reconstructed_colors_batches = trainer.predict(\n",
    "        inference_module,\n",
    "        DataLoader(\n",
    "            TensorDataset(test_data.reshape((-1, 3))),\n",
    "            batch_size=64,\n",
    "            collate_fn=lambda batch: torch.stack([row[0] for row in batch], 0),\n",
    "        ),\n",
    "    )\n",
    "    assert reconstructed_colors_batches is not None\n",
    "    # Flatten the list of batches to a single list of tensors\n",
    "    reconstructed_colors = [item for batch in reconstructed_colors_batches for item in batch]\n",
    "    # Reshape to match input\n",
    "    return torch.cat(reconstructed_colors).reshape(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6940577",
   "metadata": {},
   "source": [
    "Let's see how well the model reconstructs colors _without_ any interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4381b363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ex-2.2-true-colors.png?v=-xtmZY4G6SCtq-W2s-r9_HNgBnW8TCMG2iaT5eW5Qtg\" alt=\"\" style=\"max-width: 100%;\" />"
      ],
      "text/plain": [
       "Image"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ex_color.vis import plot_colors\n",
    "from utils.nb import displayer_img\n",
    "from utils.plt import configure_matplotlib\n",
    "\n",
    "\n",
    "configure_matplotlib()\n",
    "\n",
    "\n",
    "hsv_cube = ColorCube.from_hsv(\n",
    "    h=arange_cyclic(step_size=1 / 24),\n",
    "    s=np.linspace(0, 1, 4),\n",
    "    v=np.linspace(0, 1, 8),\n",
    ").permute('svh')\n",
    "x_hsv = torch.tensor(hsv_cube.rgb_grid, dtype=torch.float32)\n",
    "clear_output()\n",
    "with displayer_img(f'ex-{nbid}-true-colors.png') as show:\n",
    "    fig = plot_colors(hsv_cube, title='True colors', colors=x_hsv.numpy())\n",
    "    show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14da6f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ex-2.2-pred-colors-no-intervention.png?v=dwjeGkkGyKilR56GOqIRQWEM5I_CS7VWcbpeakq0ifM\" alt=\"\" style=\"max-width: 100%;\" />"
      ],
      "text/plain": [
       "Image"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "y_hsv = await infer(model, [], x_hsv)\n",
    "clear_output()\n",
    "with displayer_img(f'ex-{nbid}-pred-colors-no-intervention.png') as show:\n",
    "    fig = plot_colors(\n",
    "        hsv_cube,\n",
    "        title='Predicted colors without intervention',\n",
    "        colors=y_hsv.numpy(),\n",
    "        colors_compare=x_hsv.numpy(),\n",
    "    )\n",
    "    show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c6e78",
   "metadata": {},
   "source": [
    "That's pretty good: as expected, the reconstructed colors (predictions) _look_ almost the same as the true colors. Visually, the main differences I can see are:\n",
    "\n",
    "- Fully saturated colors ($s=1$) show some bleeding of green, red, and hot pink into neighboring hues\n",
    "- Fully desaturated colors ($s=0$) show some hint of being slightly off-gray, i.e. some saturation has crept in.\n",
    "\n",
    "Sense-check: let's look at the latent space too. We'll use an RGB cube as input for this instead of the HSV cube used above, because it gives a more regular distribution of points — which will be useful to see whether the intervention changes the point density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ec7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture encoder activations (latents) without interventions\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from ex_color.inference import InferenceModule\n",
    "\n",
    "\n",
    "# Build a tiny helper that runs predict while capturing latents from 'encoder'\n",
    "async def infer_with_latent_capture(\n",
    "    model: ColorMLP, interventions: list[InterventionConfig], test_data: Tensor, layer_name: str = 'encoder'\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    module = InferenceModule(model, interventions, capture_layers=[layer_name])\n",
    "    import lightning as L\n",
    "\n",
    "    trainer = L.Trainer(enable_checkpointing=False, enable_model_summary=False, enable_progress_bar=False)\n",
    "    batches = trainer.predict(\n",
    "        module,\n",
    "        DataLoader(\n",
    "            TensorDataset(test_data.reshape((-1, 3))),\n",
    "            batch_size=64,\n",
    "            collate_fn=lambda batch: torch.stack([row[0] for row in batch], 0),\n",
    "        ),\n",
    "    )\n",
    "    assert batches is not None\n",
    "    preds = [item for batch in batches for item in batch]\n",
    "    y = torch.cat(preds).reshape(test_data.shape)\n",
    "    # Read captured activations as a flat [N, D] tensor\n",
    "    latents = module.read_captured(layer_name)\n",
    "    return y, latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b4aa8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ex-2.2-latents-no-intervention.png?v=ZZ-RaMBMzS2dCQTaoqseCp6_gyc99q8ubmU1pRl_mJ4\" alt=\"\" style=\"max-width: 100%;\" />"
      ],
      "text/plain": [
       "Image"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting utilities for latent slices (axis-aligned 2D with depth ordering)\n",
    "from typing import cast, Sequence\n",
    "\n",
    "from matplotlib.typing import ColorType\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import torch\n",
    "\n",
    "from ex_color.data.color_cube import ColorCube\n",
    "from utils.plt import hide_decorations\n",
    "\n",
    "\n",
    "from typing import cast, Sequence\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.typing import ColorType\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection, Poly3DCollection\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import torch\n",
    "\n",
    "from ex_color.data.color_cube import ColorCube\n",
    "from utils.plt import hide_decorations\n",
    "\n",
    "\n",
    "def draw_latent_3d(\n",
    "    ax: Axes3D,\n",
    "    latents_3d: np.ndarray,\n",
    "    colors_rgb: np.ndarray,\n",
    "    colors_edge_rgb: np.ndarray | None = None,\n",
    "    *,\n",
    "    dot_size: float = 10.0,\n",
    "    alpha: float = 1.0,\n",
    "):\n",
    "    if colors_edge_rgb is None:\n",
    "        colors_edge_rgb = colors_rgb\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        latents_3d[:, 0],\n",
    "        latents_3d[:, 1],\n",
    "        latents_3d[:, 2],  # type: ignore\n",
    "        c=colors_rgb,\n",
    "        edgecolors=cast(Sequence[ColorType], colors_edge_rgb),\n",
    "        linewidths=0.01 * dot_size,\n",
    "        s=dot_size,  # type: ignore\n",
    "        alpha=alpha,\n",
    "    )\n",
    "\n",
    "    # Set equal aspect ratio for all axes\n",
    "    max_range = np.abs(latents_3d).max()\n",
    "    ax.set_xlim([-max_range * 1.1, max_range * 1.1])\n",
    "    ax.set_ylim([-max_range * 1.1, max_range * 1.1])\n",
    "    ax.set_zlim([-max_range * 1.1, max_range * 1.1])\n",
    "\n",
    "    return {'scatter': scatter}\n",
    "\n",
    "\n",
    "def draw_circle_3d(\n",
    "    ax: Axes3D,\n",
    "    r=1,\n",
    "    verts=100,\n",
    "    color: ColorType | None = None,\n",
    "    alpha: float = 1.0,\n",
    "    zorder: float | None = None,\n",
    "):\n",
    "    theta = np.linspace(0, 2 * np.pi, verts)\n",
    "    x = r * np.cos(theta)\n",
    "    y = r * np.sin(theta)\n",
    "    z = np.full_like(x, 0)\n",
    "\n",
    "    # Create vertices for the filled circle\n",
    "    circle_verts = [list(zip(x, y, z, strict=True))]\n",
    "    poly = Poly3DCollection(circle_verts, facecolors=color, alpha=alpha, zorder=-10)\n",
    "    ax.add_collection3d(poly)\n",
    "    return poly\n",
    "\n",
    "\n",
    "def _edge_index_chains(shape: tuple[int, int, int]) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute index chains for each cube edge at full resolution.\n",
    "    (This function stays the same - it's just computing connectivity)\n",
    "    \"\"\"\n",
    "    edges: list[np.ndarray] = []\n",
    "    dims = tuple(int(x) for x in shape)\n",
    "    for k in range(3):\n",
    "        fixed_axes = [a for a in (0, 1, 2) if a != k]\n",
    "        for f0 in (0, dims[fixed_axes[0]] - 1):\n",
    "            for f1 in (0, dims[fixed_axes[1]] - 1):\n",
    "                n_k = dims[k]\n",
    "                multi = np.zeros((n_k, 3), dtype=int)\n",
    "                coords = [0, 0, 0]\n",
    "                coords[fixed_axes[0]] = f0\n",
    "                coords[fixed_axes[1]] = f1\n",
    "                for t in range(n_k):\n",
    "                    coords[k] = t\n",
    "                    multi[t] = coords\n",
    "                idx_chain = cast(NDArray, np.ravel_multi_index(multi.T, dims))\n",
    "                edges.append(idx_chain)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def _draw_edge_mesh_3d(\n",
    "    ax: Axes3D,\n",
    "    latents_3d: np.ndarray,\n",
    "    point_colors: np.ndarray,\n",
    "    edge_index_chains: list[np.ndarray],\n",
    "    *,\n",
    "    alpha: float = 0.95,\n",
    "    linewidth: float = 0.6,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw 3D edge mesh. Much simpler - no manual depth sorting!\n",
    "    Axes3D handles all the depth complexity for us.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    colors = []\n",
    "\n",
    "    for chain in edge_index_chains:\n",
    "        if len(chain) < 2:\n",
    "            continue\n",
    "\n",
    "        pts_3d = latents_3d[chain]  # (n, 3)\n",
    "        cols = point_colors[chain]  # (n, 3)\n",
    "\n",
    "        # Create line segments between consecutive points\n",
    "        for i in range(len(pts_3d) - 1):\n",
    "            segments.append([pts_3d[i], pts_3d[i + 1]])\n",
    "            # Average color between the two endpoints\n",
    "            colors.append(0.5 * (cols[i] + cols[i + 1]))\n",
    "\n",
    "    if segments:\n",
    "        lc = Line3DCollection(segments, colors=colors, linewidths=linewidth, alpha=alpha)\n",
    "        ax.add_collection3d(lc)\n",
    "\n",
    "\n",
    "def plot_latent_grid_3d(\n",
    "    cube: ColorCube,\n",
    "    latents: torch.Tensor,\n",
    "    colors: torch.Tensor,\n",
    "    colors_edge: torch.Tensor | None = None,\n",
    "    *,\n",
    "    dims: list[tuple[int, int, int]],\n",
    "    title: str | None = None,\n",
    "    figsize_per_plot: tuple[float, float] = (6, 6),\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot 4D+ latent data as true 3D visualizations.\n",
    "\n",
    "    Args:\n",
    "        latents: Shape [..., D] where D >= 3\n",
    "        dims: List of (i, j, k) triplets specifying which latent dimensions to use for x, y, z\n",
    "    \"\"\"\n",
    "    lat_np = latents.detach().cpu().numpy()\n",
    "    col_np = colors.detach().cpu().reshape(-1, colors.shape[-1]).numpy()\n",
    "    col_edge_np = (\n",
    "        colors_edge.detach().cpu().reshape(-1, colors.shape[-1]).numpy() if colors_edge is not None else col_np\n",
    "    )\n",
    "\n",
    "    n = len(dims)\n",
    "    cols = min(3, n)  # Max 3 columns\n",
    "    rows = int(np.ceil(n / cols))\n",
    "\n",
    "    # Precompute edge connectivity\n",
    "    assert len(cube.shape) == 3\n",
    "    edge_chains = _edge_index_chains(cube.shape)\n",
    "\n",
    "    fig = plt.figure(figsize=(figsize_per_plot[0] * cols, figsize_per_plot[1] * rows))\n",
    "\n",
    "    for idx, (i, j, k) in enumerate(dims):\n",
    "        # Create 3D subplot\n",
    "        ax = cast(Axes3D, fig.add_subplot(rows, cols, idx + 1, axes_class=Axes3D))\n",
    "\n",
    "        # Extract 3D coordinates\n",
    "        lat_3d = lat_np[:, [i, j, k]]\n",
    "\n",
    "        draw_circle_3d(ax, color='#111')\n",
    "\n",
    "        # Draw edges and points (automatically depth-sorted by Axes3D)\n",
    "        _draw_edge_mesh_3d(ax, lat_3d, col_edge_np, edge_chains, alpha=0.7, linewidth=2.0)\n",
    "        draw_latent_3d(ax, lat_3d, col_np, colors_edge_rgb=col_edge_np, alpha=1.0, dot_size=10)\n",
    "\n",
    "        # Clean up the 3D axes\n",
    "        hide_decorations(ax)\n",
    "        ax.set_title(f'Latent dims ({i},{j},{k})')\n",
    "        ax.set_xlim(-1.2, 1.2)\n",
    "        ax.set_ylim(-1.2, 1.2)\n",
    "        ax.view_init(elev=90, azim=-90)\n",
    "        ax.set_proj_type('ortho')\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Visualize latents without interventions\n",
    "from utils.nb import displayer_img\n",
    "\n",
    "\n",
    "rgb_cube = ColorCube.from_rgb(\n",
    "    r=np.linspace(0, 1, 20),\n",
    "    g=np.linspace(0, 1, 20),\n",
    "    b=np.linspace(0, 1, 20),\n",
    ")\n",
    "x_rgb = torch.tensor(rgb_cube.rgb_grid, dtype=torch.float32)\n",
    "\n",
    "y_rgb, h_rgb = await infer_with_latent_capture(model, [], x_rgb, 'encoder')\n",
    "clear_output()\n",
    "with displayer_img(f'ex-{nbid}-latents-no-intervention.png') as show:\n",
    "    fig = plot_latent_grid_3d(\n",
    "        rgb_cube, h_rgb, y_rgb, x_rgb, title='Latents · no intervention', dims=[(1, 0, 2), (1, 2, 0), (2, 3, 0)]\n",
    "    )\n",
    "    show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68adcf20",
   "metadata": {},
   "source": [
    "The looks reasonable: similar to Ex 1.7, the latent space shows a color wheel in the first two axes with red near the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4f1dc",
   "metadata": {},
   "source": [
    "## Suppression\n",
    "\n",
    "Now that we have our model, let's try suppressing _red_. We'll use the `Suppression` function developed in [Ex 2.1](./ex-2.1-intervention-lobe.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b8cecb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ex-2.2-pred-colors-suppression.png?v=RQuJXCjfrvfEiFkOzg7C77Hv9s0AtPcT35tuv_tq16s\" alt=\"\" style=\"max-width: 100%;\" />"
      ],
      "text/plain": [
       "Image"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "from math import cos, pi\n",
    "\n",
    "import torch\n",
    "\n",
    "import ex_color.intervention\n",
    "import ex_color.intervention.bounded_falloff\n",
    "\n",
    "importlib.reload(ex_color.intervention.bounded_falloff)\n",
    "importlib.reload(ex_color.intervention)\n",
    "from ex_color.intervention import BoundedFalloff, InterventionConfig, Suppression\n",
    "\n",
    "\n",
    "suppression = InterventionConfig(\n",
    "    Suppression(\n",
    "        torch.tensor(RED, dtype=torch.float32),  # Constant!\n",
    "        BoundedFalloff(\n",
    "            0,  # within 60°\n",
    "            1,  # completely squash fully-aligned vectors\n",
    "            2,  # soft rim, sharp hub\n",
    "        ),\n",
    "    ),\n",
    "    ['encoder'],  # output of encoder is the bottleneck\n",
    ")\n",
    "\n",
    "y_hsv = await infer(model, [suppression], x_hsv)\n",
    "clear_output()\n",
    "with displayer_img(f'ex-{nbid}-pred-colors-suppression.png') as show:\n",
    "    fig = plot_colors(\n",
    "        hsv_cube,\n",
    "        title='Predicted colors with suppression',\n",
    "        colors=y_hsv.numpy(),\n",
    "        colors_compare=x_hsv.numpy(),\n",
    "    )\n",
    "    show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec409f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ex-2.2-latents-suppression.png?v=4QkKUoVHEnMBFZAUZhsp08RwGKf8AmXtXTgB50BGXjs\" alt=\"\" style=\"max-width: 100%;\" />"
      ],
      "text/plain": [
       "Image"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Capture latents with suppression\n",
    "y_rgb, h_rgb = await infer_with_latent_capture(model, [suppression], x_rgb, 'encoder')\n",
    "clear_output()\n",
    "with displayer_img(f'ex-{nbid}-latents-suppression.png') as show:\n",
    "    fig = plot_latent_grid_3d(\n",
    "        rgb_cube, h_rgb, y_rgb, x_rgb, title='Latents · suppression', dims=[(1, 0, 2), (1, 2, 0), (2, 3, 0)]\n",
    "    )\n",
    "    show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8eb019",
   "metadata": {},
   "source": [
    "## Repulsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ex-2.2-pred-colors-repulsion.png?v=5oElZcrYUspoYsLy9Paf3UCylHuFYeEbe1_tpeUVSfY\" alt=\"\" style=\"max-width: 100%;\" />"
      ],
      "text/plain": [
       "Image"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from math import cos, pi\n",
    "\n",
    "import torch\n",
    "\n",
    "from ex_color.intervention import FastBezierMapper, InterventionConfig, Repulsion\n",
    "\n",
    "repulsion = InterventionConfig(\n",
    "    Repulsion(\n",
    "        torch.tensor([1, 0, 0, 0], dtype=torch.float32),  # Constant!\n",
    "        FastBezierMapper(\n",
    "            0,  # Constrain effect to within 60° cone\n",
    "            cos(pi / 3),  # Create 30° hole (negative cone) around concept vector\n",
    "        ),\n",
    "    ),\n",
    "    ['encoder'],\n",
    ")\n",
    "\n",
    "y_hsv = await infer(model, [repulsion], x_hsv)\n",
    "clear_output()\n",
    "with displayer_img(f'ex-{nbid}-pred-colors-repulsion.png') as show:\n",
    "    fig = plot_colors(\n",
    "        hsv_cube,\n",
    "        title='Predicted colors with repulsion',\n",
    "        colors=y_hsv.numpy(),\n",
    "        colors_compare=x_hsv.numpy(),\n",
    "    )\n",
    "    show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "083b6bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ex-2.2-latents-repulsion.png?v=gWnGn0iipnqLPmTnFz1_OdxHBnZQ9mi5S94vZ9e7DpA\" alt=\"\" style=\"max-width: 100%;\" />"
      ],
      "text/plain": [
       "Image"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Capture latents with repulsion\n",
    "y_rgb, h_rgb = await infer_with_latent_capture(model, [repulsion], x_rgb, 'encoder')\n",
    "clear_output()\n",
    "with displayer_img(f'ex-{nbid}-latents-repulsion.png') as show:\n",
    "    fig = plot_latent_grid_3d(\n",
    "        rgb_cube, h_rgb, y_rgb, x_rgb, title='Latents · repulsion', dims=[(1, 0, 2), (1, 2, 0), (2, 3, 0)]\n",
    "    )\n",
    "    show(fig)\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex-color-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
