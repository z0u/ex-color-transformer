{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8817a226",
   "metadata": {},
   "source": [
    "# Experiment 2.1: Intervention lobes\n",
    "\n",
    "In this series of experiments, we shall explore the effects of intervening on latent activations. Having structured the latent space (see Ex 1.x), it should just be a matter of transforming latent embeddings that are closely aligned to the anchored concepts.\n",
    "\n",
    "We draw inspiration from shaders in computer graphics: BSDFs compute the output energy given: 1. an input light direction, and 2. the viewing direction, relative to the surface normal. Our interventions are similar: we have 1. a concept vector, and 2. activation vectors. If we treat the subject vector as analogous to a light source and acivation vectors as analogous to viewing directions, we may build on a wealth of established techniques.\n",
    "\n",
    "Here we define our intervention as a BSDF-like function:\n",
    "\n",
    "$$\\alpha' = f(\\mathbf{v},\\alpha)$$\n",
    "\n",
    "Where $\\alpha$ is an embedding vector, $\\mathbf{v}$ is the concept vector, and $\\alpha'$ is the modified embedding. In fact $\\mathbf{v}$ need not be a (directional) vector; it could be other geometric features of our embedding space, such as a subspace defined by multiple basis vectors. But for this experiment, we will limit ourselves to intervention on directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e501f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_id = '2.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819dbdf6",
   "metadata": {},
   "source": [
    "## Charting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab9aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol, Sequence\n",
    "\n",
    "import torch\n",
    "from numpy.typing import NDArray\n",
    "from torch import Tensor\n",
    "\n",
    "from ex_color.intervention.intervention import ConstAnnotation, Intervention\n",
    "\n",
    "\n",
    "def sample_idf(\n",
    "    idf: Intervention, n=360, *, eps=0.0, include_end=False\n",
    ") -> tuple[NDArray, NDArray, NDArray, tuple[str, NDArray]]:\n",
    "    # Input angles θ_in: [0, 2π)\n",
    "    thetas_in: Tensor = torch.linspace(eps, 2 * torch.pi - eps, steps=n + 1, dtype=torch.float32)\n",
    "    if not include_end:\n",
    "        thetas_in = thetas_in[:-1]\n",
    "\n",
    "    # Unit circle directions, shape [n, 2]\n",
    "    unit: Tensor = torch.stack((torch.cos(thetas_in), torch.sin(thetas_in)), dim=-1)\n",
    "\n",
    "    # Apply intervention (idf expects Tensors); disable grad for safety\n",
    "    with torch.no_grad():\n",
    "        out: Tensor = idf(unit)  # [n, 2]\n",
    "        annotation = idf.annotate_activations(unit)  # [n]\n",
    "\n",
    "    # Convert outputs to polar coordinates\n",
    "    y, x = out[..., 1], out[..., 0]\n",
    "    theta_out = torch.atan2(y, x)  # [-π, π]\n",
    "    # Wrap angles to be positive\n",
    "    theta_out = (theta_out + 2 * torch.pi) % (2 * torch.pi)  # [0, 2π]\n",
    "    r_out = torch.linalg.norm(out, dim=-1)\n",
    "\n",
    "    return (\n",
    "        theta_out.detach().cpu().numpy(),\n",
    "        r_out.detach().cpu().numpy(),\n",
    "        thetas_in.detach().cpu().numpy(),\n",
    "        (annotation.name, annotation.values.detach().cpu().numpy()),\n",
    "    )\n",
    "\n",
    "\n",
    "class Mapper(Protocol):\n",
    "    def __call__(self, alignment: Tensor) -> Tensor: ...\n",
    "\n",
    "    @property\n",
    "    def annotations(self) -> Sequence[ConstAnnotation]: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d72d23",
   "metadata": {},
   "source": [
    "### Special chart series (artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a4cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from numpy.typing import NDArray\n",
    "from torch import Tensor\n",
    "\n",
    "from ex_color.intervention.intervention import Intervention\n",
    "\n",
    "from typing import Literal\n",
    "from matplotlib.patheffects import SimpleLineShadow, Normal\n",
    "\n",
    "\n",
    "def wrapped_angular_diff(a: float, b: float) -> float:\n",
    "    \"\"\"Compute the angular difference between two angles a and b, wrapping around at 2π.\"\"\"\n",
    "    # Ensure 0 is considered close to 2pi\n",
    "    diff = (b - a) % (2 * np.pi)\n",
    "    return min(diff, 2 * np.pi - diff)\n",
    "\n",
    "\n",
    "def filled_series(\n",
    "    ax: Axes,\n",
    "    xs: NDArray,\n",
    "    ys: NDArray,\n",
    "    *,\n",
    "    color: str | None,\n",
    "    alpha=0.3,\n",
    "    close: Literal['auto', 'always'] = 'auto',\n",
    "    **kwargs,\n",
    "):\n",
    "    span_x = wrapped_angular_diff(xs[0], xs[-1])\n",
    "    _close = close == 'always' or isinstance(ax, PolarAxes) and span_x < radians(2)\n",
    "\n",
    "    ax.fill(\n",
    "        np.concatenate([xs, [xs[0]]]) if _close else np.concatenate([[0], xs, [0]]),\n",
    "        np.concatenate([ys, [ys[0]]]) if _close else np.concatenate([[0], ys, [0]]),\n",
    "        color=color,\n",
    "        alpha=alpha,\n",
    "        zorder=0,\n",
    "    )\n",
    "    ax.plot(\n",
    "        np.concatenate([xs, [xs[0]]]) if _close else xs,\n",
    "        np.concatenate([ys, [ys[0]]]) if _close else ys,\n",
    "        color=color,\n",
    "        path_effects=[\n",
    "            SimpleLineShadow((0, 0), linewidth=3, alpha=0.1),\n",
    "            SimpleLineShadow((0, 0), linewidth=6, alpha=0.05),\n",
    "            SimpleLineShadow((0, 0), linewidth=9, alpha=0.025),\n",
    "            Normal(),\n",
    "        ],\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "Shape = Literal['line', 'chord']\n",
    "\n",
    "\n",
    "def diff_series(\n",
    "    ax: Axes,\n",
    "    xs1: NDArray,\n",
    "    xs2: NDArray,\n",
    "    ys1: NDArray,\n",
    "    ys2: NDArray,\n",
    "    *,\n",
    "    shape: Shape,\n",
    "    label: str | None = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Draw line segments between two series of points (xs1, ys1) and (xs2, ys2).\"\"\"\n",
    "    # Split kwargs\n",
    "    marker_kwargs = {k: v for k, v in kwargs.items() if k.startswith('marker')}\n",
    "    other_kwargs = {k: v for k, v in kwargs.items() if not k.startswith('marker')}\n",
    "\n",
    "    # Draw line segments between series 1 and 2\n",
    "    for x1, x2, y1, y2 in zip(xs1, xs2, ys1, ys2, strict=True):\n",
    "        if np.abs(x2 - x1) > np.pi:\n",
    "            # Take the shortest path around the circle\n",
    "            x1 += 2 * np.pi\n",
    "        if shape == 'chord':\n",
    "            # Draw a curve, like a chord diagram, to make it easier to see where the points go\n",
    "            # Without this, rotations are hard to interpret because the lines have similar angles\n",
    "            curve_length_x = wrapped_angular_diff(x1, x2)\n",
    "            curve_power = 2.2\n",
    "            curve_strength = 0.97 * (curve_length_x / np.pi) + 0.03\n",
    "            xs = np.linspace(x1, x2, 100)\n",
    "            ys = np.linspace(y1, y2, 100)\n",
    "            # pull ys down in the middle\n",
    "            yfrac = np.concatenate([np.linspace(1, 0, 50), np.linspace(0, 1, 50)])\n",
    "            yfrac **= curve_power\n",
    "            ys *= yfrac * curve_strength + 1 - curve_strength\n",
    "        else:\n",
    "            xs = [x1, x2]\n",
    "            ys = [y1, y2]\n",
    "        ax.plot(xs, ys, zorder=0, **other_kwargs)\n",
    "\n",
    "    # Draw markers\n",
    "    # # Starts\n",
    "    # ax.plot(xs1, ys1, linestyle='', **marker_kwargs)\n",
    "    # Ends\n",
    "    ax.plot(xs2, ys2, linestyle='', **marker_kwargs)\n",
    "\n",
    "    # Only add the label once\n",
    "    if label:\n",
    "        ax.plot([], [], label=label, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae07f9",
   "metadata": {},
   "source": [
    "### Polar lobe charts\n",
    "\n",
    "These charts show a 2D slice of functions with a polar projection. This helps to visualize the _shape_ of the intervention, although it's a bit hard to interpret the magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350ad130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from math import pi, acos\n",
    "\n",
    "NEON = ['hotpink', 'orange', 'limegreen', 'pink', 'aqua', 'yellow']\n",
    "\n",
    "\n",
    "def draw_intervention_slice(ax: Axes | PolarAxes, idf: Intervention):\n",
    "    \"\"\"\n",
    "    Plot a 2D slice of an intervention function on a polar axes.\n",
    "\n",
    "    The angular coordinate corresponds to the direction of a unit input vector.\n",
    "    Two curves are drawn:\n",
    "      - Transformed: the output vector converted to polar (θ_out, r_out)\n",
    "      - Falloff: the magnitude of the intervention plotted against input θ\n",
    "\n",
    "    Args:\n",
    "        ax: A PolarAxes instance to draw into.\n",
    "        idf: The intervention function to plot. Will be called with a tensor of [B,E] where E=2.\n",
    "    \"\"\"\n",
    "    theta_out, r_out, thetas_in, annotation = sample_idf(idf, 360, eps=1e-7)\n",
    "\n",
    "    # Post-intervention activations\n",
    "    filled_series(\n",
    "        ax,\n",
    "        theta_out,\n",
    "        r_out,\n",
    "        color='#1f77b4',\n",
    "        linewidth=2.0,\n",
    "        label='Transformed',\n",
    "        alpha=0.15 if isinstance(ax, PolarAxes) else 0.0,\n",
    "    )\n",
    "\n",
    "    # Magnitude of intervention\n",
    "    filled_series(\n",
    "        ax,\n",
    "        thetas_in,\n",
    "        annotation[1],\n",
    "        color='#ff7f0e',\n",
    "        close='always',\n",
    "        linewidth=2.0,\n",
    "        label=annotation[0],\n",
    "        alpha=0.15 if isinstance(ax, PolarAxes) else 0.0,\n",
    "    )\n",
    "\n",
    "    # Differences\n",
    "    theta_out, r_out, thetas_in, _ = sample_idf(idf, 360 // 10, eps=1e-7, include_end=idf.type != 'linear')\n",
    "    diff_series(\n",
    "        ax,\n",
    "        thetas_in,\n",
    "        theta_out,\n",
    "        np.ones_like(thetas_in),\n",
    "        r_out,\n",
    "        shape='line' if idf.type == 'linear' else 'chord',\n",
    "        color='white',\n",
    "        alpha=0.6,\n",
    "        linewidth=0.5,\n",
    "        marker='o',\n",
    "        markersize=2.0,\n",
    "        markeredgecolor='none',\n",
    "        markerfacecolor='white',\n",
    "        label='Offset',\n",
    "    )\n",
    "\n",
    "    for annot, color in zip(idf.annotations, cycle(NEON), strict=False):\n",
    "        if annot.type == 'angular':\n",
    "            cx = acos(annot.value)\n",
    "            ax.axvline(cx, color=color, alpha=1.0, linewidth=1, linestyle='--', label=annot.name, zorder=0)\n",
    "            ax.axvline(-cx, color=color, alpha=1.0, linewidth=1, linestyle='--', zorder=0)\n",
    "        else:\n",
    "            cy = annot.value\n",
    "            ax.axhline(cy, color=color, alpha=1.0, linewidth=1, linestyle='--', label=annot.name, zorder=0)\n",
    "\n",
    "    if isinstance(ax, PolarAxes):\n",
    "        # Customize polar plot\n",
    "        ax.set_theta_zero_location('N')  # 0° at top (perfect alignment)\n",
    "        # ax.set_thetalim(0, np.pi)  # Only show 0 to π (hemisphere)\n",
    "\n",
    "        # Configure polar grid\n",
    "        ax.set_thetagrids([180], [''])  # One line: opposing (cos sim = -1)\n",
    "        ax.set_rticks([0.0, 1.0])  # Just the outer circle\n",
    "\n",
    "        # Set radial limit to comfortably contain all data and the unit radius\n",
    "        max_r = max(r_out.max(), annotation[1].max(), 1.0)\n",
    "        ax.set_rmax(max(1.0, max_r) * 1.1)\n",
    "\n",
    "    else:\n",
    "        ax.set_xlim(0, np.pi)\n",
    "        ax.set_ylim(0, max(r_out.max(), annotation[1].max(), 1.0) * 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0911dcb6",
   "metadata": {},
   "source": [
    "### Linear charts\n",
    "\n",
    "These charts show the effects of the intervention as well. The input to the intervention is the alignment with the concept vector — so we'll use that as the x-axis. The choice of y-axis depends on the type of the intervention:\n",
    "\n",
    "- For suppression, it's useful to see the magnitude of the intervention\n",
    "- For repulsion, it's more useful to see the output of the mapping (i.e. the post-intervention alignment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2363a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for linear charts reused across figures\n",
    "from math import cos, pi, sqrt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.patheffects import SimpleLineShadow, Normal\n",
    "\n",
    "\n",
    "def draw_mapping_linear(\n",
    "    ax: Axes,\n",
    "    mapping: Mapper,\n",
    "    *,\n",
    "    title: str | None = None,\n",
    "    color: str = '#1f77b4',\n",
    "    color_secondary: str = '#ff7f0e',\n",
    "    show_identity: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Draw a linear mapping y = f(x) for cosine similarity inputs.\n",
    "\n",
    "    x is cosine similarity in [-1, 1]. y is mapping(x) in [-1, 1].\n",
    "    \"\"\"\n",
    "    x = np.linspace(-1, 1, 400, dtype=np.float32)\n",
    "    xt = torch.from_numpy(x)\n",
    "    with torch.no_grad():\n",
    "        y = mapping(xt).detach().cpu().numpy()\n",
    "\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    setup_cosine_axes(ax, axis='both')\n",
    "\n",
    "    if show_identity:\n",
    "        ax.axline((0, 0), slope=1, color='gray', alpha=0.2, linewidth=1, linestyle='--')\n",
    "\n",
    "    # Fill region between identity and adjusted activations: this is the magnitude of the effect\n",
    "    ax.fill_between(x, x, y, color=color_secondary, alpha=0.15, zorder=0)\n",
    "\n",
    "    ax.plot(\n",
    "        x,\n",
    "        y,\n",
    "        label='activation alignment',\n",
    "        color=color,\n",
    "        linewidth=2,\n",
    "        path_effects=[SimpleLineShadow((0, 0), linewidth=4, alpha=0.5), Normal()],\n",
    "    )\n",
    "\n",
    "    for annot, _color in zip(mapping.annotations, cycle(NEON), strict=False):\n",
    "        # Both axes are angular, so inspect direction\n",
    "        if annot.direction == 'input':\n",
    "            ax.axvline(annot.value, color=_color, alpha=1.0, linewidth=1, linestyle='--', label=annot.name, zorder=0)\n",
    "            ax.text(\n",
    "                annot.value + 0.02,\n",
    "                ax.viewLim.ymin + 0.2,\n",
    "                f'{annot.name} = {annot.value:.2g}',\n",
    "                color=_color,\n",
    "                fontsize='x-small',\n",
    "                rotation=90,\n",
    "            )\n",
    "        else:\n",
    "            ax.axhline(annot.value, color=_color, alpha=1.0, linewidth=1, linestyle='--', label=annot.name, zorder=0)\n",
    "            ax.text(\n",
    "                ax.viewLim.xmin + 0.2,\n",
    "                annot.value + 0.02,\n",
    "                f'{annot.name} = {annot.value:.2g}',\n",
    "                color=_color,\n",
    "                fontsize='x-small',\n",
    "            )\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "\n",
    "def draw_suppression_strength(\n",
    "    ax: Axes,\n",
    "    falloff: Mapper,\n",
    "    *,\n",
    "    title: str | None = None,\n",
    "    color: str = '#ff7f0e',\n",
    "    color_secondary: str = '#1f77b4',\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Draw suppression amount vs cosine similarity.\n",
    "\n",
    "    x: cosine similarity in [-1, 1]\n",
    "    y: suppression amount in [0, 1] computed as falloff(alignment),\n",
    "       where alignment = max(0, x) for unidirectional suppression.\n",
    "    \"\"\"\n",
    "    x = np.linspace(-1, 1, 400, dtype=np.float32)\n",
    "    alignment = np.clip(x, 0.0, 1.0).astype(np.float32)  # Only positive alignment contributes\n",
    "    xt = torch.from_numpy(alignment)\n",
    "    with torch.no_grad():\n",
    "        y = falloff(xt).detach().cpu().numpy()\n",
    "\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(0, max(1.0, float(np.max(y)) * 1.05))\n",
    "    setup_cosine_axes(ax, axis='x')\n",
    "    ax.set_ylabel('Suppression amount', fontsize='small', labelpad=10)\n",
    "\n",
    "    # Fill region above: this is the residual activation magnitude\n",
    "    ax.fill_between(x, np.ones_like(x), y, color=color_secondary, alpha=0.15, zorder=0)\n",
    "\n",
    "    # Fill region below: this is the strength of the effect (mirrors the filled region in the polar plots)\n",
    "    ax.fill_between(x, np.zeros_like(x), y, color=color, alpha=0.15, zorder=0)\n",
    "\n",
    "    ax.plot(\n",
    "        x,\n",
    "        y,\n",
    "        label='amount',\n",
    "        color=color,\n",
    "        linewidth=2,\n",
    "        path_effects=[SimpleLineShadow((0, 0), linewidth=4, alpha=0.5), Normal()],\n",
    "    )\n",
    "\n",
    "    # Threshold annotation for bounded falloffs defined over alignment\n",
    "\n",
    "    for annot, _color in zip(falloff.annotations, cycle(NEON), strict=False):\n",
    "        # One angular and one linear axis, so inspect type\n",
    "        if annot.type != 'linear':\n",
    "            ax.axvline(annot.value, color=_color, alpha=1.0, linewidth=1, linestyle='--', label=annot.name, zorder=0)\n",
    "            ax.text(\n",
    "                annot.value + 0.02,\n",
    "                ax.viewLim.ymin + 0.2,\n",
    "                f'{annot.name} = {annot.value:.2g}',\n",
    "                color=_color,\n",
    "                fontsize='x-small',\n",
    "                rotation=90,\n",
    "            )\n",
    "        else:\n",
    "            ax.axhline(annot.value, color=_color, alpha=1.0, linewidth=1, linestyle='--', label=annot.name, zorder=0)\n",
    "            ax.text(\n",
    "                ax.viewLim.xmin + 0.2,\n",
    "                annot.value + 0.02,\n",
    "                f'{annot.name} = {annot.value:.2g}',\n",
    "                color=_color,\n",
    "                fontsize='x-small',\n",
    "            )\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "\n",
    "def setup_cosine_axes(ax: Axes, axis: str = 'both') -> None:\n",
    "    \"\"\"\n",
    "    Set cosine ticks/labels on axes for readability.\n",
    "\n",
    "    axis: 'x' | 'y' | 'both'\n",
    "    \"\"\"\n",
    "    # Major ticks at +-1, +-cos(30), +-cos(60), 0\n",
    "    cos_values = np.array([-1, -cos(pi / 6), -cos(pi / 3), 0, cos(pi / 3), cos(pi / 6), 1.0])\n",
    "    xlabels = np.array(['-1\\nopposing', '', '', '0\\northogonal', '', '', '1\\naligned'])\n",
    "    ylabels = np.array(['-1', '', '', '0', '', '', '1'])\n",
    "\n",
    "    if axis in ('x', 'both'):\n",
    "        ax.set_xticks(cos_values)\n",
    "        ax.set_xticklabels(xlabels, fontsize='x-small')\n",
    "        ax.set_xlabel('Cosine similarity (angle from subject)', fontsize='small', labelpad=10)\n",
    "\n",
    "    if axis in ('y', 'both'):\n",
    "        ax.set_yticks(cos_values)\n",
    "        ax.set_yticklabels(ylabels, fontsize='x-small')\n",
    "        ax.set_ylabel('Output cosine similarity', fontsize='small', labelpad=10)\n",
    "\n",
    "    # Minor ticks at every 10 degrees\n",
    "    cos_minor = np.cos(np.arange(0, 91, 10) * np.pi / 180)\n",
    "    cos_minor = np.concatenate([-cos_minor[:-1], cos_minor])\n",
    "    if axis in ('x', 'both'):\n",
    "        ax.set_xticks(cos_minor, minor=True)\n",
    "    if axis in ('y', 'both'):\n",
    "        ax.set_yticks(cos_minor, minor=True)\n",
    "\n",
    "    ax.grid(True, which='major', alpha=0.1)\n",
    "\n",
    "\n",
    "def draw_bezier_handle(ax: Axes, cp1: Tensor, cp2: Tensor, *, color: str, **kwargs):\n",
    "    cx, cy = zip(cp1, cp2, strict=True)\n",
    "    ax.plot(\n",
    "        cx,\n",
    "        cy,\n",
    "        color=color,\n",
    "        linewidth=1.5,\n",
    "        zorder=0,\n",
    "        **kwargs,\n",
    "    )\n",
    "    ax.plot(\n",
    "        cx,\n",
    "        cy,\n",
    "        color=color,\n",
    "        linestyle=' ',\n",
    "        marker='o',\n",
    "        markersize=4,\n",
    "        markerfacecolor='black',\n",
    "        markeredgewidth=1.2,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fa933",
   "metadata": {},
   "source": [
    "## Suppression\n",
    "\n",
    "This type of intervention is used to reduce the **magnitude** of embeddings that are aligned with a concept vector. There are at least two ways to do that:\n",
    "\n",
    "- Reduce the overall magnitude of aligned embeddings, without changing their direction\n",
    "- Reduce the aligned component, without changing unaligned components.\n",
    "\n",
    "The type implemented here is the second variety. The `Suppression` intervention selectively reduces the component of activations that align with a target concept:\n",
    "\n",
    "$$\\mathbf{x}' = \\mathbf{x} - g(\\alpha) \\cdot (\\mathbf{x} \\cdot \\mathbf{v}) \\cdot \\mathbf{v}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^E$ are the input activations\n",
    "- $\\mathbf{v} \\in \\mathbb{R}^E$ is the unit-norm concept vector ($\\|\\mathbf{v}\\|_2 = 1$)\n",
    "- $\\alpha = \\max(0, \\min(1, \\mathbf{x} \\cdot \\mathbf{v})) \\in [0,1]$ is the clamped alignment\n",
    "- $g(α)$ is the suppression gate strength (see below)\n",
    "- $\\mathbf{x} \\cdot \\mathbf{v} \\in [-1,1]$ is the raw signed projection magnitude used in the suppression\n",
    "\n",
    "The intervention preserves the components of $\\mathbf{x}$ orthogonal to $\\mathbf{v}$ while selectively reducing the aligned component based on how strongly the activation aligns with the concept direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b64560f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import override\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from ex_color.intervention.intervention import Intervention, VarAnnotation\n",
    "\n",
    "\n",
    "class Suppression(Intervention):\n",
    "    type = 'linear'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        concept_vector: Tensor,  # Embedding to suppress [E] (unit norm)\n",
    "        falloff: Mapper,  # Function to calculate strength of suppression\n",
    "    ):\n",
    "        super().__init__(concept_vector)\n",
    "        self.falloff = falloff\n",
    "\n",
    "    @override\n",
    "    def dist(self, activations: Tensor) -> Tensor:\n",
    "        dots = torch.sum(activations * self.concept_vector[None, :], dim=1)  # [B]\n",
    "        return dots.clamp(min=0, max=1)\n",
    "\n",
    "    def gate(self, activations: Tensor) -> Tensor:\n",
    "        return self.falloff(self.dist(activations))\n",
    "\n",
    "    @override\n",
    "    def __call__(self, activations: Tensor) -> Tensor:\n",
    "        gate = self.gate(activations)\n",
    "        p = torch.einsum('b...e,e->b...', activations, self.concept_vector)\n",
    "\n",
    "        return activations - torch.einsum('b...,e->b...e', gate * p, self.concept_vector)\n",
    "\n",
    "    @property\n",
    "    @override\n",
    "    def annotations(self):\n",
    "        return self.falloff.annotations\n",
    "\n",
    "    @override\n",
    "    def annotate_activations(self, activations: Tensor) -> VarAnnotation:\n",
    "        return VarAnnotation('strength', self.gate(activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd91ea5",
   "metadata": {},
   "source": [
    "### Bounded falloff\n",
    "\n",
    "The `BoundedMapper` class implements a threshold-based falloff function that maps alignment strength to suppression intensity:\n",
    "\n",
    "$$\n",
    "g(\\alpha) = \\begin{cases}\n",
    "0 & \\text{if } \\alpha \\leq a \\\\\n",
    "b \\cdot \\left(\\frac{\\alpha - a}{1 - a}\\right)^p & \\text{if } \\alpha > a\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\alpha \\in [-1,1]$ is the alignment between activation and concept vector\n",
    "- $a \\in [0,1)$ is the threshold below which no suppression occurs\n",
    "- $b \\in [0,1]$ is the maximum suppression strength\n",
    "- $p \\ge 1$ is the power that controls the falloff curve shape\n",
    "\n",
    "This creates a smooth transition from no suppression (below threshold $a$) to maximum suppression strength $b$, with the power parameter controlling whether the falloff is linear ($p=1$) or convex ($p>1$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6ba645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotated_types import Ge, Le\n",
    "from typing import Annotated, Sequence\n",
    "\n",
    "from pydantic import validate_call\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from ex_color.intervention.intervention import ConstAnnotation, VarAnnotation\n",
    "\n",
    "\n",
    "class BoundedMapper:\n",
    "    @validate_call(config={'arbitrary_types_allowed': True})\n",
    "    def __init__(\n",
    "        self,\n",
    "        a: Annotated[float, [Ge(0), Le(1)]],\n",
    "        b: Annotated[float, [Ge(0), Le(1)]],\n",
    "        power: Annotated[float, [Ge(1)]] = 1.0,\n",
    "        eps=1e-8,\n",
    "    ):\n",
    "        assert a < b, 'a must be less than b for Bounded falloff'\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.power = power\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, alignment: Tensor) -> Tensor:\n",
    "        if self.a > 1 - self.eps:\n",
    "            return alignment\n",
    "\n",
    "        shifted = (alignment - self.a) / (1 - self.a)\n",
    "        shifted = shifted**self.power * (self.b)\n",
    "        return torch.where(alignment > self.a, shifted, torch.zeros_like(alignment))\n",
    "\n",
    "    @property\n",
    "    def annotations(self) -> Sequence[ConstAnnotation]:\n",
    "        return [\n",
    "            ConstAnnotation('input', 'angular', 'start', self.a),\n",
    "            ConstAnnotation('output', 'linear', 'strength', self.b),\n",
    "        ]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}({self.a:.2g}, {self.b:.2g}, {self.power:.2g})'\n",
    "\n",
    "    def __str__(self):\n",
    "        exp = f'd^{self.power:.2g}' if self.power != 1.0 else 'd'\n",
    "        return f'{exp} | d>{self.a:.2g}, 1→{self.b:.2g}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d390d222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-2.1-suppression.png?v=FzItu_Yrz8Dcn0ksx4tz_0RU3pNKtwPEc9iVRJy8zTc\" alt=\"Grid of semicircular polar plots showing the effects of suppression on activations. Each plot shows two lobes: an orange one indicating the magnitude of the intervention, and a blue one showing the transformed activation space. The direction being intervened on (the &#x27;subject&#x27;) is always &#x27;up&#x27;, so the orange &#x27;magnitude&#x27; lobes are also oriented upwards. The blue &#x27;transformed&#x27; lobes are more circular but have a depression in the top, showing that the directions more aligned with the subject are squashed/attenuated by the intervention.\" style=\"max-width: 100%;\" />"
      ],
      "text/plain": [
       "Grid of semicircular polar plots showing the effects of suppression on activations. Each plot shows two lobes: an orange one indicating the magnitude of the intervention, and a blue one showing the transformed activation space. The direction being intervened on (the 'subject') is always 'up', so the orange 'magnitude' lobes are also oriented upwards. The blue 'transformed' lobes are more circular but have a depression in the top, showing that the directions more aligned with the subject are squashed/attenuated by the intervention."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from math import cos, pi\n",
    "from typing import cast, override\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from pydantic import validate_call\n",
    "\n",
    "from utils.nb import displayer_img\n",
    "from utils.plt import configure_matplotlib\n",
    "\n",
    "configure_matplotlib()\n",
    "\n",
    "\n",
    "falloffs = [\n",
    "    BoundedMapper(0, 0.5),\n",
    "    BoundedMapper(cos(pi / 3), cos(pi / 6)),\n",
    "    BoundedMapper(0, 1, power=2),\n",
    "    BoundedMapper(cos(pi / 3), cos(pi / 6), power=2),\n",
    "]\n",
    "\n",
    "\n",
    "with displayer_img(\n",
    "    f'large-assets/ex-{nb_id}-suppression.png',\n",
    "    alt_text=\"Grid of semicircular polar plots showing the effects of suppression on activations. Each plot shows two lobes: an orange one indicating the magnitude of the intervention, and a blue one showing the transformed activation space. The direction being intervened on (the 'subject') is always 'up', so the orange 'magnitude' lobes are also oriented upwards. The blue 'transformed' lobes are more circular but have a depression in the top, showing that the directions more aligned with the subject are squashed/attenuated by the intervention.\",\n",
    ") as show:\n",
    "    # Two rows: polar slices (top) and linear suppression amount (bottom)\n",
    "    n = len(falloffs)\n",
    "    fig = plt.figure(figsize=(1 + 4.5 * n, 8.5), constrained_layout=True)\n",
    "\n",
    "    axes = []\n",
    "    linear_axes = []\n",
    "    lax = None\n",
    "    for i, mapper in enumerate(falloffs):\n",
    "        ax = cast(PolarAxes, fig.add_subplot(2, n, i + 1, axes_class=PolarAxes))\n",
    "        ax.spines['polar'].set_color(c='gray')\n",
    "        ax.grid(True, color='#444', linewidth=0.5)\n",
    "\n",
    "        idf = Suppression(\n",
    "            torch.tensor([1, 0], dtype=torch.float32),  # North\n",
    "            mapper,\n",
    "        )\n",
    "        draw_intervention_slice(ax, idf)\n",
    "        ax.set_title(str(idf.falloff), pad=15)\n",
    "        ax.tick_params(labelleft=False)  # The y-axis is actually the radial axis\n",
    "        ax.spines['polar'].set_visible(False)\n",
    "        axes.append(ax)\n",
    "\n",
    "        # Linear suppression-strength chart\n",
    "        lax = fig.add_subplot(2, n, n + i + 1, sharey=lax)\n",
    "        draw_suppression_strength(lax, mapper, title='amount vs cos(θ)')\n",
    "        lax.set_aspect('equal')\n",
    "        lax.set_adjustable('box')\n",
    "        if i > 0:\n",
    "            lax.tick_params(labelleft=False)\n",
    "            lax.set_ylabel('')\n",
    "            lax.set_xlabel('')\n",
    "        linear_axes.append(lax)\n",
    "\n",
    "    # Single legend for all polar axes\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    legend = fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc='lower center',\n",
    "        ncol=len(labels),\n",
    "        frameon=False,\n",
    "        bbox_to_anchor=(0.5, -0.05),\n",
    "        bbox_transform=fig.transFigure,\n",
    "        fontsize='medium',\n",
    "    )\n",
    "\n",
    "    fig.suptitle('Intervention lobes: Suppression')\n",
    "\n",
    "    show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a78bc0",
   "metadata": {},
   "source": [
    "## Repulsion\n",
    "\n",
    "The `Repulsion` intervention rotates activations away from a concept vector within their shared 2D plane:\n",
    "\n",
    "$$\\mathbf{x}' = m(\\alpha) \\mathbf{v} + \\sqrt{1 - m(\\alpha)^2} \\mathbf{u}_\\perp$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^E$ are the input activations (assumed unit norm)\n",
    "- $\\mathbf{v} \\in \\mathbb{R}^E$ is the unit-norm concept vector\n",
    "- $\\alpha = \\max(0, \\min(1, \\mathbf{x} \\cdot \\mathbf{v}))$ is the original clamped alignment\n",
    "- $\\mathbf{u}_\\perp = \\frac{\\mathbf{x} - (\\mathbf{x} \\cdot \\mathbf{v})\\mathbf{v}}{\\|\\mathbf{x} - (\\mathbf{x} \\cdot \\mathbf{v})\\mathbf{v}\\|}$ is the unit vector perpendicular to $\\mathbf{v}$ in the plane spanned by $\\mathbf{x}$ and $\\mathbf{v}$\n",
    "\n",
    "The intervention only applies to activations with positive alignment ($\\mathbf{x} \\cdot \\mathbf{v} > 0$), leaving others unchanged.\n",
    "\n",
    "This approach preserves the geometric relationships of a unit norm embedding space: instead of damping components like in suppression, it steers the representation to a new point on the unit sphere. The rotation happens entirely within the 2D plane defined by the original activation and the concept vector, which means the resulting activation vector is as close to the original representation as possible while still being repelled away.\n",
    "\n",
    "For the edge case where activations are nearly parallel to the concept vector (making $\\mathbf{u}_\\perp$ ill-defined), our implementation generates a random orthogonal direction using Gram-Schmidt orthogonalisation. This ensures the rotation can still proceed, although this could result in the vector being pushed into an out-of-distribution region. It may make more sense to analyze the representation space beforehand to determine a default direction to use.\n",
    "\n",
    "The constraint that rotated vectors maintain unit norm emerges naturally from the spherical geometry — any point on the unit sphere can be parameterised by its alignment with a reference direction and its perpendicular component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e2ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import override\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from ex_color.intervention.intervention import Intervention\n",
    "\n",
    "\n",
    "class Repulsion(Intervention):\n",
    "    type = 'rotational'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        concept_vector: Tensor,  # Embedding to steer away from [E] (unit norm)\n",
    "        mapper: Mapper,  # Function to recalculate dot products to determine rotation of activations\n",
    "        eps: float = 1e-8,  # Numerical stability threshold\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Repel activations away from subject vector by rotating in their shared plane.\n",
    "\n",
    "        Returns:\n",
    "            Rotated activations with unit norm, shape [B, E].\n",
    "        \"\"\"\n",
    "        super().__init__(concept_vector)\n",
    "        self.mapper = mapper\n",
    "        self.eps = eps\n",
    "\n",
    "    @override\n",
    "    def dist(self, activations: Tensor) -> Tensor:\n",
    "        dots = torch.sum(activations * self.concept_vector[None, :], dim=1)  # [B]\n",
    "        return torch.clamp(dots, 0, 1)\n",
    "\n",
    "    @override\n",
    "    def __call__(self, activations: Tensor) -> Tensor:\n",
    "        # Calculate original dot products\n",
    "        dots = self.dist(activations)  # [B]\n",
    "\n",
    "        # Scale dot products with falloff function\n",
    "        target_dots = self.mapper(dots)  # [B]\n",
    "\n",
    "        # Decompose into parallel and perpendicular components\n",
    "        v_parallel = dots[:, None] * self.concept_vector[None, :]  # [B, E]\n",
    "        v_perp = activations - v_parallel  # [B, E]\n",
    "\n",
    "        # Get perpendicular unit vectors (handle near-parallel case)\n",
    "        v_perp_norm = torch.norm(v_perp, dim=1, keepdim=True)  # [B, 1]\n",
    "\n",
    "        # For nearly parallel vectors, choose random orthogonal direction\n",
    "        nearly_parallel = (v_perp_norm < self.eps).squeeze()  # [B]\n",
    "\n",
    "        if nearly_parallel.any():\n",
    "            # Generate random orthogonal vectors\n",
    "            random_vecs = torch.randn_like(v_perp[nearly_parallel])\n",
    "            # Make orthogonal to subject using Gram-Schmidt\n",
    "            proj = torch.sum(random_vecs * self.concept_vector[None, :], dim=1, keepdim=True)\n",
    "            random_vecs = random_vecs - proj * self.concept_vector[None, :]\n",
    "            random_vecs = random_vecs / torch.norm(random_vecs, dim=1, keepdim=True)\n",
    "\n",
    "            v_perp[nearly_parallel] = random_vecs\n",
    "            v_perp_norm[nearly_parallel] = 1.0\n",
    "\n",
    "        u_perp = v_perp / v_perp_norm  # [B, E]\n",
    "\n",
    "        # Construct rotated vectors in the (subject, u_perp) plane\n",
    "        target_dots_clamped = torch.clamp(target_dots, -1 + self.eps, 1 - self.eps)\n",
    "        perp_component = torch.sqrt(1 - target_dots_clamped**2)  # [B]\n",
    "\n",
    "        v_rotated = (\n",
    "            target_dots_clamped[:, None] * self.concept_vector[None, :] + perp_component[:, None] * u_perp\n",
    "        )  # [B, E]\n",
    "\n",
    "        # Only apply rotation to vectors with positive original dot product\n",
    "        should_rotate = dots > 0  # [B]\n",
    "        return torch.where(should_rotate[:, None], v_rotated, activations)\n",
    "\n",
    "    @property\n",
    "    @override\n",
    "    def annotations(self):\n",
    "        return self.mapper.annotations\n",
    "\n",
    "    @override\n",
    "    def annotate_activations(self, activations: Tensor) -> VarAnnotation:\n",
    "        dots = self.dist(activations)  # [B]\n",
    "        target_dots = self.mapper(dots)  # [B]\n",
    "        return VarAnnotation('offset', (dots - target_dots).abs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569c661",
   "metadata": {},
   "source": [
    "### Linear mapper\n",
    "\n",
    "The `LinearMapper` creates a piecewise-linear transformation that compresses the upper range of alignment values:\n",
    "\n",
    "$$\n",
    "m_{\\text{linear}}(\\alpha) = \\begin{cases}\n",
    "\\alpha & \\text{if } \\alpha \\leq a \\\\\n",
    "a + (b - a) \\cdot \\frac{\\alpha - a}{1 - a} & \\text{if } \\alpha > a\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\alpha \\in [0,1]$ is the clamped alignment between activation and concept vector\n",
    "- $a \\in [0,1)$ is the threshold below which no mapping occurs\n",
    "- $b \\in (0,1]$ is the maximum mapped value (with $a < b$)\n",
    "\n",
    "This effectively brings alignments above threshold $a$ into the range $[a,b]$, creating a \"ceiling\" effect that prevents activations from becoming too aligned with the concept vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ab0db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ex_color.intervention.intervention import ConstAnnotation\n",
    "\n",
    "\n",
    "from annotated_types import Ge, Gt, Le, Lt\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class LinearMapper(Mapper):\n",
    "    @validate_call(config={'arbitrary_types_allowed': True})\n",
    "    def __init__(\n",
    "        self,\n",
    "        a: Annotated[float, [Ge(0), Lt(1)]],\n",
    "        b: Annotated[float, [Gt(0), Le(1)]],\n",
    "        eps=1e-8,\n",
    "    ):\n",
    "        assert a < b\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, alignment: Tensor) -> Tensor:  # alignment is a batch, shape [B]\n",
    "        shifted = (alignment - self.a) / (1 - self.a)\n",
    "        shifted = shifted * (self.b - self.a) + self.a\n",
    "        return torch.where(alignment > self.a, shifted, alignment)\n",
    "\n",
    "    @property\n",
    "    def annotations(self):\n",
    "        return [\n",
    "            ConstAnnotation('input', 'angular', 'start', self.a),\n",
    "            ConstAnnotation('output', 'angular', 'end', self.b),\n",
    "        ]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}({self.a:.2g}, {self.b:.2g})'\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'd>{self.a:.2g}, 1→{self.b:.2g}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa0f1a",
   "metadata": {},
   "source": [
    "### Bézier mapper\n",
    "\n",
    "The `BezierMapper` implements a cubic Bézier curve to create smooth, controllable mapping functions:\n",
    "\n",
    "$$\\mathbf{B}(t) = (1-t)^3\\mathbf{P}_0 + 3(1-t)^2t\\mathbf{P}_1 + 3(1-t)t^2\\mathbf{P}_2 + t^3\\mathbf{P}_3$$\n",
    "\n",
    "Where the control points are constructed as:\n",
    "\n",
    "- $\\mathbf{P}_0 = (a, a)$ — start point\n",
    "- $\\mathbf{P}_3 = (1, b)$ — end point\n",
    "- $\\mathbf{P}_1, \\mathbf{P}_2$ — intermediate control points derived from tangent constraints\n",
    "\n",
    "The mapping function $m_{\\text{bezier}}(\\alpha)$ is obtained by:\n",
    "\n",
    "1. **Inverse parameterisation**: For input $\\alpha > a$, solve $B_x(t) = \\alpha$ for parameter $t$\n",
    "2. **Function evaluation**: Return $m_{\\text{bezier}}(\\alpha) = B_y(t)$\n",
    "\n",
    "The intermediate control points are positioned to satisfy tangent slope constraints:\n",
    "\n",
    "$$\\mathbf{P}_1 = \\mathbf{P}_0 + d \\cdot (\\mathbf{I} - \\mathbf{P}_0)$$\n",
    "$$\\mathbf{P}_2 = \\mathbf{P}_3 + d \\cdot (\\mathbf{I} - \\mathbf{P}_3)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{I}$ is the intersection of tangent lines at the start and end points\n",
    "- $d$ is the `control_distance` parameter\n",
    "\n",
    "This construction ensures the curve has the desired start slope (typically 1.0 to match the identity function) and end slope (typically 0.0 for a smooth ceiling effect).\n",
    "\n",
    "The core challenge is the inverse parameterisation — given $\\alpha$, finding $t$ such that $B_x(t) = \\alpha$. Our implementation uses Newton's method with automatic differentiation:\n",
    "\n",
    "$$t_{n+1} = t_n - \\frac{B_x(t_n) - \\alpha}{B'_x(t_n)}$$\n",
    "\n",
    "The `FastBezierMapper` variant trades memory for speed by precomputing a lookup table and using linear interpolation — a trick commonly used in computer graphics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a8c3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class BezierMapper(Mapper):\n",
    "    @validate_call\n",
    "    def __init__(\n",
    "        self,\n",
    "        a: Annotated[float, [Ge(0), Lt(1)]],\n",
    "        b: Annotated[float, [Gt(0), Le(1)]],\n",
    "        start_slope: float = 1.0,  # Aligned with unmapped leadup\n",
    "        end_slope: float = 0.0,  # Flat\n",
    "        control_distance: float = 1 / sqrt(2),  # Relative to intersection point\n",
    "    ):\n",
    "        assert a < b <= 1\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "        # Find intersection of the two tangent lines\n",
    "        # Line 1: y - a = start_slope * (x - a)  =>  y = start_slope * (x - a) + a\n",
    "        # Line 2: y - b = end_slope * (x - 1)    =>  y = end_slope * (x - 1) + b\n",
    "        # At intersection: start_slope * (x - a) + a = end_slope * (x - 1) + b\n",
    "\n",
    "        if abs(start_slope - end_slope) < 1e-8:\n",
    "            # Parallel lines - use midpoint as fallback\n",
    "            intersection_x = (a + 1) / 2\n",
    "            intersection_y = (a + b) / 2\n",
    "        else:\n",
    "            intersection_x = (a * (start_slope - 1) + b - end_slope) / (start_slope - end_slope)\n",
    "            intersection_y = start_slope * (intersection_x - a) + a\n",
    "\n",
    "        intersection = torch.tensor([intersection_x, intersection_y], dtype=torch.float32)\n",
    "\n",
    "        # Define the 4 control points for cubic Bézier\n",
    "        self.P0 = torch.tensor([a, a], dtype=torch.float32)\n",
    "        self.P3 = torch.tensor([1.0, b], dtype=torch.float32)\n",
    "\n",
    "        # P1: distance from P0 towards intersection, scaled by control_distance\n",
    "        direction_to_intersection = intersection - self.P0\n",
    "        self.P1 = self.P0 + control_distance * direction_to_intersection\n",
    "\n",
    "        # P2: distance from P3 towards intersection, scaled by control_distance\n",
    "        direction_to_intersection = intersection - self.P3\n",
    "        self.P2 = self.P3 + control_distance * direction_to_intersection\n",
    "\n",
    "    def bezier_point(self, t: Tensor) -> Tensor:\n",
    "        \"\"\"Evaluate cubic Bézier curve at parameter t\"\"\"\n",
    "        one_minus_t = 1 - t\n",
    "\n",
    "        term0 = (one_minus_t**3)[:, None] * self.P0[None, :]\n",
    "        term1 = (3 * one_minus_t**2 * t)[:, None] * self.P1[None, :]\n",
    "        term2 = (3 * one_minus_t * t**2)[:, None] * self.P2[None, :]\n",
    "        term3 = (t**3)[:, None] * self.P3[None, :]\n",
    "\n",
    "        return term0 + term1 + term2 + term3\n",
    "\n",
    "    def bezier_x(self, t: Tensor) -> Tensor:\n",
    "        \"\"\"Get x-coordinate of Bézier curve at parameter t\"\"\"\n",
    "        return self.bezier_point(t)[:, 0]  # Changed from [..., 0]\n",
    "\n",
    "    def bezier_y(self, t: Tensor) -> Tensor:\n",
    "        \"\"\"Get y-coordinate of Bézier curve at parameter t\"\"\"\n",
    "        return self.bezier_point(t)[:, 1]  # Changed from [..., 1]\n",
    "\n",
    "    def solve_for_t(self, x: Tensor, max_iters: int = 10) -> Tensor:\n",
    "        \"\"\"Solve for parameter t such that bezier_x(t) = x using Newton's method\"\"\"\n",
    "        # Initial guess: linear interpolation\n",
    "        t = (x - self.a) / (1 - self.a)\n",
    "        t = torch.clamp(t, 0.01, 0.99)  # Avoid endpoints\n",
    "\n",
    "        for _ in range(max_iters):\n",
    "            # Newton step: t_new = t - f(t)/f'(t)\n",
    "            # where f(t) = bezier_x(t) - target_x\n",
    "\n",
    "            # Enable gradients for automatic differentiation\n",
    "            t_var = t.clone().requires_grad_(True)\n",
    "            x_pred = self.bezier_x(t_var)\n",
    "            error = x_pred - x\n",
    "\n",
    "            # Compute derivative dx/dt\n",
    "            dx_dt = torch.autograd.grad(x_pred.sum(), t_var, create_graph=False)[0]\n",
    "\n",
    "            # Newton update (be careful with division by zero)\n",
    "            dt = error / (dx_dt + 1e-8)\n",
    "            t = t - dt\n",
    "            t = torch.clamp(t, 0.0, 1.0)\n",
    "\n",
    "            # Check convergence\n",
    "            if torch.max(torch.abs(error)) < 1e-6:\n",
    "                break\n",
    "\n",
    "        return t\n",
    "\n",
    "    def __call__(self, alignment: Tensor) -> Tensor:\n",
    "        result = alignment.clone()\n",
    "\n",
    "        # Only apply Bézier mapping for alignment > a\n",
    "        mask = alignment > self.a\n",
    "        if mask.any():\n",
    "            x_vals = alignment[mask]\n",
    "\n",
    "            # Solve for t parameters\n",
    "            t_vals = self.solve_for_t(x_vals)\n",
    "\n",
    "            # Get corresponding y values\n",
    "            y_vals = self.bezier_y(t_vals)\n",
    "\n",
    "            result[mask] = y_vals\n",
    "\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def annotations(self):\n",
    "        return [\n",
    "            ConstAnnotation('input', 'angular', 'start', self.a),\n",
    "            ConstAnnotation('output', 'angular', 'end', self.b),\n",
    "        ]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'BezierMapper(a={self.a:.2g}, b={self.b:.2g})'\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Bézier[{self.a:.2g}→{self.b:.2g}]'\n",
    "\n",
    "\n",
    "class FastBezierMapper(BezierMapper):\n",
    "    def __init__(self, *args, lookup_resolution: int = 1000, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Precompute lookup table\n",
    "        t_vals = torch.linspace(0, 1, lookup_resolution, dtype=torch.float32)\n",
    "        bezier_points = self.bezier_point(t_vals)\n",
    "\n",
    "        # Ensure contiguous storage to avoid searchsorted warning\n",
    "        self.x_lookup = bezier_points[:, 0].contiguous()  # x coordinates\n",
    "        self.y_lookup = bezier_points[:, 1].contiguous()  # y coordinates\n",
    "\n",
    "    def interpolate_1d(self, x_query: Tensor) -> Tensor:\n",
    "        \"\"\"1D linear interpolation using lookup table\"\"\"\n",
    "        # Find insertion points for x_query in x_lookup\n",
    "        indices = torch.searchsorted(self.x_lookup, x_query, right=False)\n",
    "\n",
    "        # Clamp indices to valid range\n",
    "        indices = torch.clamp(indices, 1, len(self.x_lookup) - 1)\n",
    "\n",
    "        # Get surrounding points\n",
    "        x0 = self.x_lookup[indices - 1]\n",
    "        x1 = self.x_lookup[indices]\n",
    "        y0 = self.y_lookup[indices - 1]\n",
    "        y1 = self.y_lookup[indices]\n",
    "\n",
    "        # Linear interpolation: y = y0 + (y1 - y0) * (x - x0) / (x1 - x0)\n",
    "        t = (x_query - x0) / (x1 - x0 + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "        y_interp = y0 + t * (y1 - y0)\n",
    "\n",
    "        return y_interp\n",
    "\n",
    "    @override\n",
    "    def __call__(self, alignment: Tensor) -> Tensor:\n",
    "        result = alignment.clone()\n",
    "        mask = alignment > self.a\n",
    "\n",
    "        if mask.any():\n",
    "            x_vals = alignment[mask]\n",
    "\n",
    "            # Use interpolation on lookup table instead of Newton's method\n",
    "            y_vals = self.interpolate_1d(x_vals)\n",
    "\n",
    "            result[mask] = y_vals\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc92b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-2.1-repulsion.png?v=Myt44fj7DE4cYoLUZbPfuh7kNy7waRFp1EE8eR0ZISQ\" alt=\"Grid of semicircular polar plots showing the effects of repulsion on activations. Each plot shows two lobes: an orange one indicating the magnitude of the intervention, and a blue one showing the transformed activation space. The direction being intervened on (the &#x27;subject&#x27;) is always &#x27;up&#x27;, so the orange &#x27;magnitude&#x27; lobes are also oriented upwards. The blue &#x27;transformed&#x27; lobes are more circular but have a chunk taken out of the top, showing that the directions more aligned with the subject are rotated/pushed away by the intervention.\" style=\"max-width: 100%;\" />"
      ],
      "text/plain": [
       "Grid of semicircular polar plots showing the effects of repulsion on activations. Each plot shows two lobes: an orange one indicating the magnitude of the intervention, and a blue one showing the transformed activation space. The direction being intervened on (the 'subject') is always 'up', so the orange 'magnitude' lobes are also oriented upwards. The blue 'transformed' lobes are more circular but have a chunk taken out of the top, showing that the directions more aligned with the subject are rotated/pushed away by the intervention."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from math import cos, pi\n",
    "from typing import cast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "\n",
    "from utils.plt import configure_matplotlib\n",
    "\n",
    "configure_matplotlib()\n",
    "\n",
    "\n",
    "falloffs: list[Mapper] = [\n",
    "    LinearMapper(0, cos(pi / 3)),\n",
    "    FastBezierMapper(0, cos(pi / 3), end_slope=0.2),\n",
    "    LinearMapper(cos(pi / 3), cos(pi / 6)),\n",
    "    FastBezierMapper(cos(pi / 3), cos(pi / 6), end_slope=0.2),\n",
    "]\n",
    "\n",
    "\n",
    "with displayer_img(\n",
    "    f'large-assets/ex-{nb_id}-repulsion.png',\n",
    "    alt_text=\"Grid of semicircular polar plots showing the effects of repulsion on activations. Each plot shows two lobes: an orange one indicating the magnitude of the intervention, and a blue one showing the transformed activation space. The direction being intervened on (the 'subject') is always 'up', so the orange 'magnitude' lobes are also oriented upwards. The blue 'transformed' lobes are more circular but have a chunk taken out of the top, showing that the directions more aligned with the subject are rotated/pushed away by the intervention.\",\n",
    ") as show:\n",
    "    # Two rows: polar slices (top) and linear mapping (bottom)\n",
    "    n = len(falloffs)\n",
    "    fig = plt.figure(figsize=(1 + 4.5 * n, 10.5), constrained_layout=True)\n",
    "\n",
    "    axes = []\n",
    "    linear_axes = []\n",
    "    lax = None\n",
    "    for i, mapper in enumerate(falloffs):\n",
    "        ax = cast(PolarAxes, fig.add_subplot(2, n, i + 1, axes_class=PolarAxes))\n",
    "        ax.spines['polar'].set_color(c='gray')\n",
    "        ax.grid(True, color='#444', linewidth=0.5)\n",
    "\n",
    "        idf = Repulsion(\n",
    "            torch.tensor([1, 0], dtype=torch.float32),  # North\n",
    "            mapper,\n",
    "        )\n",
    "        draw_intervention_slice(ax, idf)\n",
    "        ax.set_title(str(idf.mapper), pad=15)\n",
    "        ax.tick_params(labelleft=False)  # The y-axis is actually the radial axis\n",
    "        ax.spines['polar'].set_visible(False)\n",
    "        axes.append(ax)\n",
    "\n",
    "        # Linear mapping chart using the same mapping function (\"falloff\" here)\n",
    "        lax = fig.add_subplot(2, n, n + i + 1, sharey=lax)\n",
    "        draw_mapping_linear(lax, mapper, title='mapping')\n",
    "        lax.set_aspect('equal')\n",
    "        if i > 0:\n",
    "            lax.tick_params(labelleft=False)\n",
    "            lax.set_ylabel('')\n",
    "            lax.set_xlabel('')\n",
    "\n",
    "        # Control points overlay\n",
    "        if isinstance(mapper, BezierMapper):\n",
    "            draw_bezier_handle(lax, mapper.P0, mapper.P1, color='hotpink')\n",
    "            draw_bezier_handle(lax, mapper.P2, mapper.P3, color='orange')\n",
    "        linear_axes.append(lax)\n",
    "\n",
    "    # Single legend for all polar axes\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    legend = fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc='lower center',\n",
    "        ncol=len(labels),\n",
    "        frameon=False,\n",
    "        bbox_to_anchor=(0.5, -0.05),\n",
    "        bbox_transform=fig.transFigure,\n",
    "        fontsize='medium',\n",
    "    )\n",
    "\n",
    "    fig.suptitle('Intervention lobes: Repulsion')\n",
    "\n",
    "    show(fig)\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex-color-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
