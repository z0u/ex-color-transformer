{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191d5190",
   "metadata": {},
   "source": [
    "# Experiment 2.4: Post-norm regularization\n",
    "\n",
    "In [Ex 2.3](./ex-2.3-explicit-norm.ipynb), we applied interventions to an autoencoder that included regularization and an explicit normalization step. That performed significantly better than earlier model versions that relied solely on regularization. That experiment (2.3) had all regularizers immediately before the normalization to try to get the latent space into a fairly good shape before normalizing. In this experiment, we'll see what happens if we apply some of the regularizers after normalization. Only the unitarity regularizer should need to run beforehand.\n",
    "\n",
    "The model will be the same as the linear output variant from 2.3, i.e. without sigmoid applied to the output.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "The separate, planarity, and anchor regularizers are (approximately) angular. If we move them to run after the explicit normalization, they will be more effective at shaping latent space. We should see lower reconstruction loss overall and a more regular structure in latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db26371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "nbid = '2.4'  # ID for tagging assets\n",
    "nbname = 'Post-norm regularization'\n",
    "experiment_name = f'Ex {nbid}: {nbname}'\n",
    "project = 'ex-color-transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaf0edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup: Logging, Experiment (Modal)\n",
    "import logging\n",
    "\n",
    "import modal\n",
    "\n",
    "from infra.requirements import freeze, project_packages\n",
    "from mini.experiment import Experiment\n",
    "from utils.logging import SimpleLoggingConfig\n",
    "\n",
    "logging_config = (\n",
    "    SimpleLoggingConfig()\n",
    "    .info('notebook', 'utils', 'mini', 'ex_color')\n",
    "    .error('matplotlib.axes')  # Silence warnings about set_aspect\n",
    ")\n",
    "logging_config.apply()\n",
    "\n",
    "# This is the logger for this notebook\n",
    "log = logging.getLogger(f'notebook.{nbid}')\n",
    "\n",
    "run = Experiment(experiment_name, project=project)\n",
    "run.image = modal.Image.debian_slim().pip_install(*freeze(all=True)).add_local_python_source(*project_packages())\n",
    "run.before_each(logging_config.apply)\n",
    "None  # prevent auto-display of this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2028d",
   "metadata": {},
   "source": [
    "## Regularizers\n",
    "\n",
    "Like Ex 2.3:\n",
    "\n",
    "- **Anchor:** pins `red` to $(1,0,0,0)$\n",
    "- **Separate:** angular repulsion to reduce global clumping (applied within each batch)\n",
    "- **Planarity:** pulls vibrant hues to the $[0, 1]$ plane\n",
    "- **Unitarity:** pulls all embeddings to the surface of the unit hypersphere, i.e. it makes the embedding vectors have unit length.\n",
    "\n",
    "Like 2.3, _unitarity_ is applied to the output of the encoder. But _unlike_ 2.3:\n",
    "\n",
    "- All other regularizers are applied _after_ the activations are explitly normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e10c434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from ex_color.loss import Anchor, Separate, Planarity, Unitarity, RegularizerConfig\n",
    "\n",
    "from ex_color.training import TrainingModule\n",
    "\n",
    "RED = (1, 0, 0, 0)\n",
    "\n",
    "ALL_REGULARIZERS = [\n",
    "    RegularizerConfig(\n",
    "        name='reg-unit',\n",
    "        compute_loss_term=Unitarity(),\n",
    "        label_affinities=None,\n",
    "        layer_affinities=['encoder'],\n",
    "    ),\n",
    "    RegularizerConfig(\n",
    "        name='reg-anchor',\n",
    "        compute_loss_term=Anchor(torch.tensor(RED, dtype=torch.float32)),\n",
    "        label_affinities={'red': 1.0},\n",
    "        layer_affinities=['bottleneck'],\n",
    "    ),\n",
    "    RegularizerConfig(\n",
    "        name='reg-separate',\n",
    "        compute_loss_term=Separate(power=100.0, shift=True),\n",
    "        label_affinities=None,\n",
    "        layer_affinities=['bottleneck'],\n",
    "    ),\n",
    "    RegularizerConfig(\n",
    "        name='reg-planar',\n",
    "        compute_loss_term=Planarity(),\n",
    "        label_affinities={'vibrant': 1.0},\n",
    "        layer_affinities=['bottleneck'],\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "import logging\n",
    "from typing import override\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CNColorMLP(nn.Module):\n",
    "    \"\"\"A clamped-and-normalized RGB-to-RGB bottlenecked autoencoder\"\"\"\n",
    "\n",
    "    def __init__(self, n_bottleneck: int):\n",
    "        super().__init__()\n",
    "        # RGB input (3D) → hidden layer → bottleneck → hidden layer → RGB output\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(16, n_bottleneck),\n",
    "        )\n",
    "\n",
    "        self.bottleneck = L2Norm()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_bottleneck, 16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(16, 3),\n",
    "        )\n",
    "\n",
    "    @override  # Overridden to narrow types\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return super().__call__(x)\n",
    "\n",
    "    @override\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Get the bottleneck representation (captured by a hook for regularization)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Normalize\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Decode back to RGB\n",
    "        x = self.decoder(x)\n",
    "        return x if self.training else torch.clamp(x, 0, 1)\n",
    "\n",
    "\n",
    "class L2Norm(nn.Module):\n",
    "    def forward(self, x: Tensor):\n",
    "        return nn.functional.normalize(x, dim=-1)\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    return CNColorMLP(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22376072",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data is the same as last time:\n",
    "- Train: an HSV cube (of RGB values)\n",
    "- Test: an RGB cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de07fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "from ex_color.data.color_cube import ColorCube\n",
    "from ex_color.data.cube_sampler import vibrancy\n",
    "from ex_color.data.cyclic import arange_cyclic\n",
    "from ex_color.labelling import collate_with_generated_labels\n",
    "\n",
    "\n",
    "def prep_data() -> tuple[DataLoader, Tensor]:\n",
    "    \"\"\"\n",
    "    Prepare data for training.\n",
    "\n",
    "    Returns: (train, val)\n",
    "    \"\"\"\n",
    "    hsv_cube = ColorCube.from_hsv(\n",
    "        h=arange_cyclic(step_size=10 / 360),\n",
    "        s=np.linspace(0, 1, 10),\n",
    "        v=np.linspace(0, 1, 10),\n",
    "    )\n",
    "    hsv_tensor = torch.tensor(hsv_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "    vibrancy_tensor = torch.tensor(vibrancy(hsv_cube).flatten(), dtype=torch.float32)\n",
    "    hsv_dataset = TensorDataset(hsv_tensor, vibrancy_tensor)\n",
    "\n",
    "    labeller = partial(\n",
    "        collate_with_generated_labels,\n",
    "        soft=False,  # Use binary labels (stochastic) to simulate the labelling of internet text\n",
    "        scale={'red': 0.5, 'vibrant': 0.5},\n",
    "    )\n",
    "    # Desaturated and dark colors are over-represented in the cube, so we use a weighted sampler to balance them out\n",
    "    hsv_loader = DataLoader(\n",
    "        hsv_dataset,\n",
    "        batch_size=64,\n",
    "        num_workers=2,\n",
    "        sampler=WeightedRandomSampler(\n",
    "            weights=hsv_cube.bias.flatten().tolist(),\n",
    "            num_samples=len(hsv_dataset),\n",
    "            replacement=True,\n",
    "        ),\n",
    "        collate_fn=labeller,\n",
    "    )\n",
    "\n",
    "    rgb_cube = ColorCube.from_rgb(\n",
    "        r=np.linspace(0, 1, 8),\n",
    "        g=np.linspace(0, 1, 8),\n",
    "        b=np.linspace(0, 1, 8),\n",
    "    )\n",
    "    rgb_tensor = torch.tensor(rgb_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "    return hsv_loader, rgb_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fecb906",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Like in Ex 2.2, the model is trained with PyTorch Lightning, with regularizers applied as custom hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c22398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 957.7 li.fa.ut.se:Seed set to 0\n",
      "I 957.8 ex.se: PyTorch set to deterministic mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 958.0 li.py.ut.ra:GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 958.0 li.py.ut.ra:TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 958.0 li.py.ut.ra:HPU available: False, using: 0 HPUs\n",
      "max_steps: 3001, hsv_loader length: 57\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250902_125928-mgn90z8d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/z0r/ex-color-transformer/runs/mgn90z8d' target=\"_blank\">Ex 2.4: Post-norm regularization</a></strong> to <a href='https://wandb.ai/z0r/ex-color-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/z0r/ex-color-transformer' target=\"_blank\">https://wandb.ai/z0r/ex-color-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/z0r/ex-color-transformer/runs/mgn90z8d' target=\"_blank\">https://wandb.ai/z0r/ex-color-transformer/runs/mgn90z8d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"width: 100%; padding: 5px 0; font-family: monospace\">\n",
       "  <div style=\"position: relative; height: calc(1em * 5/3); width: 100%; margin-bottom: 2em\">\n",
       "    <div style=\"position: absolute; top: 100%; height: 3.5px; left: 0; width: 100%; background: linear-gradient(to right, var(--h1) 0.0%, var(--h0) 2.0%, var(--h0) 4.1%, var(--h0) 6.1%, var(--h0) 8.2%, var(--h0) 10.2%, var(--h0) 12.2%, var(--h0) 14.3%, var(--h0) 16.3%, var(--h0) 18.4%, var(--h0) 20.4%, var(--h0) 22.4%, var(--h0) 24.5%, var(--h0) 26.5%, var(--h0) 28.6%, var(--h0) 30.6%, var(--h0) 32.7%, var(--h0) 34.7%, var(--h1) 36.7%, var(--h0) 38.8%, var(--h0) 40.8%, var(--h0) 42.9%, var(--h0) 44.9%, var(--h0) 46.9%, var(--h0) 49.0%, var(--h0) 51.0%, var(--h0) 53.1%, var(--h0) 55.1%, var(--h0) 57.1%, var(--h0) 59.2%, var(--h0) 61.2%, var(--h0) 63.3%, var(--h0) 65.3%, var(--h0) 67.3%, var(--h0) 69.4%, var(--h0) 71.4%, var(--h0) 73.5%, var(--h1) 75.5%, var(--h0) 77.6%, var(--h0) 79.6%, var(--h0) 81.6%, var(--h0) 83.7%, var(--h0) 85.7%, var(--h0) 87.8%, var(--h0) 89.8%, var(--h0) 91.8%, var(--h0) 93.9%, var(--h0) 95.9%, var(--h0) 98.0%, var(--h1) 100.0%);--h0:color(from currentColor srgb r g b / 0.11);--h1:color(from currentColor srgb r g b / 0.44)\"></div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 1.9%; border-left: 0.5px solid currentColor\">0.0245</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 13.3%; border-left: 0.5px solid currentColor\">0.0062</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 24.7%; border-left: 0.5px solid currentColor\">0.0032</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 36.1%; border-left: 0.5px solid currentColor\">0.0070</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 47.5%; border-left: 0.5px solid currentColor\">0.0060</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 58.8%; border-left: 0.5px solid currentColor\">0.0018</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 70.2%; border-left: 0.5px solid currentColor\">0.0020</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; left: 81.6%; border-left: 0.5px solid currentColor\">0.0002</div>\n",
       "    <div style=\"position: absolute; top: 100%; font-size: 70%; padding: 3px 2px 0; right: 0.0%; border-right: 0.5px solid currentColor\">0.0000</div>\n",
       "    <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px)\">\n",
       "      <div style=\"border-left: 4px solid transparent; border-right: 4px solid transparent; border-bottom: 4px solid currentColor\"></div>\n",
       "    </div>\n",
       "    <div style=\"position: absolute; height: 100%; width: 100.0%; background-color: color(from currentColor srgb r g b / 0.1); border-bottom: 1px solid currentColor\"></div>\n",
       "    <div style=\"position: absolute; width: 100%; height: 100%; text-align: center; line-height: calc((1em * 5/3) / 0.9); font-size: 90%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5)\"><b>Training</b>: 100.0% [3001/3001] [<b>00:16</b>/<00:00, 186.49 it/s]</div>\n",
       "  </div>\n",
       "  <div style=\"display: grid; grid-template-columns: repeat(2, minmax(80px, 1fr)); gap: 5px 0px; width: 100%; margin: 1em 0; font-size: 0.85em\">\n",
       "    <div style=\"font-weight: bold; border-bottom: 0.5px solid currentColor; padding: 2px 10px; text-align: left; overflow: hidden; text-overflow: ellipsis; white-space: nowrap\">v_num</div>\n",
       "    <div style=\"font-weight: bold; border-bottom: 0.5px solid currentColor; padding: 2px 10px; text-align: left; overflow: hidden; text-overflow: ellipsis; white-space: nowrap\">train_loss</div>\n",
       "    <div style=\"padding: 2px 10px; text-align: left; overflow: hidden; text-overflow: ellipsis; white-space: nowrap\">0z8d</div>\n",
       "    <div style=\"padding: 2px 10px; text-align: left; overflow: hidden; text-overflow: ellipsis; white-space: nowrap\">3.036e-05</div>\n",
       "  </div>\n",
       "</div>"
      ],
      "text/plain": [
       "Training: 100.0% [3001/3001]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting phase: Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_steps=3001` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 975.5 li.py.ut.ra:`Trainer.fit` stopped: `max_steps=3001` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▄▆▆▃▄▃▄▂▃▆▆▄▃▃▃▃▄▃▂▂▂▃▃▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_recon</td><td>█▂▂▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_reg-anchor</td><td>▁▁▁█▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>train_reg-planar</td><td>▁▅▁█▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>train_reg-separate</td><td>█▆█▆█▆▆▆▄▅▃▃▄▆▃▄▁▂▅▂▃▁▃▄▄▂▁▂▃▁▂▂▄▁▃▂▃▂▃▄</td></tr><tr><td>train_reg-unit</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>52</td></tr><tr><td>train_loss</td><td>3e-05</td></tr><tr><td>train_recon</td><td>3e-05</td></tr><tr><td>train_reg-anchor</td><td>0</td></tr><tr><td>train_reg-planar</td><td>0.05332</td></tr><tr><td>train_reg-separate</td><td>0.43601</td></tr><tr><td>train_reg-unit</td><td>0.00924</td></tr><tr><td>trainer/global_step</td><td>2999</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Ex 2.4: Post-norm regularization</strong> at: <a href='https://wandb.ai/z0r/ex-color-transformer/runs/mgn90z8d' target=\"_blank\">https://wandb.ai/z0r/ex-color-transformer/runs/mgn90z8d</a><br> View project at: <a href='https://wandb.ai/z0r/ex-color-transformer' target=\"_blank\">https://wandb.ai/z0r/ex-color-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250902_125928-mgn90z8d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from ex_color.inference import InferenceModule\n",
    "from ex_color.intervention.intervention import InterventionConfig\n",
    "\n",
    "\n",
    "# @run.thither(env={'WANDB_API_KEY': wandb.Api().api_key})\n",
    "async def train(\n",
    "    dopesheet: Dopesheet,\n",
    "    regularizers: list[RegularizerConfig],\n",
    ") -> CNColorMLP:\n",
    "    \"\"\"Train the model with the given dopesheet and variant.\"\"\"\n",
    "    import lightning as L\n",
    "    from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "    from ex_color.seed import set_deterministic_mode\n",
    "\n",
    "    from utils.progress.lightning import LightningProgress\n",
    "\n",
    "    log.info(f'Training with: {[r.name for r in regularizers]}')\n",
    "\n",
    "    seed = 0\n",
    "    set_deterministic_mode(seed)\n",
    "\n",
    "    hsv_loader, _ = prep_data()\n",
    "\n",
    "    model = make_model()\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log.debug(f'Model initialized with {total_params:,} trainable parameters.')\n",
    "\n",
    "    training_module = TrainingModule(model, dopesheet, torch.nn.MSELoss(), regularizers)\n",
    "\n",
    "    logger = WandbLogger(experiment_name, project='ex-color-transformer')\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_steps=len(dopesheet),\n",
    "        callbacks=[\n",
    "            LightningProgress(),\n",
    "        ],\n",
    "        enable_checkpointing=False,\n",
    "        enable_model_summary=False,\n",
    "        # enable_progress_bar=True,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    print(f'max_steps: {len(dopesheet)}, hsv_loader length: {len(hsv_loader)}')\n",
    "\n",
    "    # Train the model\n",
    "    try:\n",
    "        trainer.fit(training_module, hsv_loader)\n",
    "    finally:\n",
    "        wandb.finish()\n",
    "    # This is only a small model, so it's OK to return it rather than storing and loading a checkpoint remotely\n",
    "    return model\n",
    "\n",
    "\n",
    "async with run():\n",
    "    model = await train(Dopesheet.from_csv(f'./ex-{nbid}-dopesheet.csv'), ALL_REGULARIZERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928e404",
   "metadata": {},
   "source": [
    "The charts and loss values from training look much the same as last time.\n",
    "- Roughly the same shape overall\n",
    "- All loss values slightly higher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9688ac3",
   "metadata": {},
   "source": [
    "## Inference ('test-time')\n",
    "\n",
    "We wrap the model that we trained above in an `InferenceModule`, which knows how to apply our interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76d2de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @run.thither\n",
    "async def infer(\n",
    "    model: CNColorMLP,\n",
    "    interventions: list[InterventionConfig],\n",
    "    test_data: Tensor,\n",
    ") -> Tensor:\n",
    "    \"\"\"Run inference with the given model and interventions.\"\"\"\n",
    "    import lightning as L\n",
    "\n",
    "    inference_module = InferenceModule(model, interventions)\n",
    "    trainer = L.Trainer(\n",
    "        enable_checkpointing=False,\n",
    "        enable_model_summary=False,\n",
    "        enable_progress_bar=True,\n",
    "    )\n",
    "    reconstructed_colors_batches = trainer.predict(\n",
    "        inference_module,\n",
    "        DataLoader(\n",
    "            TensorDataset(test_data.reshape((-1, 3))),\n",
    "            batch_size=64,\n",
    "            collate_fn=lambda batch: torch.stack([row[0] for row in batch], 0),\n",
    "        ),\n",
    "    )\n",
    "    assert reconstructed_colors_batches is not None\n",
    "    # Flatten the list of batches to a single list of tensors\n",
    "    reconstructed_colors = [item for batch in reconstructed_colors_batches for item in batch]\n",
    "    # Reshape to match input\n",
    "    return torch.cat(reconstructed_colors).reshape(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6940577",
   "metadata": {},
   "source": [
    "Let's see how well the model reconstructs colors _without_ any interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4381b363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<picture>\n",
       "  <source srcset=\"large-assets/ex-2.4-true-colors.dark.png?v=hs7GkGaaYfKtu3fb6qCRDH9AIQ6YGRebtY7T17yz0Hw\" media=\"(prefers-color-scheme: dark)\" />\n",
       "  <source srcset=\"large-assets/ex-2.4-true-colors.png?v=hs7GkGaaYfKtu3fb6qCRDH9AIQ6YGRebtY7T17yz0Hw\" media=\"(prefers-color-scheme: light)\" />\n",
       "  <img src=\"large-assets/ex-2.4-true-colors.png?v=hs7GkGaaYfKtu3fb6qCRDH9AIQ6YGRebtY7T17yz0Hw\" alt=\"Plot showing four slices of the HSV cube, titled &quot;True colors · V vs H by S&quot;. Each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. The first slice shows a grayscale gradient from black to white; the last shows the fully-saturated color spectrum.\" />\n",
       "</picture>"
      ],
      "text/plain": [
       "Plot showing four slices of the HSV cube, titled \"True colors · V vs H by S\". Each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. The first slice shows a grayscale gradient from black to white; the last shows the fully-saturated color spectrum."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import importlib\n",
    "import utils.nb\n",
    "import utils.plt\n",
    "\n",
    "importlib.reload(utils.nb)\n",
    "importlib.reload(utils.plt)\n",
    "\n",
    "from ex_color.vis import plot_colors, plot_loss_lines\n",
    "from utils.nb import displayer_mpl\n",
    "\n",
    "\n",
    "hsv_cube = ColorCube.from_hsv(\n",
    "    h=arange_cyclic(step_size=1 / 24),\n",
    "    s=np.linspace(0, 1, 4),\n",
    "    v=np.linspace(0, 1, 8),\n",
    ").permute('svh')\n",
    "x_hsv = torch.tensor(hsv_cube.rgb_grid, dtype=torch.float32)\n",
    "\n",
    "hd_hsv_cube = ColorCube.from_hsv(\n",
    "    h=arange_cyclic(step_size=1 / 240),\n",
    "    s=np.linspace(0, 1, 20),\n",
    "    v=np.linspace(0, 1, 20),\n",
    ").permute('svh')\n",
    "hd_x_hsv = torch.tensor(hd_hsv_cube.rgb_grid, dtype=torch.float32)\n",
    "\n",
    "rgb_cube = ColorCube.from_rgb(\n",
    "    r=np.linspace(0, 1, 20),\n",
    "    g=np.linspace(0, 1, 20),\n",
    "    b=np.linspace(0, 1, 20),\n",
    ")\n",
    "x_rgb = torch.tensor(rgb_cube.rgb_grid, dtype=torch.float32)\n",
    "\n",
    "with displayer_mpl(\n",
    "    f'large-assets/ex-{nbid}-true-colors.png',\n",
    "    alt_text=\"\"\"Plot showing four slices of the HSV cube, titled \"{title}\". Each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. The first slice shows a grayscale gradient from black to white; the last shows the fully-saturated color spectrum.\"\"\",\n",
    ") as show:\n",
    "    show(lambda: plot_colors(hsv_cube, title='True colors', colors=x_hsv.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14da6f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<picture>\n",
       "  <source srcset=\"large-assets/ex-2.4-pred-colors-no-intervention.dark.png?v=uXMZU1y0I1YEZCmfhIO6KlzJrGcEJ-kOmy6hMRx96Mk\" media=\"(prefers-color-scheme: dark)\" />\n",
       "  <source srcset=\"large-assets/ex-2.4-pred-colors-no-intervention.png?v=uXMZU1y0I1YEZCmfhIO6KlzJrGcEJ-kOmy6hMRx96Mk\" media=\"(prefers-color-scheme: light)\" />\n",
       "  <img src=\"large-assets/ex-2.4-pred-colors-no-intervention.png?v=uXMZU1y0I1YEZCmfhIO6KlzJrGcEJ-kOmy6hMRx96Mk\" alt=\"Plot showing four slices of the HSV cube, titled &quot;Predicted colors without intervention · V vs H by S&quot;. Nominally, each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. Each color value is represented as a square patch of that color. The outer portion of the patches shows the color as reconstructed by the model; the inner portion shows the true (input) color. The reconstructed and true colors agree fairly well, but some slight differences are visible; for example, &quot;white&quot; is slightly gray, and many of the fully-saturated colors are less saturated than they should be.\" />\n",
       "</picture>"
      ],
      "text/plain": [
       "Plot showing four slices of the HSV cube, titled \"Predicted colors without intervention · V vs H by S\". Nominally, each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. Each color value is represented as a square patch of that color. The outer portion of the patches shows the color as reconstructed by the model; the inner portion shows the true (input) color. The reconstructed and true colors agree fairly well, but some slight differences are visible; for example, \"white\" is slightly gray, and many of the fully-saturated colors are less saturated than they should be."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<picture>\n",
       "  <source srcset=\"large-assets/ex-2.4-loss-colors-no-intervention.dark.png?v=FUV6RJpC7hKk35GkoYIHfKLlZecI2M7Mt7lZDxRNZPI\" media=\"(prefers-color-scheme: dark)\" />\n",
       "  <source srcset=\"large-assets/ex-2.4-loss-colors-no-intervention.png?v=FUV6RJpC7hKk35GkoYIHfKLlZecI2M7Mt7lZDxRNZPI\" media=\"(prefers-color-scheme: light)\" />\n",
       "  <img src=\"large-assets/ex-2.4-loss-colors-no-intervention.png?v=FUV6RJpC7hKk35GkoYIHfKLlZecI2M7Mt7lZDxRNZPI\" alt=\"Line chart showing loss per color, titled &quot;&quot;. Y-axis: mean square error, ranging from zero to 0.0011. X-axis: hue. The range of loss values is small, but there are two notable peaks at all primary and secondary colors (red, yellow, green, etc.).\" />\n",
       "</picture>"
      ],
      "text/plain": [
       "Line chart showing loss per color, titled \"\". Y-axis: mean square error, ranging from zero to 0.0011. X-axis: hue. The range of loss values is small, but there are two notable peaks at all primary and secondary colors (red, yellow, green, etc.)."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max loss: 0.0011\n",
      "Median MSE: 1.5e-05\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "interventions = []\n",
    "y_hsv = await infer(model, interventions, x_hsv)\n",
    "hd_y_hsv = await infer(model, interventions, hd_x_hsv)\n",
    "clear_output()\n",
    "\n",
    "with displayer_mpl(\n",
    "    f'large-assets/ex-{nbid}-pred-colors-no-intervention.png',\n",
    "    alt_text=\"\"\"Plot showing four slices of the HSV cube, titled \"{title}\". Nominally, each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. Each color value is represented as a square patch of that color. The outer portion of the patches shows the color as reconstructed by the model; the inner portion shows the true (input) color. The reconstructed and true colors agree fairly well, but some slight differences are visible; for example, \"white\" is slightly gray, and many of the fully-saturated colors are less saturated than they should be.\"\"\",\n",
    ") as show:\n",
    "    show(\n",
    "        lambda: plot_colors(\n",
    "            hsv_cube,\n",
    "            title='Predicted colors without intervention',\n",
    "            colors=y_hsv.numpy(),\n",
    "            colors_compare=x_hsv.numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "per_color_loss = F.mse_loss(hd_y_hsv, hd_x_hsv, reduction='none').mean(dim=-1)\n",
    "max_loss = per_color_loss.max().item()\n",
    "median_loss = per_color_loss.median().item()\n",
    "with displayer_mpl(\n",
    "    f'large-assets/ex-{nbid}-loss-colors-no-intervention.png',\n",
    "    alt_text=f\"\"\"Line chart showing loss per color, titled \"{{title}}\". Y-axis: mean square error, ranging from zero to {max_loss:.2g}. X-axis: hue. The range of loss values is small, but there are two notable peaks at all primary and secondary colors (red, yellow, green, etc.).\"\"\",\n",
    ") as show:\n",
    "    show(\n",
    "        lambda: plot_loss_lines(\n",
    "            hd_hsv_cube,\n",
    "            title='No intervention',\n",
    "            loss=per_color_loss.numpy().reshape(hd_hsv_cube.shape),\n",
    "            ylabel='Mean square error',\n",
    "        )\n",
    "    )\n",
    "print(f'Max loss: {max_loss:.2g}')\n",
    "print(f'Median MSE: {median_loss:.2g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c6e78",
   "metadata": {},
   "source": [
    "That's a good baseline: visually similar to 2.3 (linear variant), with a slightly lower maximum reconstruction loss. One notable feature is that the primary and secondary colors are more regular in their loss spikes: they all have roughly the same peak loss, whereas last time green and blue were worse than the others. I expect this means latent space will be slightly more globally regular.\n",
    "\n",
    "There's an odd and oddly straight line for a set of very dark colors — maybe black — at around 4e-4 (compared to the median of 1.5e-5). I'm not sure what is happening there, but these numbers are all pretty small so I'll ignore it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89ec7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture encoder activations (latents) without interventions\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from ex_color.inference import InferenceModule\n",
    "\n",
    "\n",
    "# Build a tiny helper that runs predict while capturing latents from 'encoder'\n",
    "async def infer_with_latent_capture(\n",
    "    model: CNColorMLP, interventions: list[InterventionConfig], test_data: Tensor, layer_name: str = 'bottleneck'\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    module = InferenceModule(model, interventions, capture_layers=[layer_name])\n",
    "    import lightning as L\n",
    "\n",
    "    trainer = L.Trainer(enable_checkpointing=False, enable_model_summary=False, enable_progress_bar=False)\n",
    "    batches = trainer.predict(\n",
    "        module,\n",
    "        DataLoader(\n",
    "            TensorDataset(test_data.reshape((-1, 3))),\n",
    "            batch_size=64,\n",
    "            collate_fn=lambda batch: torch.stack([row[0] for row in batch], 0),\n",
    "        ),\n",
    "    )\n",
    "    assert batches is not None\n",
    "    preds = [item for batch in batches for item in batch]\n",
    "    y = torch.cat(preds).reshape(test_data.shape)\n",
    "    # Read captured activations as a flat [N, D] tensor\n",
    "    latents = module.read_captured(layer_name)\n",
    "    return y, latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b4aa8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<picture>\n",
       "  <source srcset=\"large-assets/ex-2.4-latents-no-intervention.dark.png?v=taIBfeXnYRob2WV_lqATSau6RBg_Qh160Hl9n2cOHpc\" media=\"(prefers-color-scheme: dark)\" />\n",
       "  <source srcset=\"large-assets/ex-2.4-latents-no-intervention.png?v=taIBfeXnYRob2WV_lqATSau6RBg_Qh160Hl9n2cOHpc\" media=\"(prefers-color-scheme: light)\" />\n",
       "  <img src=\"large-assets/ex-2.4-latents-no-intervention.png?v=taIBfeXnYRob2WV_lqATSau6RBg_Qh160Hl9n2cOHpc\" alt=\"Three spherical plots, titled &quot;Latents · no intervention&quot;. Each plot shows a vibrant collection of colored circles or balls scattered over the surface of a black sphere. The first plot has the appearance of a color wheel, with the full set of vibrant colors around the rim (like a rainbow), varying to black in the center. The other plots show different views of the same sphere, with hue varying across the equator and tone varying from top to bottom, and red in the center. Each ball shows the reconstructed color, with a dot in the center showing the true (input) color. In this plot the true and reconstructor colors agree fairly well, but slight differences can be seen if you look closely.\" />\n",
       "</picture>"
      ],
      "text/plain": [
       "Three spherical plots, titled \"Latents · no intervention\". Each plot shows a vibrant collection of colored circles or balls scattered over the surface of a black sphere. The first plot has the appearance of a color wheel, with the full set of vibrant colors around the rim (like a rainbow), varying to black in the center. The other plots show different views of the same sphere, with hue varying across the equator and tone varying from top to bottom, and red in the center. Each ball shows the reconstructed color, with a dot in the center showing the true (input) color. In this plot the true and reconstructor colors agree fairly well, but slight differences can be seen if you look closely."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "from ex_color.vis import plot_latent_grid_3d\n",
    "\n",
    "y_rgb, h_rgb = await infer_with_latent_capture(model, [], x_rgb, 'bottleneck')\n",
    "clear_output()\n",
    "\n",
    "with displayer_mpl(\n",
    "    f'large-assets/ex-{nbid}-latents-no-intervention.png',\n",
    "    alt_text=\"\"\"Three spherical plots, titled \"{title}\". Each plot shows a vibrant collection of colored circles or balls scattered over the surface of a black sphere. The first plot has the appearance of a color wheel, with the full set of vibrant colors around the rim (like a rainbow), varying to black in the center. The other plots show different views of the same sphere, with hue varying across the equator and tone varying from top to bottom, and red in the center. Each ball shows the reconstructed color, with a dot in the center showing the true (input) color. In this plot the true and reconstructor colors agree fairly well, but slight differences can be seen if you look closely.\"\"\",\n",
    ") as show:\n",
    "    show(\n",
    "        lambda theme: plot_latent_grid_3d(\n",
    "            h_rgb,\n",
    "            y_rgb,\n",
    "            x_rgb,\n",
    "            title='Latents · no intervention',\n",
    "            dims=[(1, 0, 2), (1, 2, 0), (1, 3, 0)],\n",
    "            dot_radius=10,\n",
    "            theme=theme,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68adcf20",
   "metadata": {},
   "source": [
    "Ah, here we see why the peak losses for primary and secondary colors were more uniform: the middle plot shows that they have been positioned more uniformly around the hub of the dome, i.e. they are more planar. That suggests that the planarity regularizer was more effective, having been applied after the normalization. Or rather, given that the overall planarity loss was _not_ lower than last time, perhaps it's just more uniform (less variance).\n",
    "\n",
    "Also: the middle plot in 2.3 had a \"tail\" of dark colors extending down from the middle. That's absent in these plots. On dimensions $(1,2)$, the latent embeddings appear cleanly hemispherical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4f1dc",
   "metadata": {},
   "source": [
    "## Suppression\n",
    "\n",
    "Now that we have our model, let's try suppressing _red_. We'll use the `Suppression` function developed in [Ex 2.1](./ex-2.1-intervention-lobe.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b8cecb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<picture>\n",
       "  <source srcset=\"large-assets/ex-2.4-pred-colors-suppression.dark.png?v=vZFIeTWHMOWfHsnyzIAfShjiF93c_Rht3Xtgmm7qW_A\" media=\"(prefers-color-scheme: dark)\" />\n",
       "  <source srcset=\"large-assets/ex-2.4-pred-colors-suppression.png?v=vZFIeTWHMOWfHsnyzIAfShjiF93c_Rht3Xtgmm7qW_A\" media=\"(prefers-color-scheme: light)\" />\n",
       "  <img src=\"large-assets/ex-2.4-pred-colors-suppression.png?v=vZFIeTWHMOWfHsnyzIAfShjiF93c_Rht3Xtgmm7qW_A\" alt=\"Plot showing four slices of the HSV cube, titled &quot;Predicted colors with suppression · V vs H by S&quot;. Nominally, each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. Each color value is represented as a square patch of that color. The outer portion of the patches shows the color as reconstructed by the model; the inner portion shows the true (input) color. The reconstructed and true colors agree fairly well, but &quot;red&quot; and nearby colors are clearly different: red itself appears as middle-gray, and the surrounding colors up to orange and pink look washed out. &quot;Red-orange&quot; actually appears to be green, moreso even than yellow (which is geometrically closer to green).\" />\n",
       "</picture>"
      ],
      "text/plain": [
       "Plot showing four slices of the HSV cube, titled \"Predicted colors with suppression · V vs H by S\". Nominally, each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. Each color value is represented as a square patch of that color. The outer portion of the patches shows the color as reconstructed by the model; the inner portion shows the true (input) color. The reconstructed and true colors agree fairly well, but \"red\" and nearby colors are clearly different: red itself appears as middle-gray, and the surrounding colors up to orange and pink look washed out. \"Red-orange\" actually appears to be green, moreso even than yellow (which is geometrically closer to green)."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<picture>\n",
       "  <source srcset=\"large-assets/ex-2.4-loss-colors-suppression.dark.png?v=C11dCgCYHzyqbrZp30YGC4ATuBzO38TU8AUYoSXMRzc\" media=\"(prefers-color-scheme: dark)\" />\n",
       "  <source srcset=\"large-assets/ex-2.4-loss-colors-suppression.png?v=C11dCgCYHzyqbrZp30YGC4ATuBzO38TU8AUYoSXMRzc\" media=\"(prefers-color-scheme: light)\" />\n",
       "  <img src=\"large-assets/ex-2.4-loss-colors-suppression.png?v=C11dCgCYHzyqbrZp30YGC4ATuBzO38TU8AUYoSXMRzc\" alt=\"Line chart showing loss per color, titled &quot;&quot;. Y-axis: mean square error, ranging from zero to 0.16. X-axis: hue. There is a significant peak at red at either end of the X-axis, gradually sloping down to lower loss values near yellow and pink. Two very small peaks are at green and blue (apparently around 1% of the height of the peaks at red).\" />\n",
       "</picture>"
      ],
      "text/plain": [
       "Line chart showing loss per color, titled \"\". Y-axis: mean square error, ranging from zero to 0.16. X-axis: hue. There is a significant peak at red at either end of the X-axis, gradually sloping down to lower loss values near yellow and pink. Two very small peaks are at green and blue (apparently around 1% of the height of the peaks at red)."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max loss: 0.16\n",
      "Median MSE: 2.1e-05\n"
     ]
    }
   ],
   "source": [
    "from math import cos, pi\n",
    "\n",
    "import torch\n",
    "\n",
    "from ex_color.intervention import BoundedFalloff, InterventionConfig, Suppression\n",
    "from ex_color.vis import plot_loss_lines\n",
    "\n",
    "\n",
    "suppression = Suppression(\n",
    "    torch.tensor(RED, dtype=torch.float32),  # Constant!\n",
    "    BoundedFalloff(\n",
    "        0,  # within 60°\n",
    "        1,  # completely squash fully-aligned vectors\n",
    "        2,  # soft rim, sharp hub\n",
    "    ),\n",
    ")\n",
    "\n",
    "interventions = [InterventionConfig(suppression, ['bottleneck'])]\n",
    "y_hsv = await infer(model, interventions, x_hsv)\n",
    "hd_y_hsv = await infer(model, interventions, hd_x_hsv)\n",
    "clear_output()\n",
    "\n",
    "with displayer_mpl(\n",
    "    f'large-assets/ex-{nbid}-pred-colors-suppression.png',\n",
    "    alt_text=\"\"\"Plot showing four slices of the HSV cube, titled \"{title}\". Nominally, each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. Each color value is represented as a square patch of that color. The outer portion of the patches shows the color as reconstructed by the model; the inner portion shows the true (input) color. The reconstructed and true colors agree fairly well, but \"red\" and nearby colors are clearly different: red itself appears as middle-gray, and the surrounding colors up to orange and pink look washed out. \"Red-orange\" actually appears to be green, moreso even than yellow (which is geometrically closer to green).\"\"\",\n",
    ") as show:\n",
    "    show(\n",
    "        lambda: plot_colors(\n",
    "            hsv_cube,\n",
    "            title='Predicted colors with suppression',\n",
    "            colors=y_hsv.numpy(),\n",
    "            colors_compare=x_hsv.numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "per_color_loss = F.mse_loss(hd_y_hsv, hd_x_hsv, reduction='none').mean(dim=-1)\n",
    "max_loss = per_color_loss.max().item()\n",
    "median_loss = per_color_loss.median().item()\n",
    "with displayer_mpl(\n",
    "    f'large-assets/ex-{nbid}-loss-colors-suppression.png',\n",
    "    alt_text=f\"\"\"Line chart showing loss per color, titled \"{{title}}\". Y-axis: mean square error, ranging from zero to {max_loss:.2g}. X-axis: hue. There is a significant peak at red at either end of the X-axis, gradually sloping down to lower loss values near yellow and pink. Two very small peaks are at green and blue (apparently around 1% of the height of the peaks at red).\"\"\",\n",
    ") as show:\n",
    "    show(\n",
    "        lambda: plot_loss_lines(\n",
    "            hd_hsv_cube,\n",
    "            title='Suppression',\n",
    "            loss=per_color_loss.numpy().reshape(hd_hsv_cube.shape),\n",
    "            ylabel='Mean square error',\n",
    "        )\n",
    "    )\n",
    "print(f'Max loss: {max_loss:.2g}')\n",
    "print(f'Median MSE: {median_loss:.2g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35e4b3",
   "metadata": {},
   "source": [
    "This looks great: the reconstructed colors look similar to the intervened ones in in 2.3, but the loss curves are even smoother: they show less crossing-over-each-other near _red_ — which I think should mean that the intervention has introduced less high-frequency perturbation, and should have fewer unwanted downstream effects. They also rise steadily to a peak at red, whereas the ones in 2.3 tended to flatten out near the top. The peak is what we should expect, given that the falloff function is quadratic. I think it may be that these results are sharper because _red_ is more precicely aligned with $1,0,0,0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ec409f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<picture>\n",
       "  <source srcset=\"large-assets/ex-2.4-latents-suppression.dark.png?v=4YC0lsfXmgMSEBwxkKs7rQFn0f7aV3LPmQ7q7X-ZH3o\" media=\"(prefers-color-scheme: dark)\" />\n",
       "  <source srcset=\"large-assets/ex-2.4-latents-suppression.png?v=4YC0lsfXmgMSEBwxkKs7rQFn0f7aV3LPmQ7q7X-ZH3o\" media=\"(prefers-color-scheme: light)\" />\n",
       "  <img src=\"large-assets/ex-2.4-latents-suppression.png?v=4YC0lsfXmgMSEBwxkKs7rQFn0f7aV3LPmQ7q7X-ZH3o\" alt=\"Three spherical plots, titled &quot;Latents · suppression&quot;. Each plot shows a vibrant collection of colored circles or balls scattered over the surface of a black sphere. The first plot has the appearance of a partial color wheel, with  vibrant colors around the rim (like a rainbow), with with a conspicuously absent space at the top where &quot;red&quot; should be. The other plots show different views of the same sphere, with hue varying across the equator and tone varying from top to bottom, and warm-but-not-red colors in the center (where &quot;red&quot; should be). Each ball shows the reconstructed color, with a dot in the center showing the true (input) color. The true and reconstructor colors agree fairly well, even for the warmer colors. &quot;Red&quot; and nearby colors are in fact not visible, being buried somewhere inside the sphere.\" />\n",
       "</picture>"
      ],
      "text/plain": [
       "Three spherical plots, titled \"Latents · suppression\". Each plot shows a vibrant collection of colored circles or balls scattered over the surface of a black sphere. The first plot has the appearance of a partial color wheel, with  vibrant colors around the rim (like a rainbow), with with a conspicuously absent space at the top where \"red\" should be. The other plots show different views of the same sphere, with hue varying across the equator and tone varying from top to bottom, and warm-but-not-red colors in the center (where \"red\" should be). Each ball shows the reconstructed color, with a dot in the center showing the true (input) color. The true and reconstructor colors agree fairly well, even for the warmer colors. \"Red\" and nearby colors are in fact not visible, being buried somewhere inside the sphere."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "from ex_color.vis import plot_latent_grid_3d, ConicalAnnotation\n",
    "\n",
    "# Capture latents with suppression\n",
    "y_rgb, h_rgb = await infer_with_latent_capture(model, interventions, x_rgb, 'bottleneck')\n",
    "clear_output()\n",
    "\n",
    "with displayer_mpl(\n",
    "    f'large-assets/ex-{nbid}-latents-suppression.png',\n",
    "    alt_text=\"\"\"Three spherical plots, titled \"{title}\". Each plot shows a vibrant collection of colored circles or balls scattered over the surface of a black sphere. The first plot has the appearance of a partial color wheel, with  vibrant colors around the rim (like a rainbow), with with a conspicuously absent space at the top where \"red\" should be. The other plots show different views of the same sphere, with hue varying across the equator and tone varying from top to bottom, and warm-but-not-red colors in the center (where \"red\" should be). Each ball shows the reconstructed color, with a dot in the center showing the true (input) color. The true and reconstructor colors agree fairly well, even for the warmer colors. \"Red\" and nearby colors are in fact not visible, being buried somewhere inside the sphere.\"\"\",\n",
    ") as show:\n",
    "    show(\n",
    "        lambda theme: plot_latent_grid_3d(\n",
    "            h_rgb,\n",
    "            y_rgb,\n",
    "            x_rgb,\n",
    "            title='Latents · suppression',\n",
    "            dims=[(1, 0, 2), (1, 2, 0), (1, 3, 0)],\n",
    "            dot_radius=10,\n",
    "            theme=theme,\n",
    "            annotations=[\n",
    "                ConicalAnnotation(\n",
    "                    RED,\n",
    "                    2 * (np.pi / 2 - suppression.falloff.a),  # type: ignore\n",
    "                    color=theme.val('black', dark='#fffa'),\n",
    "                    linewidth=theme.val(0.5, dark=1),\n",
    "                    dashes=theme.val((8, 8), dark=(4, 4)),\n",
    "                    gapcolor=theme.val('#fff4', dark='#0004'),\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e4215",
   "metadata": {},
   "source": [
    "Again, the effect of suppression on the latent space looks really good — visually, around as good as 2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8eb019",
   "metadata": {},
   "source": [
    "## Repulsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<picture>\n",
       "  <source srcset=\"large-assets/ex-2.4-pred-colors-repulsion.dark.png?v=Xg0I7PqbQQ8RDt9z8SJIQbkNEyEogc7WD0zNuPtYwCc\" media=\"(prefers-color-scheme: dark)\" />\n",
       "  <source srcset=\"large-assets/ex-2.4-pred-colors-repulsion.png?v=Xg0I7PqbQQ8RDt9z8SJIQbkNEyEogc7WD0zNuPtYwCc\" media=\"(prefers-color-scheme: light)\" />\n",
       "  <img src=\"large-assets/ex-2.4-pred-colors-repulsion.png?v=Xg0I7PqbQQ8RDt9z8SJIQbkNEyEogc7WD0zNuPtYwCc\" alt=\"Plot showing four slices of the HSV cube, titled &quot;Predicted colors with repulsion · V vs H by S&quot;. Nominally, each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. Each color value is represented as a square patch of that color. The outer portion of the patches shows the color as reconstructed by the model; the inner portion shows the true (input) color. The reconstructed and true colors agree fairly well, but &quot;red&quot; and nearby colors are clearly different, and different again from how the suppression intervention looked: &quot;red&quot; itself appears as pink or hot pink, and the surrounding colors up to orange and pink look shifted. &quot;Red-orange&quot; actually appears to be fully-saturated yellow. Overall the effect is as though the nearby colors have bled into the neighborhood of red.\" />\n",
       "</picture>"
      ],
      "text/plain": [
       "Plot showing four slices of the HSV cube, titled \"Predicted colors with repulsion · V vs H by S\". Nominally, each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. Each color value is represented as a square patch of that color. The outer portion of the patches shows the color as reconstructed by the model; the inner portion shows the true (input) color. The reconstructed and true colors agree fairly well, but \"red\" and nearby colors are clearly different, and different again from how the suppression intervention looked: \"red\" itself appears as pink or hot pink, and the surrounding colors up to orange and pink look shifted. \"Red-orange\" actually appears to be fully-saturated yellow. Overall the effect is as though the nearby colors have bled into the neighborhood of red."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<picture>\n",
       "  <source srcset=\"large-assets/ex-2.4-loss-colors-repulsion.dark.png?v=VnaVAvtkRcNUute-lo-CSRQ5W2SCbKVRlJ94g1Ad47o\" media=\"(prefers-color-scheme: dark)\" />\n",
       "  <source srcset=\"large-assets/ex-2.4-loss-colors-repulsion.png?v=VnaVAvtkRcNUute-lo-CSRQ5W2SCbKVRlJ94g1Ad47o\" media=\"(prefers-color-scheme: light)\" />\n",
       "  <img src=\"large-assets/ex-2.4-loss-colors-repulsion.png?v=VnaVAvtkRcNUute-lo-CSRQ5W2SCbKVRlJ94g1Ad47o\" alt=\"Line chart showing loss per color, titled &quot;&quot;. Y-axis: mean square error, ranging from zero to 0.094. X-axis: hue. There is a significant peak at red at either end of the X-axis, gradually sloping down to lower loss values near yellow and pink. Two very small peaks are at green and blue (apparently less than 1% of the height of the peaks at red).\" />\n",
       "</picture>"
      ],
      "text/plain": [
       "Line chart showing loss per color, titled \"\". Y-axis: mean square error, ranging from zero to 0.094. X-axis: hue. There is a significant peak at red at either end of the X-axis, gradually sloping down to lower loss values near yellow and pink. Two very small peaks are at green and blue (apparently less than 1% of the height of the peaks at red)."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max loss: 0.094\n",
      "Median MSE: 2.6e-05\n"
     ]
    }
   ],
   "source": [
    "from math import cos, pi\n",
    "\n",
    "import torch\n",
    "\n",
    "from ex_color.intervention import FastBezierMapper, InterventionConfig, Repulsion\n",
    "\n",
    "repulsion = Repulsion(\n",
    "    torch.tensor([1, 0, 0, 0], dtype=torch.float32),  # Constant!\n",
    "    FastBezierMapper(\n",
    "        0,  # Constrain effect to within 60° cone\n",
    "        cos(pi / 3),  # Create 30° hole (negative cone) around concept vector\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "interventions = [InterventionConfig(repulsion, ['bottleneck'])]\n",
    "y_hsv = await infer(model, interventions, x_hsv)\n",
    "hd_y_hsv = await infer(model, interventions, hd_x_hsv)\n",
    "clear_output()\n",
    "\n",
    "with displayer_mpl(\n",
    "    f'large-assets/ex-{nbid}-pred-colors-repulsion.png',\n",
    "    alt_text=\"\"\"Plot showing four slices of the HSV cube, titled \"{title}\". Nominally, each slice has constant saturation, but varies in value (brightness) from top to bottom, and in hue from left to right. Each color value is represented as a square patch of that color. The outer portion of the patches shows the color as reconstructed by the model; the inner portion shows the true (input) color. The reconstructed and true colors agree fairly well, but \"red\" and nearby colors are clearly different, and different again from how the suppression intervention looked: \"red\" itself appears as pink or hot pink, and the surrounding colors up to orange and pink look shifted. \"Red-orange\" actually appears to be fully-saturated yellow. Overall the effect is as though the nearby colors have bled into the neighborhood of red.\"\"\",\n",
    ") as show:\n",
    "    show(\n",
    "        lambda: plot_colors(\n",
    "            hsv_cube,\n",
    "            title='Predicted colors with repulsion',\n",
    "            colors=y_hsv.numpy(),\n",
    "            colors_compare=x_hsv.numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "per_color_loss = F.mse_loss(hd_y_hsv, hd_x_hsv, reduction='none').mean(dim=-1)\n",
    "max_loss = per_color_loss.max().item()\n",
    "median_loss = per_color_loss.median().item()\n",
    "with displayer_mpl(\n",
    "    f'large-assets/ex-{nbid}-loss-colors-repulsion.png',\n",
    "    alt_text=f\"\"\"Line chart showing loss per color, titled \"{{title}}\". Y-axis: mean square error, ranging from zero to {max_loss:.2g}. X-axis: hue. There is a significant peak at red at either end of the X-axis, gradually sloping down to lower loss values near yellow and pink. Two very small peaks are at green and blue (apparently less than 1% of the height of the peaks at red).\"\"\",\n",
    ") as show:\n",
    "    show(\n",
    "        lambda: plot_loss_lines(\n",
    "            hd_hsv_cube,\n",
    "            title='Repulsion',\n",
    "            loss=per_color_loss.numpy().reshape(hd_hsv_cube.shape),\n",
    "            ylabel='Mean square error',\n",
    "        )\n",
    "    )\n",
    "print(f'Max loss: {max_loss:.2g}')\n",
    "print(f'Median MSE: {median_loss:.2g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d947b",
   "metadata": {},
   "source": [
    "Again, visually, the colors look similar to 2.3. And again, the loss curves look similar but more regular (less crossing over). They also rise to more of a peak than 2.3, although they are a bit flat on the purple side of red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "083b6bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<picture>\n",
       "  <source srcset=\"large-assets/ex-2.4-latents-repulsion.dark.png?v=o2GvSzaqSEpg2CcUKwQkrTZ-goPtejyLuSA-Dbgw2Yc\" media=\"(prefers-color-scheme: dark)\" />\n",
       "  <source srcset=\"large-assets/ex-2.4-latents-repulsion.png?v=o2GvSzaqSEpg2CcUKwQkrTZ-goPtejyLuSA-Dbgw2Yc\" media=\"(prefers-color-scheme: light)\" />\n",
       "  <img src=\"large-assets/ex-2.4-latents-repulsion.png?v=o2GvSzaqSEpg2CcUKwQkrTZ-goPtejyLuSA-Dbgw2Yc\" alt=\"Three spherical plots, titled &quot;Latents · repulsion&quot;. Each plot shows a vibrant collection of colored circles or balls scattered over the surface of a black sphere. The first plot has the appearance of a partial color wheel, with  vibrant colors around the rim (like a rainbow), with with a conspicuously absent space at the top where &quot;red&quot; should be. The other plots show different views of the same sphere, with hue varying across the equator and tone varying from top to bottom. The central region of the second and third plots show something interesting: &quot;Red&quot; and nearby colors have been arranged into a wide ring or disc, rather than being clustered in the center. Each ball shows the reconstructed color, with a dot in the center showing the true (input) color. The true and reconstructor colors agree fairly well, except for colors close to &quot;red&quot;, which roughly agree in saturation but differ significantly in tone and hue.\" />\n",
       "</picture>"
      ],
      "text/plain": [
       "Three spherical plots, titled \"Latents · repulsion\". Each plot shows a vibrant collection of colored circles or balls scattered over the surface of a black sphere. The first plot has the appearance of a partial color wheel, with  vibrant colors around the rim (like a rainbow), with with a conspicuously absent space at the top where \"red\" should be. The other plots show different views of the same sphere, with hue varying across the equator and tone varying from top to bottom. The central region of the second and third plots show something interesting: \"Red\" and nearby colors have been arranged into a wide ring or disc, rather than being clustered in the center. Each ball shows the reconstructed color, with a dot in the center showing the true (input) color. The true and reconstructor colors agree fairly well, except for colors close to \"red\", which roughly agree in saturation but differ significantly in tone and hue."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "from ex_color.vis import plot_latent_grid_3d, ConicalAnnotation\n",
    "\n",
    "# Capture latents with repulsion\n",
    "y_rgb, h_rgb = await infer_with_latent_capture(model, interventions, x_rgb, 'bottleneck')\n",
    "clear_output()\n",
    "\n",
    "with displayer_mpl(\n",
    "    f'large-assets/ex-{nbid}-latents-repulsion.png',\n",
    "    alt_text=\"\"\"Three spherical plots, titled \"{title}\". Each plot shows a vibrant collection of colored circles or balls scattered over the surface of a black sphere. The first plot has the appearance of a partial color wheel, with  vibrant colors around the rim (like a rainbow), with with a conspicuously absent space at the top where \"red\" should be. The other plots show different views of the same sphere, with hue varying across the equator and tone varying from top to bottom. The central region of the second and third plots show something interesting: \"Red\" and nearby colors have been arranged into a wide ring or disc, rather than being clustered in the center. Each ball shows the reconstructed color, with a dot in the center showing the true (input) color. The true and reconstructor colors agree fairly well, except for colors close to \"red\", which roughly agree in saturation but differ significantly in tone and hue.\"\"\",\n",
    ") as show:\n",
    "    show(\n",
    "        lambda theme: plot_latent_grid_3d(\n",
    "            h_rgb,\n",
    "            y_rgb,\n",
    "            x_rgb,\n",
    "            title='Latents · repulsion',\n",
    "            dims=[(1, 0, 2), (1, 2, 0), (1, 3, 0)],\n",
    "            dot_radius=10,\n",
    "            theme=theme,\n",
    "            annotations=[\n",
    "                ConicalAnnotation(\n",
    "                    RED,\n",
    "                    2 * (np.pi / 2 - repulsion.mapper.a),  # type: ignore\n",
    "                    color=theme.val('black', dark='#fffa'),\n",
    "                    linewidth=theme.val(0.5, dark=1),\n",
    "                    dashes=theme.val((8, 8), dark=(4, 4)),\n",
    "                    gapcolor=theme.val('#fff4', dark='#0004'),\n",
    "                ),\n",
    "                ConicalAnnotation(\n",
    "                    RED,\n",
    "                    2 * (np.pi / 2 - repulsion.mapper.b),  # type: ignore\n",
    "                    color=theme.val('black', dark='#fffa'),\n",
    "                    linewidth=theme.val(0.5, dark=1),\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b5643",
   "metadata": {},
   "source": [
    "This looks _really_ clean: reds have been pushed to a nearly smooth arc towards to the top of the middle plot, and the distribution of reds in the right plot are is more regular than in 2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17177769",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Visually, these results are mostly better than [Ex 2.3](./ex-2.3-explicit-norm.ipynb):\n",
    "- Loss vs. hue curves are smoother and more regular\n",
    "- Latent space looks very regular, with primary and secondary colors mostly co-planar\n",
    "- Repuslion pushed reds into a clean arc.\n",
    "\n",
    "Numerically, the global training loss values are _slightly_ worse, so it's difficult to say if post-norm regularization would be better in more complicated networks.\n",
    "\n",
    "Overall, I feel like it's probably a small improvement. Let's adopt this configuration for future experiments, but be open to trying the previous configuration again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex-color-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
