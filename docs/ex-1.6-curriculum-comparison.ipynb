{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "id": "45f89542",
    "language": "markdown"
   },
   "source": [
    "# Experiment 1.6: Smooth vs. stepped hyperparameter transitions\n",
    "\n",
    "In previous experiments, we explored curriculum learning ([Ex 1.3](./ex-1.3-color-mlp-curriculum.ipynb)) with abrupt phase changes and later introduced smooth hyperparameter transitions using a dopesheet and minimum jerk interpolation ([Ex 1.5](./ex-1.5-color-mlp-anchoring.ipynb)).\n",
    "\n",
    "This notebook directly compares these two approaches:\n",
    "\n",
    "1.  **Stepped transitions:** Mimicking the traditional approach with discrete phases and sharp parameter changes at boundaries. We'll simulate the LR warmup used in Ex 1.3 within the dopesheet.\n",
    "2.  **Smooth transitions:** Using the minimum jerk trajectories from Ex 1.5 for all hyperparameters.\n",
    "\n",
    "Both methods will use the same 4D bottleneck model architecture, initialization seeds, loss functions (including anchoring), and target the same final hyperparameter values at equivalent points in the curriculum.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "While both approaches might reach similar final performance, we hypothesize that the smooth transitions will lead to:\n",
    "\n",
    "- **More stable training:** Fewer and smaller loss spikes, especially during periods corresponding to phase transitions in the stepped approach.\n",
    "- **Lower loss variance:** Quantifiably less fluctuation in the loss signal.\n",
    "- **Smoother latent space evolution:** A more gradual and less chaotic development of the final representation structure.\n",
    "\n",
    "## Experiment design\n",
    "\n",
    "We'll train the 4D MLP autoencoder using two different dopesheets representing the stepped and smooth schedules. We will track:\n",
    "\n",
    "- Training loss curves (total and components).\n",
    "- Loss variance over time.\n",
    "- Final latent space structure.\n",
    "- Evolution of the latent space (via animation, similar to Ex 1.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70808a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from utils.logging import SimpleLoggingConfig\n",
    "\n",
    "# Configure logging\n",
    "logging_config = SimpleLoggingConfig().info('notebook', 'utils', 'mini', 'ex_color')\n",
    "logging_config.apply()\n",
    "log = logging.getLogger('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8084321b",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Model architecture\n",
    "\n",
    "We'll use the same 4-dimensional bottleneck MLP as in Experiment 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ece3a6",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "E = 4  # Bottleneck dimension\n",
    "\n",
    "\n",
    "class ColorMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(16, E),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(E, 16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(16, 3),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        bottleneck = self.encoder(x)\n",
    "        output = self.decoder(bottleneck)\n",
    "        return output, bottleneck\n",
    "\n",
    "\n",
    "# Instantiate once to check params, will re-instantiate for each run\n",
    "temp_model = ColorMLP()\n",
    "total_params = sum(p.numel() for p in temp_model.parameters() if p.requires_grad)\n",
    "log.info(f'Model initialized with {total_params:,} trainable parameters.')\n",
    "del temp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b845ca",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Data loading\n",
    "\n",
    "We need the same datasets as in Ex 1.5:\n",
    "- `primary_dataset`: Just the 6 primary and secondary colors (used for anchoring).\n",
    "- `full_dataset`: The full grid of H, S, V colors (used for general training).\n",
    "- `rgb_tensor`: A dense grid covering the RGB cube (used for validation and visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e066c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ex_color.losses import Separate, planarity, objective, unitary\n",
    "from ex_color.data.color_cube import ColorCube\n",
    "from ex_color.data.cyclic import arange_cyclic\n",
    "\n",
    "# --- Primary Colors (for Anchor) ---\n",
    "primary_cube = ColorCube.from_hsv(\n",
    "    h=arange_cyclic(step_size=1 / 6),\n",
    "    s=np.ones(1),\n",
    "    v=np.ones(1),\n",
    ")\n",
    "primary_tensor = torch.tensor(primary_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "primary_dataset = TensorDataset(primary_tensor)\n",
    "# Batch size = all primary colors, as Anchor processes them together\n",
    "primary_loader = DataLoader(primary_dataset, batch_size=len(primary_tensor))\n",
    "log.info(f'Primary dataset shape: {primary_tensor.shape}')\n",
    "\n",
    "# --- Full HSV Grid (for Training) ---\n",
    "full_cube = ColorCube.from_hsv(\n",
    "    h=arange_cyclic(step_size=10 / 360),\n",
    "    s=np.linspace(0, 1, 10),\n",
    "    v=np.linspace(0, 1, 10),\n",
    ")\n",
    "full_tensor = torch.tensor(full_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "full_dataset = TensorDataset(full_tensor)\n",
    "# Sampler will be configured per run\n",
    "log.info(f'Full HSV dataset shape: {full_tensor.shape}')\n",
    "\n",
    "# --- Full RGB Grid (for Validation/Visualization) ---\n",
    "rgb_cube = ColorCube.from_rgb(\n",
    "    r=np.linspace(0, 1, 10),\n",
    "    g=np.linspace(0, 1, 10),\n",
    "    b=np.linspace(0, 1, 10),\n",
    ")\n",
    "rgb_tensor = torch.tensor(rgb_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "log.info(f'RGB validation tensor shape: {rgb_tensor.shape}')\n",
    "\n",
    "# --- Datasets Dictionary (used by training loop) ---\n",
    "# We'll create the specific loaders/samplers within each run setup\n",
    "datasets_config: dict[str, tuple[TensorDataset, Tensor]] = {\n",
    "    'Primary & secondary': (primary_dataset, primary_tensor),\n",
    "    'All hues': (full_dataset, rgb_tensor),\n",
    "    'Full color space': (full_dataset, rgb_tensor),\n",
    "}\n",
    "\n",
    "# --- Loss Criteria ---\n",
    "loss_criteria = {\n",
    "    'loss-recon': objective(nn.MSELoss()),\n",
    "    'reg-separate': Separate((0, 1)),  # Separate based on first two dims\n",
    "    'reg-planar': planarity,  # Penalize dims 2 and 3\n",
    "    'reg-norm': unitary,\n",
    "    # Anchor needs to be instantiated per run with fresh ref_data\n",
    "    # 'reg-anchor': Anchor(ref_data=primary_tensor),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f7f1d",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Event handlers and callbacks\n",
    "\n",
    "We'll reuse the `ModelRecorder` and `MetricsRecorder` from the engine. We also need the callback to update the `DynamicWeightedRandomBatchSampler` based on the `data-fraction` property from the dopesheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c4254",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from ex_color.data.cube_sampler import DynamicWeightedRandomBatchSampler\n",
    "from ex_color.data.filters import levels\n",
    "from ex_color.engine.events import Event\n",
    "\n",
    "\n",
    "# Callback to update the sampler weights based on dopesheet\n",
    "def update_sampler_weights(event: Event, sampler: DynamicWeightedRandomBatchSampler, weights_source: np.ndarray):\n",
    "    frac = event.timeline_state.props['data-fraction']\n",
    "    # Interpolation logic from Ex 1.5 to shift focus from vibrant to all colors\n",
    "    in_low = np.interp(frac, [0, 0.5], [0.99, 0])\n",
    "    out_low = np.interp(frac, [0.5, 1], [0, 1])\n",
    "    sampler.weights = levels(weights_source, in_low=in_low, out_low=out_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e8387",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Dopesheets for comparison\n",
    "\n",
    "We define two dopesheets, stored in separate CSV files:\n",
    "1. **Stepped:** [`ex-1.6-stepped-dopesheet.csv`](./ex-1.6-stepped-dopesheet.csv) - Mimics the abrupt phase changes and LR schedule from Ex 1.3.\n",
    "2. **Smooth:** [`ex-1.6-smooth-dopesheet.csv`](./ex-1.6-smooth-dopesheet.csv) - Uses minimum jerk interpolation between keyframes, based on Ex 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339a0ac",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "\n",
    "dopesheet_dir = Path('.')  # Assumes notebook is run from docs/\n",
    "\n",
    "# --- Load Dopesheets ---\n",
    "stepped_dopesheet_path = dopesheet_dir / 'ex-1.6-stepped-dopesheet.csv'\n",
    "smooth_dopesheet_path = dopesheet_dir / 'ex-1.6-smooth-dopesheet.csv'\n",
    "\n",
    "stepped_dopesheet = Dopesheet.from_csv(stepped_dopesheet_path)\n",
    "smooth_dopesheet = Dopesheet.from_csv(smooth_dopesheet_path)\n",
    "\n",
    "log.info(f'Loaded stepped dopesheet from {stepped_dopesheet_path}')\n",
    "log.info(f'Loaded smooth dopesheet from {smooth_dopesheet_path}')\n",
    "\n",
    "# Display the resolved dopesheets (optional)\n",
    "# display(stepped_dopesheet.as_df(styled=True))\n",
    "# display(smooth_dopesheet.as_df(styled=True))\n",
    "\n",
    "# --- Verify Initial Values ---\n",
    "initial_stepped = stepped_dopesheet.get_initial_values()\n",
    "initial_smooth = smooth_dopesheet.get_initial_values()\n",
    "log.info(f'Initial values (Stepped): {initial_stepped}')\n",
    "log.info(f'Initial values (Smooth): {initial_smooth}')\n",
    "\n",
    "# Check that they are identical (they should be)\n",
    "assert initial_stepped == initial_smooth, 'Initial values differ between dopesheets!'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
