{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1.7: Sparse labels (per-sample regularization)\n",
    "\n",
    "In previous experiments, we imposed structure on latent space with curriculum learning: varying the training data and hyperparameters over the course of the training run ([Ex 1.3](./ex-1.3-color-mlp-curriculum.ipynb), [Ex 1.5](./ex-1.5-color-mlp-anchoring.ipynb), [Ex 1.6](./ex-1.6-curriculum-comparison.ipynb)). It worked (in that the latent space looked _ok_), but we are unsure whether it worked because the color wheel was found and anchored _before_ expanding the data to include all colors, or whether it was just that the primary and secondary colors were (in effect) labelled: that is, special regularization was applied to those samples.\n",
    "\n",
    "In this experiment, we do away with the phased curriculum, and instead apply per-sample regularization. Our hypothesis is that this will in fact outperform the curriculum-based methods, because the model will have access to the data full distribution from the start (limited only by batch size).\n",
    "\n",
    "## Dataset design\n",
    "\n",
    "We chose color as a domain because it's easy to reason about and visualize. Since our eventual goal is to apply these techniques to LLM training, we should consider how to constrain the labels in a way that could realistically be replicated for text. We assume that:\n",
    "\n",
    "1. An LLM would be trained with something like internet text\n",
    "2. Sentiment analysis could be run over it to generate labels — attributes that we care about, such as \"malicious\", \"benign\", \"honest\", etc.\n",
    "3. Such labelling would not be entirely accurate.\n",
    "\n",
    "For this experiment, we will apply the following regigme:\n",
    "\n",
    "1. An autoencoder trained on a color cube\n",
    "2. Certain colors are given labels (e.g. $(1,0,0)=red$)\n",
    "3. The labelling would be noisy, e.g. maybe $(1,0,0)$ wouldn't _always_ be labelled as $red$, and sometimes other colors close to red would be given that label.\n",
    "\n",
    "Certain regularizers will be activated based on those labels, e.g. colors labelled $red$ could be penalized for not being embedded at $(1,0,0,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from utils.logging import SimpleLoggingConfig\n",
    "\n",
    "logging_config = SimpleLoggingConfig().info('notebook', 'utils', 'mini', 'ex_color')\n",
    "logging_config.apply()\n",
    "\n",
    "# ID for tagging assets\n",
    "nbid = '1.7'\n",
    "# This is the logger for this notebook\n",
    "log = logging.getLogger(f'notebook.{nbid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "We use the same simple 2-layer MLP autoencoder with a bottleneck as in previous experiments. The key difference lies not in the architecture, but in the training process governed by the smooth curriculum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "E = 4\n",
    "\n",
    "\n",
    "class ColorMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # RGB input (3D) → hidden layer → bottleneck → hidden layer → RGB output\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, E),  # Our critical bottleneck!\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(E, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, 3),\n",
    "            nn.Sigmoid(),  # Keep RGB values in [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Get our bottleneck representation\n",
    "        bottleneck = self.encoder(x)\n",
    "\n",
    "        # Decode back to RGB\n",
    "        output = self.decoder(bottleneck)\n",
    "        return output, bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training machinery with timeline and events\n",
    "\n",
    "The `train_color_model` function orchestrates the training process based on a `Timeline` derived from the dopesheet. It handles:\n",
    "\n",
    "- Iterating through training steps.\n",
    "- Fetching the correct data loader for the current phase.\n",
    "- Updating hyperparameters (like learning rate and loss weights) smoothly based on the timeline state.\n",
    "- Calculating the combined loss from reconstruction and various regularizers.\n",
    "- Executing the optimizer step.\n",
    "- Emitting events at different points (phase start/end, pre-step, actions like 'anchor', step metrics) to trigger callbacks like plotting, recording, or updating loss terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Protocol, runtime_checkable\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.timeline import State\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceResult:\n",
    "    outputs: Tensor\n",
    "    latents: Tensor\n",
    "\n",
    "    def detach(self):\n",
    "        return InferenceResult(self.outputs.detach(), self.latents.detach())\n",
    "\n",
    "    def clone(self):\n",
    "        return InferenceResult(self.outputs.clone(), self.latents.clone())\n",
    "\n",
    "    def cpu(self):\n",
    "        return InferenceResult(self.outputs.cpu(), self.latents.cpu())\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class LossCriterion(Protocol):\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor: ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RegularizerConfig:\n",
    "    \"\"\"Configuration for a regularizer, including label affinities.\"\"\"\n",
    "    name: str\n",
    "    \"\"\"Matched with hyperparameter for weighting\"\"\"\n",
    "    criterion: LossCriterion\n",
    "    label_affinities: dict[str, float] | None\n",
    "    \"\"\"Maps label names to affinity strengths\"\"\"\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class Event:\n",
    "    name: str\n",
    "    step: int\n",
    "    model: ColorMLP\n",
    "    timeline_state: State\n",
    "    optimizer: optim.Optimizer\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class PhaseEndEvent(Event):\n",
    "    validation_data: Tensor\n",
    "    inference_result: InferenceResult\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class StepMetricsEvent(Event):\n",
    "    \"\"\"Event carrying metrics calculated during a training step.\"\"\"\n",
    "\n",
    "    total_loss: float\n",
    "    losses: dict[str, float]\n",
    "\n",
    "\n",
    "class EventHandler[T](Protocol):\n",
    "    def __call__(self, event: T) -> None: ...\n",
    "\n",
    "\n",
    "class EventBinding[T]:\n",
    "    \"\"\"A class to bind events to handlers.\"\"\"\n",
    "\n",
    "    def __init__(self, event_name: str):\n",
    "        self.event_name = event_name\n",
    "        self.handlers: list[tuple[str, EventHandler[T]]] = []\n",
    "\n",
    "    def add_handler(self, event_name: str, handler: EventHandler[T]) -> None:\n",
    "        self.handlers.append((event_name, handler))\n",
    "\n",
    "    def emit(self, event_name: str, event: T) -> None:\n",
    "        for name, handler in self.handlers:\n",
    "            if name == event_name:\n",
    "                handler(event)\n",
    "\n",
    "\n",
    "class EventHandlers:\n",
    "    \"\"\"A simple event system to allow for custom callbacks.\"\"\"\n",
    "\n",
    "    phase_start: EventBinding[Event]\n",
    "    pre_step: EventBinding[Event]\n",
    "    action: EventBinding[Event]\n",
    "    phase_end: EventBinding[PhaseEndEvent]\n",
    "    step_metrics: EventBinding[StepMetricsEvent]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.phase_start = EventBinding[Event]('phase-start')\n",
    "        self.pre_step = EventBinding[Event]('pre-step')\n",
    "        self.action = EventBinding[Event]('action')\n",
    "        self.phase_end = EventBinding[PhaseEndEvent]('phase-end')\n",
    "        self.step_metrics = EventBinding[StepMetricsEvent]('step-metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from skimage import metrics\n",
    "import torch\n",
    "from typing import Iterable, Iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "from utils.progress import RichProgress\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    log.info(f'Global random seed set to {seed}')\n",
    "\n",
    "\n",
    "def set_deterministic_mode(seed: int):\n",
    "    \"\"\"Make experiments reproducible.\"\"\"\n",
    "    seed_everything(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    log.info('PyTorch set to deterministic mode')\n",
    "\n",
    "\n",
    "def reiterate[T](it: Iterable[T]) -> Iterator[T]:\n",
    "    \"\"\"\n",
    "    Iterates over an iterable indefinitely.\n",
    "\n",
    "    When the iterable is exhausted, it starts over from the beginning. Unlike\n",
    "    `itertools.cycle`, yielded values are not cached — so each iteration may be\n",
    "    different.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        yield from it\n",
    "\n",
    "\n",
    "def train_color_model(  # noqa: C901\n",
    "    model: ColorMLP,\n",
    "    train_loader: DataLoader,\n",
    "    val_data: Tensor,\n",
    "    dopesheet: Dopesheet,\n",
    "    loss_criterion: LossCriterion,\n",
    "    regularizers: list[RegularizerConfig],\n",
    "    event_handlers: EventHandlers | None = None,\n",
    "):\n",
    "    if event_handlers is None:\n",
    "        event_handlers = EventHandlers()\n",
    "\n",
    "    # --- Validate inputs ---\n",
    "    if 'lr' not in dopesheet.props:\n",
    "        raise ValueError(\"Dopesheet must define the 'lr' property column.\")\n",
    "    # --- End Validation ---\n",
    "\n",
    "    timeline = Timeline(dopesheet)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    train_data = iter(reiterate(train_loader))\n",
    "\n",
    "    total_steps = len(timeline)\n",
    "\n",
    "    with RichProgress(total=total_steps, description='Training Steps') as pbar:\n",
    "        for step in range(total_steps):\n",
    "            # Get state *before* advancing timeline for this step's processing\n",
    "            current_state = timeline.state\n",
    "            current_phase_name = current_state.phase\n",
    "\n",
    "            batch_data, batch_labels = next(train_data)\n",
    "            # Should already be on device\n",
    "            # batch_data = batch_data.to(device)\n",
    "            # batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # --- Event Handling ---\n",
    "            event_template = {\n",
    "                'step': step,\n",
    "                'model': model,\n",
    "                'timeline_state': current_state,\n",
    "                'optimizer': optimizer,\n",
    "            }\n",
    "\n",
    "            if current_state.is_phase_start:\n",
    "                event = Event(name=f'phase-start:{current_phase_name}', **event_template)\n",
    "                event_handlers.phase_start.emit(event.name, event)\n",
    "                event_handlers.phase_start.emit('phase-start', event)\n",
    "\n",
    "            for action in current_state.actions:\n",
    "                event = Event(name=f'action:{action}', **event_template)\n",
    "                event_handlers.action.emit(event.name, event)\n",
    "                event_handlers.action.emit('action', event)\n",
    "\n",
    "            event = Event(name='pre-step', **event_template)\n",
    "            event_handlers.pre_step.emit('pre-step', event)\n",
    "\n",
    "            # --- Training Step ---\n",
    "            # ... (get data, update LR, zero grad, forward pass, calculate loss, backward, step) ...\n",
    "\n",
    "            current_lr = current_state.props['lr']\n",
    "            # REF_BATCH_SIZE = 32\n",
    "            # lr_scale_factor = batch.shape[0] / REF_BATCH_SIZE\n",
    "            # current_lr = current_lr * lr_scale_factor\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs, latents = model(batch_data)\n",
    "            current_results = InferenceResult(outputs, latents)\n",
    "\n",
    "            primary_loss = loss_criterion(batch_data, current_results).mean()\n",
    "            losses = {'recon': primary_loss.item()}\n",
    "            total_loss = primary_loss\n",
    "            zeros = torch.tensor(0.0, device=batch_data.device)\n",
    "\n",
    "            for regularizer in regularizers:\n",
    "                name = regularizer.name\n",
    "                criterion = regularizer.criterion\n",
    "\n",
    "                weight = current_state.props.get(name, 1.0)\n",
    "                if weight == 0:\n",
    "                    continue\n",
    "\n",
    "                if regularizer.label_affinities is not None:\n",
    "                    label_probs = [\n",
    "                        batch_labels[k] * v  # Soft labels that indicate how much effect this regularizer has, based on its affinity with the label\n",
    "                        for k, v in regularizer.label_affinities.items()\n",
    "                        if k in batch_labels\n",
    "                    ]\n",
    "                    if not label_probs:\n",
    "                        continue\n",
    "\n",
    "                    sample_affinities = torch.stack(label_probs, dim=0).sum(dim=0)\n",
    "                    sample_affinities = torch.clamp(sample_affinities, 0.0, 1.0)\n",
    "                    if torch.allclose(sample_affinities, zeros):\n",
    "                        continue\n",
    "                else:\n",
    "                    sample_affinities = torch.ones(batch_data.shape[0], device=batch_data.device)\n",
    "\n",
    "                per_sample_loss = criterion(batch_data, current_results)\n",
    "                if len(per_sample_loss.shape) == 0:\n",
    "                    # If the loss is a scalar, we need to expand it to match the batch size\n",
    "                    per_sample_loss = per_sample_loss.expand(batch_data.shape[0])\n",
    "                assert per_sample_loss.shape[0] == batch_data.shape[0], f'Loss should be per-sample OR scalar: {name}'\n",
    "\n",
    "                # Apply sample affinities\n",
    "                weighted_loss = per_sample_loss * sample_affinities\n",
    "\n",
    "                # Apply sample importance weights\n",
    "                # weighted_loss *= batch_weights\n",
    "\n",
    "                # Calculate mean only over selected samples. If we used torch.mean, it would average over all samples, including those with 0 weight\n",
    "                term_loss = weighted_loss.sum() / (sample_affinities.sum() + 1e-8)\n",
    "\n",
    "                losses[name] = term_loss.item()\n",
    "                if not torch.isfinite(term_loss):\n",
    "                    log.warning(f'Loss term {name} at step {step} is not finite: {term_loss}')\n",
    "                    continue\n",
    "                total_loss += term_loss * weight\n",
    "\n",
    "            if total_loss > 0:\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "            # --- End Training Step ---\n",
    "\n",
    "            # Emit step metrics event\n",
    "            step_metrics_event = StepMetricsEvent(\n",
    "                name='step-metrics',\n",
    "                **event_template,\n",
    "                total_loss=total_loss.item(),\n",
    "                losses=losses,\n",
    "            )\n",
    "            event_handlers.step_metrics.emit('step-metrics', step_metrics_event)\n",
    "\n",
    "            # --- Post-Step Event Handling ---\n",
    "            if current_state.is_phase_end:\n",
    "                # Trigger phase-end for the *current* phase\n",
    "                # validation_data = batch_data\n",
    "                with torch.no_grad():\n",
    "                    val_outputs, val_latents = model(val_data.to(device))\n",
    "                event = PhaseEndEvent(\n",
    "                    name=f'phase-end:{current_phase_name}',\n",
    "                    **event_template,\n",
    "                    validation_data=val_data,\n",
    "                    inference_result=InferenceResult(val_outputs, val_latents),\n",
    "                )\n",
    "                event_handlers.phase_end.emit(event.name, event)\n",
    "                event_handlers.phase_end.emit('phase-end', event)\n",
    "            # --- End Event Handling ---\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(\n",
    "                metrics={\n",
    "                    'PHASE': current_phase_name,\n",
    "                    'lr': f'{current_lr:.6f}',\n",
    "                    'loss': f'{total_loss.item():.4f}',\n",
    "                    **{name: f'{lt:.4f}' for name, lt in losses.items()},\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Advance timeline *after* processing the current step\n",
    "            if step < total_steps:  # Avoid stepping past the end\n",
    "                timeline.step()\n",
    "\n",
    "    log.info('Training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from torch import Tensor\n",
    "from IPython.display import HTML\n",
    "\n",
    "from utils.nb import save_fig\n",
    "\n",
    "\n",
    "class PhasePlotter:\n",
    "    \"\"\"Event handler to plot latent space at the end of each phase.\"\"\"\n",
    "\n",
    "    def __init__(self, val_data: Tensor, *, dim_pairs: list[tuple[int, int]], interval: int = 100):\n",
    "        from utils.nb import displayer\n",
    "\n",
    "        # Store (phase_name, end_step, data, result) - data comes from event now\n",
    "        self.val_data = val_data\n",
    "        self.history: list[tuple[str, int, Tensor, Tensor]] = []\n",
    "        self.display = displayer()\n",
    "        self.dim_pairs = dim_pairs\n",
    "        self.interval = interval\n",
    "\n",
    "    def __call__(self, event: Event):\n",
    "        # TODO: Don't assume device = CPU\n",
    "        # TODO: Split this class so that the event handler is separate from the plotting, and so the plotting can happen locally with @run.hither\n",
    "        if event.step % self.interval != 0:\n",
    "            return\n",
    "        phase_name = event.timeline_state.phase\n",
    "        step = event.step\n",
    "        output, latents = event.model(self.val_data)\n",
    "\n",
    "        log.debug(f'Plotting end of phase: {phase_name} at step {step} using provided results.')\n",
    "\n",
    "        # Append to history\n",
    "        self.history.append((phase_name, step, output.detach().cpu(), latents.detach().cpu()))\n",
    "\n",
    "        # Plotting logic remains the same as it already expected CPU tensors\n",
    "        fig = self._plot_phase_history()\n",
    "        self.display(\n",
    "            HTML(\n",
    "                save_fig(\n",
    "                    fig,\n",
    "                    f'large-assets/ex-{nbid}-color-phase-history.png',\n",
    "                    alt_text='Visualizations of latent space at the end of each curriculum phase.',\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _plot_phase_history(self):\n",
    "        if not self.history:\n",
    "            fig, ax = plt.subplots()\n",
    "            fig.set_facecolor('#333')\n",
    "            ax.set_facecolor('#222')\n",
    "            ax.text(0.5, 0.5, 'Waiting...', ha='center', va='center')\n",
    "            return fig\n",
    "\n",
    "        plt.style.use('dark_background')\n",
    "\n",
    "        # Number of dimension pairs\n",
    "        num_dim_pairs = len(self.dim_pairs)\n",
    "\n",
    "        # Cap the number of thumbnails to a maximum for readability\n",
    "        max_thumbnails = 10\n",
    "        # If we have more history than max_thumbnails, only show the most recent ones\n",
    "        stride = math.ceil(len(self.history) / max_thumbnails)\n",
    "        history_to_show = self.history[::stride]\n",
    "        history_to_show = history_to_show[:max_thumbnails]\n",
    "        # history_to_show = self.history[-max_thumbnails:] if len(self.history) > max_thumbnails else self.history\n",
    "\n",
    "        # Create figure with gridspec for flexible layout\n",
    "        fig = plt.figure(figsize=(12, 5), facecolor='#333')\n",
    "\n",
    "        # Create two separate gridspecs - one for thumbnails, one for latest state\n",
    "        gs = fig.add_gridspec(2, 1, hspace=0.1, height_ratios=[1, 4])\n",
    "\n",
    "        # Thumbnail gridspec (top row) - only first dimension pair\n",
    "        # Remove spacing between thumbnails by setting wspace=0\n",
    "        thumbnail_gs = gs[0].subgridspec(2, max_thumbnails, wspace=0, hspace=0.1, height_ratios=[0, 1])\n",
    "\n",
    "        # Latest state gridspec (bottom row) - all dimension pairs\n",
    "        latest_gs = gs[1].subgridspec(2, num_dim_pairs, wspace=0, hspace=0.1, height_ratios=[1, 0])\n",
    "\n",
    "        # Get the data\n",
    "        _colors = self.val_data.numpy()\n",
    "\n",
    "        # Create thumbnail axes and plot history\n",
    "        for i, (_, step, _, latents) in enumerate(history_to_show):\n",
    "            _latents = latents.numpy()\n",
    "\n",
    "            # Only plot the first dimension pair for thumbnails\n",
    "            dim1, dim2 = self.dim_pairs[0]\n",
    "\n",
    "            # Create title for the thumbnail as its own axes, so that it's aligned with the other titles\n",
    "            axt = fig.add_subplot(thumbnail_gs[0, i])\n",
    "            axt.text(\n",
    "                0,\n",
    "                0,\n",
    "                f'{step}',\n",
    "                # transform=axt.transAxes,\n",
    "                horizontalalignment='center',\n",
    "                fontsize=7,\n",
    "            )\n",
    "            # Remove all decorations\n",
    "            axt.patch.set_alpha(0)\n",
    "            axt.set_xticks([])\n",
    "            axt.set_yticks([])\n",
    "            axt.spines['top'].set_visible(False)\n",
    "            axt.spines['right'].set_visible(False)\n",
    "            axt.spines['bottom'].set_visible(False)\n",
    "            axt.spines['left'].set_visible(False)\n",
    "\n",
    "            # Create thumbnail axis\n",
    "            ax = fig.add_subplot(thumbnail_gs[1, i])\n",
    "            ax.sharex(axt)\n",
    "\n",
    "            # Plot the data\n",
    "            ax.scatter(_latents[:, dim1], _latents[:, dim2], c=_colors, s=50, alpha=0.7)\n",
    "\n",
    "            # Add reference circle\n",
    "            ax.add_patch(Circle((0, 0), 1, fill=True, facecolor='black', edgecolor='gray', zorder=-1))\n",
    "\n",
    "            # Remove all decorations\n",
    "            ax.patch.set_alpha(0)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "\n",
    "            # Ensure square aspect ratio\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_adjustable('box')\n",
    "\n",
    "        # Plot latest state\n",
    "        # Get the latest data\n",
    "        phase_name, step, output, latents = self.history[-1]\n",
    "        _latents = latents.numpy()\n",
    "\n",
    "        for i, (dim1, dim2) in enumerate(self.dim_pairs):\n",
    "            # Create title for the thumbnail as its own axes, so that it's aligned with the other titles\n",
    "            axt = fig.add_subplot(latest_gs[1, i])\n",
    "            axt.text(\n",
    "                0,\n",
    "                0,\n",
    "                f'[{dim1}, {dim2}]',\n",
    "                # transform=axt.transAxes,\n",
    "                horizontalalignment='center',\n",
    "                fontsize=10,\n",
    "            )\n",
    "            # Remove all decorations\n",
    "            axt.patch.set_alpha(0)\n",
    "            axt.set_xticks([])\n",
    "            axt.set_yticks([])\n",
    "            axt.spines['top'].set_visible(False)\n",
    "            axt.spines['right'].set_visible(False)\n",
    "            axt.spines['bottom'].set_visible(False)\n",
    "            axt.spines['left'].set_visible(False)\n",
    "\n",
    "            # Plot\n",
    "            ax = fig.add_subplot(latest_gs[0, i])\n",
    "            ax.sharex(axt)\n",
    "\n",
    "            # Plot the data with larger markers\n",
    "            ax.scatter(_latents[:, dim1], _latents[:, dim2], c=_colors, s=200, alpha=0.7)\n",
    "\n",
    "            # Add reference circle\n",
    "            ax.add_patch(Circle((0, 0), 1, fill=True, facecolor='black', edgecolor='gray', zorder=-1))\n",
    "\n",
    "            # Remove all decorations\n",
    "            ax.patch.set_alpha(0)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "\n",
    "            # Ensure square aspect ratio\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_adjustable('box')\n",
    "\n",
    "        # Add overall title\n",
    "        fig.suptitle('Latent Space', fontsize=12, color='white')\n",
    "\n",
    "        # Use subplots_adjust instead of tight_layout to avoid warnings\n",
    "        fig.subplots_adjust(top=0.85, bottom=0.1, left=0.1, right=0.95)\n",
    "\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter dopesheet\n",
    "\n",
    "We'll define [a dopesheet](./ex-1.7-dopesheet.csv) (timelines) to allow hyperparameters to vary over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.7-color-timeline.png?v=-G0uY_gDVn8k7Z07RCQnL_tUxPfbaChSG2y_SeZRtpw\" alt=\"Line chart showing the hyperparameter schedule over time.\" style=\"max-width: 70rem;\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from mini.temporal.vis import plot_timeline, realize_timeline, ParamGroup\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "from utils.nb import save_fig\n",
    "\n",
    "\n",
    "line_styles = [\n",
    "    (re.compile(r'^data-'), {'linewidth': 5, 'zorder': -1, 'alpha': 0.5}),\n",
    "    # (re.compile(r'-(anchor|norm)$'), {'linewidth': 2, 'linestyle': (0, (8, 1, 1, 1))}),\n",
    "]\n",
    "\n",
    "\n",
    "def load_dopesheet():\n",
    "    dopesheet = Dopesheet.from_csv(f'ex-{nbid}-dopesheet.csv')\n",
    "    # display(Markdown(f\"\"\"## Parameter schedule ({variant})\\n{dopesheet.to_markdown()}\"\"\"))\n",
    "\n",
    "    timeline = Timeline(dopesheet)\n",
    "    history_df = realize_timeline(timeline)\n",
    "    keyframes_df = dopesheet.as_df()\n",
    "\n",
    "    groups = (\n",
    "        ParamGroup(\n",
    "            name='',\n",
    "            params=[p for p in dopesheet.props if p not in {'lr'}],\n",
    "            height_ratio=2,\n",
    "        ),\n",
    "        ParamGroup(\n",
    "            name='',\n",
    "            params=[p for p in dopesheet.props if p in {'lr'}],\n",
    "            height_ratio=1,\n",
    "        ),\n",
    "    )\n",
    "    fig, ax = plot_timeline(history_df, keyframes_df, groups, line_styles=line_styles)\n",
    "    # Add assertion to satisfy type checker\n",
    "    assert isinstance(fig, Figure), 'plot_timeline should return a Figure'\n",
    "    display(\n",
    "        HTML(\n",
    "            save_fig(\n",
    "                fig,\n",
    "                f'large-assets/ex-{nbid}-color-timeline.png',\n",
    "                alt_text='Line chart showing the hyperparameter schedule over time.',\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return dopesheet\n",
    "\n",
    "\n",
    "dopesheet = load_dopesheet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These schedules seem pretty well matched for a fair comparison. The core hyperparameter targets are hit at similar times, with the main difference being, well, the smoothness. This should give us a good basis for seeing what impact the transition style has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions and regularizers\n",
    "\n",
    "Like Ex 1.5, we use mean squared error for the main reconstruction loss (`loss-recon`), and regularizers that encourage embeddings of unit length, and for primary colors to be on the plane of the first two dimensions.\n",
    "\n",
    "Unlike Ex 1.5, most of the criteria and regularizers now return per-sample loss, which allows new samples to be given lower weight (see data loaders below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import linalg as LA\n",
    "\n",
    "from ex_color.data.color_cube import ColorCube\n",
    "from ex_color.data.cyclic import arange_cyclic\n",
    "\n",
    "\n",
    "def objective(fn):\n",
    "    \"\"\"Adapt loss function to look like a regularizer\"\"\"\n",
    "\n",
    "    def wrapper(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        loss = fn(data, res.outputs)\n",
    "        # Reduce element-wise loss to per-sample loss by averaging over feature dimensions\n",
    "        if loss.ndim > 1:\n",
    "            # Calculate mean over all dimensions except the first (batch) dimension\n",
    "            reduce_dims = tuple(range(1, loss.ndim))\n",
    "            loss = torch.mean(loss, dim=reduce_dims)\n",
    "        return loss\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def unitary(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to have unit norm (vectors of length 1)\"\"\"\n",
    "    norms = LA.vector_norm(res.latents, dim=-1)\n",
    "    # Return per-sample loss, shape [B]\n",
    "    return (norms - 1.0) ** 2\n",
    "\n",
    "\n",
    "def planarity(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to be planar in the first two channels (so zero in other channels)\"\"\"\n",
    "    if res.latents.shape[1] <= 2:\n",
    "        # No dimensions beyond the first two, return zero loss per sample\n",
    "        return torch.zeros(res.latents.shape[0], device=res.latents.device)\n",
    "    # Sum squares across the extra dimensions for each sample, shape [B]\n",
    "    return torch.sum(res.latents[:, 2:] ** 2, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anchor(LossCriterion):\n",
    "    def __init__(self, anchor_point: Tensor):\n",
    "        self.anchor_point = anchor_point\n",
    "\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        \"\"\"\n",
    "        Regularize latents to be close to the anchor point.\n",
    "\n",
    "        Returns:\n",
    "            loss: Per-sample loss, shape [B].\n",
    "        \"\"\"\n",
    "        # Calculate squared distances to the anchor\n",
    "        sq_dists = torch.sum((res.latents - self.anchor_point) ** 2, dim=-1)  # [B]\n",
    "        return sq_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Separate(LossCriterion):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: tuple[int, ...] | None = None,\n",
    "        bias: float = 1e-8,\n",
    "        scale: float = 1.0,\n",
    "        power: float = 1.0,\n",
    "    ):\n",
    "        self.channels = channels\n",
    "        self.scale = scale\n",
    "        self.power = power\n",
    "        self.bias = bias\n",
    "\n",
    "    # def __call__(self, data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    #     \"\"\"\n",
    "    #     Regularize latents to be separated from each other in first two channels.\n",
    "\n",
    "    #     Returns:\n",
    "    #         loss: Per-sample loss, shape [B].\n",
    "    #     \"\"\"\n",
    "    #     # Get pairwise differences in the first two dimensions\n",
    "    #     points = res.latents[:, self.channels]  # [B, C]\n",
    "    #     diffs = points.unsqueeze(1) - points.unsqueeze(0)  # [B, B, C]\n",
    "    #     diffs *= self.scale\n",
    "    #     # diffs += 1.0\n",
    "\n",
    "    #     # Calculate squared distances\n",
    "    #     sq_dists = torch.sum(diffs**2, dim=-1)  # [B, B]\n",
    "    #     invmask = torch.isclose(sq_dists, torch.zeros_like(sq_dists))  # [B, B]\n",
    "    #     losses = 1.0 / (sq_dists + self.bias)\n",
    "\n",
    "    #     # Remove self-loss (and degenerate pairs)\n",
    "    #     losses[invmask] = 0.0\n",
    "\n",
    "    #     losses = torch.sum(losses, dim=1)  # [B]\n",
    "    #     # losses = torch.sigmoid_(losses)  # [B]\n",
    "    #     # return losses\n",
    "    #     return losses**self.power\n",
    "\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        \"\"\"\n",
    "        Regularize latents to be separated from each other in first two channels.\n",
    "\n",
    "        Returns:\n",
    "            loss: Per-sample loss, shape [B].\n",
    "        \"\"\"\n",
    "        # Get pairwise differences in the first two dimensions\n",
    "        points = res.latents[:, self.channels]  # [B, C]\n",
    "\n",
    "        # Normalize to unit hypersphere, so that points are repelled along its surface\n",
    "        magnitudes = torch.norm(points, dim=-1, keepdim=True)\n",
    "        points = points / (magnitudes + 1e-8)  # Normalize to unit sphere\n",
    "\n",
    "        diffs = points.unsqueeze(1) - points.unsqueeze(0)  # [B, B, C]\n",
    "        diffs *= self.scale\n",
    "        # diffs += 1.0\n",
    "\n",
    "        # Calculate squared distances\n",
    "        sq_dists = torch.sum(diffs**2, dim=-1)  # [B, B]\n",
    "        invmask = torch.isclose(sq_dists, torch.zeros_like(sq_dists))  # [B, B]\n",
    "        losses = 1.0 / (sq_dists + self.bias)\n",
    "\n",
    "        # Remove self-loss (and degenerate pairs)\n",
    "        losses[invmask] = 0.0\n",
    "\n",
    "        losses = torch.sum(losses, dim=1)  # [B]\n",
    "        # losses = torch.sigmoid_(losses)  # [B]\n",
    "        # return losses\n",
    "        return losses**self.power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading, sampling, and event handling\n",
    "\n",
    "Here we set up:\n",
    "\n",
    "- **Datasets:** Define the datasets used (primary/secondary colors, full color grid).\n",
    "- **Sampler:** Use `DynamicWeightedRandomBatchSampler` for the full dataset. Its weights are updated by the `update_sampler_weights` callback, which responds to the `data-fraction` parameter from the dopesheet. This smoothly shifts the sampling focus from highly vibrant colors early on to the full range of colors later.\n",
    "- **Recorders:** `ModelRecorder` and `MetricsRecorder` are event handlers that save the model state and loss values at each step.\n",
    "- **Event bindings:** Connect event handlers to specific events (e.g., `plotter` to `phase-end`, `reg_anchor.on_anchor` to `action:anchor`, recorders to `pre-step` and `step-metrics`).\n",
    "- **Training execution:** Finally, call `train_color_model` with the model, datasets, dopesheet, loss criteria, and configured event handlers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ModelRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record model parameters.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, dict[str, Tensor]]]\n",
    "    \"\"\"A list of tuples (step, state_dict) where state_dict is a copy of the model's state dict.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: Event):\n",
    "        # Get a *copy* of the state dict and move it to the CPU\n",
    "        # so we don't hold onto GPU memory or track gradients unnecessarily.\n",
    "        model_state = {k: v.cpu().clone() for k, v in event.model.state_dict().items()}\n",
    "        self.history.append((event.step, model_state))\n",
    "        log.debug(f'Recorded model state at step {event.step}')\n",
    "\n",
    "\n",
    "class MetricsRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record training metrics.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, float, dict[str, float]]]\n",
    "    \"\"\"A list of tuples (step, total_loss, losses_dict).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: StepMetricsEvent):\n",
    "        # Ensure we are handling the correct event type\n",
    "        if not isinstance(event, StepMetricsEvent):\n",
    "            log.warning(f'MetricsRecorder received unexpected event type: {type(event)}')\n",
    "            return\n",
    "\n",
    "        self.history.append((event.step, event.total_loss, event.losses.copy()))\n",
    "        log.debug(f'Recorded metrics at step {event.step}: loss={event.total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# TODO: remove forced reload\n",
    "if True:\n",
    "    import importlib\n",
    "    import ex_color.data.cube_sampler\n",
    "\n",
    "    importlib.reload(ex_color.data.cube_sampler)\n",
    "\n",
    "\n",
    "def generate_color_labels(data: Tensor, vibrancies: Tensor) -> dict[str, Tensor]:\n",
    "    \"\"\"\n",
    "    Generate label probabilities based on RGB values.\n",
    "\n",
    "    Args:\n",
    "        data: Batch of RGB values [B, 3]\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping label names to probabilities str -> [B]\n",
    "    \"\"\"\n",
    "    labels: dict[str, Tensor] = {}\n",
    "\n",
    "    # Proximity to primary colors\n",
    "    r, g, b = data[:, 0], data[:, 1], data[:, 2]\n",
    "    labels['red'] = (r * (1 - g / 2 - b / 2)) ** 10\n",
    "    # labels['green'] = g * (1 - r / 2 - b / 2)\n",
    "    # labels['blue'] = b * (1 - r / 2 - g / 2)\n",
    "    # Proximity to any fully-saturated, fully-bright color\n",
    "    labels['vibrant'] = vibrancies ** 100\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def collate_with_generated_labels(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that generates labels for the samples.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of ((data_tensor,), index_tensor) tuples from TensorDataset.\n",
    "               Note: TensorDataset wraps single tensors in a tuple.\n",
    "\n",
    "    Returns:\n",
    "        A tuple: (collated_data_tensor, collated_labels_tensor)\n",
    "    \"\"\"\n",
    "    # Separate data and indices\n",
    "    # TensorDataset yields tuples like ((data_point_tensor,), index_scalar_tensor)\n",
    "    data_tuple_list = [item[0] for item in batch]  # List of (data_tensor,) tuples\n",
    "    vibrancies = [item[1] for item in batch]\n",
    "\n",
    "    # Collate the data points using the default collate function\n",
    "    # default_collate handles the list of (data_tensor,) tuples correctly\n",
    "    collated_data = default_collate(data_tuple_list)\n",
    "    vibrancies = default_collate(vibrancies)\n",
    "    label_probs = generate_color_labels(collated_data, vibrancies)\n",
    "\n",
    "    # Either return the probabilities directly\n",
    "    return collated_data, label_probs\n",
    "\n",
    "    # TODO: Sample discrete labels stochastically? Perhaps only one label per sample?\n",
    "    # sampled_labels = sample_labels(label_probs)\n",
    "    # return collated_data, sampled_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ex_color.data.cube_sampler import vibrancy\n",
    "\n",
    "hsv_cube = ColorCube.from_hsv(\n",
    "    h=arange_cyclic(step_size=10 / 360),\n",
    "    s=np.linspace(0, 1, 10),\n",
    "    v=np.linspace(0, 1, 10),\n",
    ")\n",
    "hsv_tensor = torch.tensor(hsv_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "vibrancy_tensor = torch.tensor(vibrancy(hsv_cube).flatten(), dtype=torch.float32)\n",
    "hsv_dataset = TensorDataset(hsv_tensor, vibrancy_tensor)\n",
    "\n",
    "# Desaturated and dark colors are over-represented in the cube, so we use a weighted sampler to balance them out\n",
    "hsv_loader = DataLoader(\n",
    "    hsv_dataset,\n",
    "    batch_size=32,\n",
    "    sampler=WeightedRandomSampler(\n",
    "        weights=hsv_cube.bias.flatten().tolist(),\n",
    "        num_samples=len(hsv_dataset),\n",
    "        replacement=True,\n",
    "    ),\n",
    "    collate_fn=collate_with_generated_labels,\n",
    ")\n",
    "\n",
    "rgb_cube = ColorCube.from_rgb(\n",
    "    r=np.linspace(0, 1, 8),\n",
    "    g=np.linspace(0, 1, 8),\n",
    "    b=np.linspace(0, 1, 8),\n",
    ")\n",
    "rgb_tensor = torch.tensor(rgb_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dopesheet: Dopesheet):\n",
    "    \"\"\"Train the model with the given dopesheet and variant.\"\"\"\n",
    "    log.info('Training')\n",
    "    recorder = ModelRecorder()\n",
    "    metrics_recorder = MetricsRecorder()\n",
    "\n",
    "    # seed = 0\n",
    "    # set_deterministic_mode(seed)\n",
    "\n",
    "    model = ColorMLP()\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log.info(f'Model initialized with {total_params:,} trainable parameters.')\n",
    "\n",
    "    event_handlers = EventHandlers()\n",
    "    event_handlers.pre_step.add_handler('pre-step', recorder)\n",
    "    event_handlers.step_metrics.add_handler('step-metrics', metrics_recorder)\n",
    "\n",
    "    plotter = PhasePlotter(rgb_tensor, dim_pairs=[(0, 1), (0, 2), (0, 3)], interval=200)\n",
    "    event_handlers.pre_step.add_handler('pre-step', plotter)\n",
    "\n",
    "    regularizers = [\n",
    "        RegularizerConfig(\n",
    "            name='reg-polar',\n",
    "            criterion=Anchor(torch.tensor([1, 0, 0, 0], dtype=torch.float32, device=hsv_tensor.device)),\n",
    "            label_affinities={'red': 1.0},\n",
    "        ),\n",
    "        RegularizerConfig(\n",
    "            name='reg-separate',\n",
    "            # criterion=Separate(scale=1000),\n",
    "            criterion=Separate(bias=1, scale=1_000, power=0.5),\n",
    "            label_affinities=None,\n",
    "        ),\n",
    "        RegularizerConfig(\n",
    "            name='reg-planar',\n",
    "            criterion=planarity,\n",
    "            label_affinities={'vibrant': 1.0},\n",
    "        ),\n",
    "        RegularizerConfig(\n",
    "            name='reg-norm-v',\n",
    "            criterion=unitary,\n",
    "            label_affinities={'vibrant': 1.0},\n",
    "        ),\n",
    "        RegularizerConfig(\n",
    "            name='reg-norm',\n",
    "            criterion=unitary,\n",
    "            label_affinities=None,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    train_color_model(\n",
    "        model,\n",
    "        hsv_loader,\n",
    "        rgb_tensor,\n",
    "        dopesheet,\n",
    "        # loss_criterion=objective(nn.MSELoss(reduction='none')),  # No reduction; allows per-sample loss weights\n",
    "        loss_criterion=objective(nn.MSELoss()),\n",
    "        regularizers=regularizers,\n",
    "        event_handlers=event_handlers,\n",
    "    )\n",
    "\n",
    "    return recorder, metrics_recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 6480.4 no.1.7:Training\n",
      "I 6480.4 no.1.7:Model initialized with 263 trainable parameters.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Training Steps</b>: 100.0% [<b>10001</b>/10001] [<b>00:49</b>/<00:00, 200.57 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        \n",
       "            <div style=\"\n",
       "                display: grid;\n",
       "                grid-template-columns: repeat(9, minmax(80px, 1fr));\n",
       "                gap: 5px 0px;\n",
       "                width: 100%;\n",
       "                margin-top: 10px;\n",
       "                font-size: 0.85em;\n",
       "            \"><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">PHASE</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">lr</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">loss</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">recon</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-polar</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-separate</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-planar</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-norm-v</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-norm</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">Train</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.003000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0115</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0005</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.1365</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.1790</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.1044</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0032</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0363</div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"large-assets/ex-1.7-color-phase-history.png?v=bcL9PAEWq5yBMIQ4BcqhFoB9wzPwXqmuqudiNpM-m_I\" alt=\"Visualizations of latent space at the end of each curriculum phase.\" style=\"max-width: 70rem;\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 6530.3 no.1.7:Training finished!\n"
     ]
    }
   ],
   "source": [
    "recorder, metrics = train(dopesheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models trained fairly well! There are some differences, but they look like they have similar characteristics. Surprisingly, the smooth variant seemed to have a _noisier_ (i.e. worse) latent space at the end of the _All hues_ phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space evolution analysis\n",
    "\n",
    "Let's visualize how the latent spaces evolved over time. Like Ex 1.5, we'll use the `ModelRecorder`'s history to load the model state at each recorded step and evaluate the latent positions for a fixed set of input colors (the full RGB grid). This gives us a sequence of latent space snapshots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Evaluating latents</b>: 100.0% [<b>10001</b>/10001] [<b>00:08</b>/<00:00, 1190.81 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def eval_latent_history(\n",
    "    recorder: ModelRecorder,\n",
    "    rgb_tensor: Tensor,\n",
    "):\n",
    "    \"\"\"Evaluate the latent space for each step in the recorder's history.\"\"\"\n",
    "    # Create a new model instance\n",
    "    from utils.progress import RichProgress\n",
    "\n",
    "    model = ColorMLP()\n",
    "\n",
    "    latent_history: list[tuple[int, np.ndarray]] = []\n",
    "    # Iterate over the recorded history\n",
    "    for step, state_dict in RichProgress(recorder.history, description='Evaluating latents'):\n",
    "        # Load the model state dict\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get the latents for the RGB tensor\n",
    "            _, latents = model(rgb_tensor.to(next(model.parameters()).device))\n",
    "            latents = latents.cpu().numpy()\n",
    "            latent_history.append((step, latents))\n",
    "    return latent_history\n",
    "\n",
    "\n",
    "latent_history = eval_latent_history(recorder, rgb_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation of latent space\n",
    "\n",
    "This final visualization combines multiple views into a single animation:\n",
    "\n",
    "- **Latent space:** Shows the 2D projection (Dims 0 vs 1) of the latent embeddings for the full RGB color grid, colored by their true RGB values. We can see the color wheel forming.\n",
    "- **Hyperparameters:** Replots the parameter schedule from the dopesheet, with a vertical line indicating the current step in the animation.\n",
    "- **Training metrics:** Plots the total loss and the contribution of each individual loss/regularization term (on a log scale), again with a vertical line for the current step.\n",
    "\n",
    "_(Note: A variable stride is used for sampling frames to focus on periods of rapid change.)_\n",
    "\n",
    "The smooth training run is shown on the left, and the stepped run on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smooth_latents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 209\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Use smooth_latents for stride calculation\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     current_step = \u001b[43msmooth_latents\u001b[49m[\u001b[38;5;28mround\u001b[39m(last_sampled_index)][\u001b[32m0\u001b[39m]\n\u001b[32m    210\u001b[39m     stride = get_stride(current_step)\n\u001b[32m    211\u001b[39m     next_index = last_sampled_index + stride\n",
      "\u001b[31mNameError\u001b[39m: name 'smooth_latents' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import imageio_ffmpeg\n",
    "from matplotlib import rcParams\n",
    "import pandas as pd\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from mini.temporal.dopesheet import RESERVED_COLS\n",
    "from utils.progress import RichProgress\n",
    "\n",
    "# TODO: remove forced reload\n",
    "import importlib\n",
    "import mini.temporal.vis\n",
    "importlib.reload(mini.temporal.vis)\n",
    "from mini.temporal.vis import group_properties_by_scale, plot_timeline\n",
    "\n",
    "rcParams['animation.ffmpeg_path'] = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "\n",
    "\n",
    "def animate_latent_evolution_with_metrics(\n",
    "    # Smooth variant data\n",
    "    smooth_latent_history: list[tuple[int, np.ndarray]],\n",
    "    smooth_metrics_history: list[tuple[int, float, dict[str, float]]],\n",
    "    smooth_param_history_df: pd.DataFrame,\n",
    "    smooth_param_keyframes_df: pd.DataFrame,\n",
    "    # Stepped variant data\n",
    "    stepped_latent_history: list[tuple[int, np.ndarray]],\n",
    "    stepped_metrics_history: list[tuple[int, float, dict[str, float]]],\n",
    "    stepped_param_history_df: pd.DataFrame,\n",
    "    stepped_param_keyframes_df: pd.DataFrame,\n",
    "    # Common data and settings\n",
    "    colors: np.ndarray,\n",
    "    dim_pair: tuple[int, int] = (0, 1),\n",
    "    interval=1 / 30,\n",
    "):\n",
    "    \"\"\"Create a side-by-side animation of latent space evolution, hyperparameters, and metrics.\"\"\"\n",
    "    plt.style.use('dark_background')\n",
    "    # Aim for 16:9 aspect ratio, give latent plots more height\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    # Use the height ratios from your latest version\n",
    "    gs = GridSpec(3, 2, height_ratios=[5, 1, 1], width_ratios=[1, 1], hspace=0, wspace=0.02)\n",
    "\n",
    "    # --- Create Axes ---\n",
    "    # Latent plots (Top row) - No sharing needed initially\n",
    "    ax_latent_s = fig.add_subplot(gs[0, 0])\n",
    "    ax_latent_t = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    # Parameter plots (Middle row) - Share x-axis with metrics plot BELOW\n",
    "    ax_params_s = fig.add_subplot(gs[1, 0])\n",
    "    ax_params_t = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    # Metrics plots (Bottom row) - Share x-axis with parameter plot ABOVE\n",
    "    ax_metrics_s = fig.add_subplot(gs[2, 0], sharex=ax_params_s)\n",
    "    ax_metrics_t = fig.add_subplot(gs[2, 1], sharex=ax_params_t)\n",
    "\n",
    "\n",
    "    fig.patch.set_facecolor('#333')\n",
    "    all_axes = [ax_latent_s, ax_params_s, ax_metrics_s, ax_latent_t, ax_params_t, ax_metrics_t]\n",
    "    for ax in all_axes:\n",
    "        ax.patch.set_facecolor('#222')\n",
    "\n",
    "    latent_lim = 1.1\n",
    "\n",
    "    # --- Setup Smooth Plots (Left Column) ---\n",
    "    step_s, current_latents_s = smooth_latent_history[0]\n",
    "    ax_latent_s.set_xlim(-latent_lim, latent_lim)\n",
    "    ax_latent_s.set_ylim(-latent_lim, latent_lim)\n",
    "    ax_latent_s.set_aspect('equal', adjustable='datalim')\n",
    "    # ax_latent_s.set_xlabel(f'Dim {dim_pair[0]}') # Set X label for latent plot\n",
    "    ax_latent_s.tick_params(axis='x', labelleft=False) # Hide x labels\n",
    "    plt.setp(ax_latent_s.get_xticklabels(), visible=False)\n",
    "    # ax_latent_s.set_ylabel(f'Dim {dim_pair[1]}')\n",
    "    ax_latent_s.set_ylabel('Latent space')\n",
    "    ax_latent_s.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_latent_s.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_latent_s.add_patch(Circle((0, 0), 1, fill=False, linestyle='--', color='gray', alpha=0.3))\n",
    "    scatter_s = ax_latent_s.scatter(\n",
    "        current_latents_s[:, dim_pair[0]], current_latents_s[:, dim_pair[1]], c=colors, s=150, alpha=0.7\n",
    "    )\n",
    "    title_latent_s = ax_latent_s.set_title('placeholder') # Title set in update()\n",
    "    # No need to hide x-ticks here anymore\n",
    "\n",
    "    param_props_s = smooth_param_keyframes_df.columns.difference(list(RESERVED_COLS)).tolist()\n",
    "    param_groups_s = group_properties_by_scale(smooth_param_keyframes_df[param_props_s])\n",
    "    # Pass show_legend=False, show_phase_labels=False as you did\n",
    "    plot_timeline(smooth_param_history_df, smooth_param_keyframes_df, [param_groups_s[0]], ax=ax_params_s, show_legend=False, show_phase_labels=False, line_styles=line_styles)\n",
    "    param_vline_s = ax_params_s.axvline(step_s, color='white', linestyle='--', lw=1)\n",
    "    ax_params_s.set_ylabel('Param value', fontsize='x-small')\n",
    "    ax_params_s.set_xlabel('') # Remove xlabel, it will be on the plot below\n",
    "    # Hide x-tick labels because they are shared with the plot below\n",
    "    plt.setp(ax_params_s.get_xticklabels(), visible=False)\n",
    "\n",
    "    metrics_steps_s = [h[0] for h in smooth_metrics_history]\n",
    "    total_losses_s = [h[1] for h in smooth_metrics_history]\n",
    "    loss_components_s = {k: [h[2].get(k, np.nan) for h in smooth_metrics_history] for k in smooth_metrics_history[0][2].keys()}\n",
    "    ax_metrics_s.plot(metrics_steps_s, total_losses_s, label='Total Loss', lw=latent_lim)\n",
    "    for name, values in loss_components_s.items():\n",
    "        ax_metrics_s.plot(metrics_steps_s, values, label=name, lw=1, alpha=0.8)\n",
    "    ax_metrics_s.set_xlabel('Step') # Set X label for the bottom plot\n",
    "    ax_metrics_s.set_ylabel('Loss (log)', fontsize='x-small')\n",
    "    ax_metrics_s.set_yscale('log')\n",
    "    ax_metrics_s.set_ylim(bottom=1e-6)\n",
    "    metrics_vline_s = ax_metrics_s.axvline(step_s, color='white', linestyle='--', lw=1)\n",
    "\n",
    "    # --- Setup Stepped Plots (Right Column) ---\n",
    "    step_t, current_latents_t = stepped_latent_history[0]\n",
    "    ax_latent_t.set_xlim(-latent_lim, latent_lim)\n",
    "    ax_latent_t.set_ylim(-latent_lim, latent_lim)\n",
    "    ax_latent_t.set_aspect('equal', adjustable='datalim')\n",
    "    # ax_latent_t.set_xlabel(f'Dim {dim_pair[0]}') # Set X label for latent plot\n",
    "    ax_latent_t.tick_params(axis='x', labelleft=False) # Hide x labels\n",
    "    plt.setp(ax_latent_t.get_xticklabels(), visible=False)\n",
    "    ax_latent_t.tick_params(axis='y', labelleft=False) # Hide y labels\n",
    "    ax_latent_t.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_latent_t.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_latent_t.add_patch(Circle((0, 0), 1, fill=False, linestyle='--', color='gray', alpha=0.3))\n",
    "    scatter_t = ax_latent_t.scatter(\n",
    "        current_latents_t[:, dim_pair[0]], current_latents_t[:, dim_pair[1]], c=colors, s=150, alpha=0.7\n",
    "    )\n",
    "    title_latent_t = ax_latent_t.set_title('placeholder') # Title set in update()\n",
    "    # No need to hide x-ticks here anymore\n",
    "\n",
    "    param_props_t = stepped_param_keyframes_df.columns.difference(list(RESERVED_COLS)).tolist()\n",
    "    param_groups_t = group_properties_by_scale(stepped_param_keyframes_df[param_props_t])\n",
    "    # Pass show_legend=False, show_phase_labels=False as you did\n",
    "    plot_timeline(stepped_param_history_df, stepped_param_keyframes_df, [param_groups_t[0]], ax=ax_params_t, show_legend=False, show_phase_labels=False, line_styles=line_styles)\n",
    "    param_vline_t = ax_params_t.axvline(step_t, color='white', linestyle='--', lw=1)\n",
    "    ax_params_t.set_ylabel('')  # Y label only on left\n",
    "    ax_params_t.set_xlabel('') # Remove xlabel, it will be on the plot below\n",
    "    # Hide x-tick labels because they are shared with the plot below\n",
    "    plt.setp(ax_params_t.get_xticklabels(), visible=False)\n",
    "    ax_params_t.tick_params(axis='y', labelleft=False) # Hide y labels\n",
    "\n",
    "    metrics_steps_t = [h[0] for h in stepped_metrics_history]\n",
    "    total_losses_t = [h[1] for h in stepped_metrics_history]\n",
    "    loss_components_t = {k: [h[2].get(k, np.nan) for h in stepped_metrics_history] for k in stepped_metrics_history[0][2].keys()}\n",
    "    ax_metrics_t.plot(metrics_steps_t, total_losses_t, label='Total Loss', lw=1.5)\n",
    "    for name, values in loss_components_t.items():\n",
    "        ax_metrics_t.plot(metrics_steps_t, values, label=name, lw=1, alpha=0.8)\n",
    "    ax_metrics_t.set_xlabel('Step') # Set X label for the bottom plot\n",
    "    ax_metrics_t.set_yscale('log')\n",
    "    ax_metrics_t.set_ylim(bottom=1e-6)\n",
    "    ax_metrics_t.tick_params(axis='y', labelleft=False) # Hide y labels\n",
    "    metrics_vline_t = ax_metrics_t.axvline(step_t, color='white', linestyle='--', lw=1)\n",
    "\n",
    "    # --- Set common X limits ---\n",
    "    # Only set xlim for the timeline plots (params and metrics)\n",
    "    max_step = max(smooth_param_history_df['STEP'].max(), stepped_param_history_df['STEP'].max())\n",
    "    for ax in [ax_params_s, ax_metrics_s, ax_params_t, ax_metrics_t]:\n",
    "         ax.set_xlim(left=0, right=max_step)\n",
    "\n",
    "    # fig.tight_layout(h_pad=0, w_pad=0.5)  # Adjust padding\n",
    "    fig.subplots_adjust(\n",
    "        left=0.05,    # Smaller left margin\n",
    "        right=0.95,   # Smaller right margin\n",
    "        bottom=0.08,  # Smaller bottom margin (leave room for x-label)\n",
    "        top=0.95,     # Smaller top margin (leave room for titles)\n",
    "        wspace=0.1,   # Adjust space between columns (tweak as needed)\n",
    "        hspace=0.0    # Keep vertical space at 0 (set in GridSpec)\n",
    "    )\n",
    "\n",
    "    def update(frame: int):\n",
    "        # ... (update logic remains the same) ...\n",
    "        # Assume smooth and stepped histories have the same length and aligned steps after sampling\n",
    "        smooth_step, current_latents_s = smooth_latent_history[frame]\n",
    "        stepped_step, current_latents_t = stepped_latent_history[frame]\n",
    "        # Use the smooth step for titles and lines, assuming they are aligned\n",
    "        current_step = smooth_step\n",
    "\n",
    "        # Update smooth plots\n",
    "        scatter_s.set_offsets(current_latents_s[:, dim_pair])\n",
    "        title_latent_s.set_text(f'Smooth curriculum (step {current_step})') # Use current_step\n",
    "        param_vline_s.set_xdata([current_step])\n",
    "        metrics_vline_s.set_xdata([current_step])\n",
    "\n",
    "        # Update stepped plots\n",
    "        scatter_t.set_offsets(current_latents_t[:, dim_pair])\n",
    "        title_latent_t.set_text(f'Stepped curriculum (step {current_step})') # Use current_step\n",
    "        param_vline_t.set_xdata([current_step])\n",
    "        metrics_vline_t.set_xdata([current_step])\n",
    "\n",
    "        return (\n",
    "            scatter_s, title_latent_s, param_vline_s, metrics_vline_s,\n",
    "            scatter_t, title_latent_t, param_vline_t, metrics_vline_t,\n",
    "        )\n",
    "\n",
    "    # Use the length of the (potentially strided) latent_history for frames\n",
    "    # Assuming both histories have the same length after sampling\n",
    "    num_frames = len(smooth_latent_history)\n",
    "    ani = animation.FuncAnimation(fig, update, frames=num_frames, interval=interval * 1000, blit=True)\n",
    "    return fig, ani\n",
    "\n",
    "\n",
    "# --- Variable Stride Logic ---\n",
    "def get_stride(step: int):\n",
    "    import math\n",
    "\n",
    "    a = 7.9236\n",
    "    b = 0.0005\n",
    "    # Ensure stride is at least 1\n",
    "    return max(1.0, a * math.log(b * step + 1) + 1)\n",
    "\n",
    "\n",
    "# Apply stride logic based on smooth history (assuming stepped is similar)\n",
    "sampled_indices = [0]\n",
    "last_sampled_index = 0\n",
    "# Use smooth_latents for stride calculation\n",
    "while True:\n",
    "    current_step = smooth_latents[round(last_sampled_index)][0]\n",
    "    stride = get_stride(current_step)\n",
    "    next_index = last_sampled_index + stride\n",
    "    # Ensure indices stay within bounds for *both* histories\n",
    "    if round(next_index) >= len(smooth_latents) or round(next_index) >= len(stepped_latents):\n",
    "        break\n",
    "    sampled_indices.append(round(next_index))\n",
    "    last_sampled_index = next_index\n",
    "\n",
    "# Ensure the last frame is included if missed\n",
    "if sampled_indices[-1] < len(smooth_latents) - 1:\n",
    "     sampled_indices.append(len(smooth_latents) - 1)\n",
    "\n",
    "# sampled_indices = sampled_indices[:200]  # Limit the number of samples during development\n",
    "\n",
    "# Sample both latent histories using the same indices\n",
    "sampled_smooth_latents = [smooth_latents[i] for i in sampled_indices]\n",
    "sampled_stepped_latents = [stepped_latents[i] for i in sampled_indices]\n",
    "\n",
    "# --- End Variable Stride Logic ---\n",
    "\n",
    "# Filter metrics history to align with the *new* sampled latent history steps\n",
    "# Use steps from the sampled smooth history (assuming alignment)\n",
    "sampled_steps_set = {step for step, _ in sampled_smooth_latents}\n",
    "filtered_smooth_metrics = [h for h in smooth_metrics.history if h[0] in sampled_steps_set]\n",
    "filtered_stepped_metrics = [h for h in stepped_metrics.history if h[0] in sampled_steps_set]\n",
    "\n",
    "# Realize timelines for both dopesheets\n",
    "smooth_timeline = Timeline(smooth_dopesheet)\n",
    "smooth_history_df = realize_timeline(smooth_timeline)\n",
    "smooth_keyframes_df = smooth_dopesheet.as_df()\n",
    "\n",
    "stepped_timeline = Timeline(stepped_dopesheet)\n",
    "stepped_history_df = realize_timeline(stepped_timeline)\n",
    "stepped_keyframes_df = stepped_dopesheet.as_df()\n",
    "\n",
    "\n",
    "# --- Call the updated animation function ---\n",
    "fig, ani = animate_latent_evolution_with_metrics(\n",
    "    # Smooth\n",
    "    smooth_latent_history=sampled_smooth_latents,\n",
    "    smooth_metrics_history=filtered_smooth_metrics,\n",
    "    smooth_param_history_df=smooth_history_df,\n",
    "    smooth_param_keyframes_df=smooth_keyframes_df,\n",
    "    # Stepped\n",
    "    stepped_latent_history=sampled_stepped_latents,\n",
    "    stepped_metrics_history=filtered_stepped_metrics,\n",
    "    stepped_param_history_df=stepped_history_df,\n",
    "    stepped_param_keyframes_df=stepped_keyframes_df,\n",
    "    # Common\n",
    "    colors=rgb_tensor.cpu().numpy(),\n",
    "    dim_pair=(0, 1),\n",
    ")\n",
    "\n",
    "# --- Save the video ---\n",
    "video_file = f'large-assets/ex-{nbid}-latent-evolution-comparison.mp4' # Updated filename\n",
    "num_frames_to_render = len(sampled_smooth_latents) # Base on sampled length\n",
    "with RichProgress(total=num_frames_to_render, description='Rendering comparison video') as pbar:\n",
    "    ani.save(\n",
    "        video_file,\n",
    "        fps=30,\n",
    "        extra_args=['-vcodec', 'libx264'],\n",
    "        progress_callback=lambda i, n: pbar.update(1),\n",
    "    )\n",
    "plt.close(fig)\n",
    "\n",
    "# --- Display the video ---\n",
    "import secrets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "cache_buster = secrets.token_urlsafe()\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        f\"\"\"\n",
    "        <video width=\"960\" height=\"540\" controls loop>\n",
    "            <source src=\"{video_file}?v={cache_buster}\" type=\"video/mp4\">\n",
    "            Your browser does not support the video tag.\n",
    "        </video>\n",
    "        \"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Qualitatively, we observe that:\n",
    "- The Smooth variant seems noisier overall: it's more jittery in general, and becomes more misshapen during the _All hues_ phase. This might be due to the specific values of the hyperparameters, e.g. maybe the normalization loss was too high.\n",
    "- The Stepped variant does indeed show loss spikes at the start of each phase, while the Smooth varint does not — as predicted! However, the spikes don't seem to cause any problem; perhaps they were fully mitigated by the LR warmup.\n",
    "- Even though the data are introduced to each variant differently (in chunks to the Stepped variant, and gradually to the Smooth variant), the effect is almost identical. This is particularly apparent at the start of the _Full color space_ phase: the Stepped variant bulges suddenly at the start of the phase, while the Smooth variant bulges a little later and somewhat less violently — but both end up in almost the exact same shape.\n",
    "\n",
    "Perhaps the dynamics and final latent space could be improved for the Smooth curriculum by reducing the learning rate at times when the parameters are changing a lot — but since per-phase LR schedules are already common in curriculum learning, using them _in addition_ to smooth parameter changes may not have much benefit. On the other hand, we note that the smooth curriculum was easier to specify than the stepped one, purely because it had fewer phases and fewer keyframes.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Our hypothesis seems to have been wrong: smooth parameter changes _don't_ appear to improve training dynamics compared to a traditional curriculum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
