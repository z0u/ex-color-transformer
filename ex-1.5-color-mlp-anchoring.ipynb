{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superposition by embedding bottleneck\n",
    "\n",
    "This is an experiment to squash higher-dimensional data into a lower-dimensional embedding space. We'll start with color: RGB values (3 dimensions) ranging from 0..1. If we compress them into a 2D embedding, we should expect to see superposition, with directions interpretable as they would be in a classic color wheel: three primary color directions (RGB) spaced 120° apart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from utils.logging import SimpleLoggingConfig\n",
    "\n",
    "logging_config = SimpleLoggingConfig().info('notebook', 'utils', 'mini', 'ex_color')\n",
    "logging_config.apply()\n",
    "\n",
    "# This is the logger for this notebook\n",
    "log = logging.getLogger('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MLP with bottleneck\n",
    "\n",
    "We'll train a simple 2-layer MLP with low-dimensional bottleneck to map RGB values (inputs like [1.0, 0.0, 0.0] for red) to RGB values. This would force colors into an embedding space where we expect to see the superposition effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "E = 4\n",
    "\n",
    "class ColorMLP(nn.Module):\n",
    "    def __init__(self, normalize_bottleneck=False):\n",
    "        super().__init__()\n",
    "        # RGB input (3D) → hidden layer → bottleneck → hidden layer → RGB output\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, E),  # Our critical bottleneck!\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(E, 16),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(16, 16),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(16, 3),\n",
    "            nn.Sigmoid(),  # Keep RGB values in [0,1]\n",
    "        )\n",
    "\n",
    "        self.normalize = normalize_bottleneck\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Get our bottleneck representation\n",
    "        bottleneck = self.encoder(x)\n",
    "\n",
    "        # Optionally normalize to unit vectors (like nGPT)\n",
    "        if self.normalize:\n",
    "            norm = torch.norm(bottleneck, dim=1, keepdim=True)\n",
    "            bottleneck = bottleneck / (norm + 1e-8)  # Avoid division by zero\n",
    "\n",
    "        # Decode back to RGB\n",
    "        output = self.decoder(bottleneck)\n",
    "        return output, bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training machinery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Protocol, runtime_checkable\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.timeline import State\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceResult:\n",
    "    outputs: Tensor\n",
    "    latents: Tensor\n",
    "\n",
    "    def detach(self):\n",
    "        return InferenceResult(self.outputs.detach(), self.latents.detach())\n",
    "\n",
    "    def clone(self):\n",
    "        return InferenceResult(self.outputs.clone(), self.latents.clone())\n",
    "\n",
    "    def cpu(self):\n",
    "        return InferenceResult(self.outputs.cpu(), self.latents.cpu())\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class LossCriterion(Protocol):\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor: ...\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class SpecialLossCriterion(LossCriterion, Protocol):\n",
    "    def forward(self, model: ColorMLP, data: Tensor) -> InferenceResult | None: ...\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class Event:\n",
    "    name: str\n",
    "    step: int\n",
    "    model: ColorMLP\n",
    "    timeline_state: State\n",
    "    optimizer: optim.Optimizer\n",
    "\n",
    "\n",
    "@dataclass(eq=False, frozen=True)\n",
    "class PhaseEndEvent(Event):\n",
    "    validation_data: Tensor\n",
    "    inference_result: InferenceResult\n",
    "\n",
    "\n",
    "class EventHandler[T](Protocol):\n",
    "    def __call__(self, event: T) -> None: ...\n",
    "\n",
    "\n",
    "class EventBinding[T]:\n",
    "    \"\"\"A class to bind events to handlers.\"\"\"\n",
    "\n",
    "    def __init__(self, event_name: str):\n",
    "        self.event_name = event_name\n",
    "        self.handlers: list[tuple[str, EventHandler[T]]] = []\n",
    "\n",
    "    def add_handler(self, event_name: str, handler: EventHandler[T]) -> None:\n",
    "        self.handlers.append((event_name, handler))\n",
    "\n",
    "    def emit(self, event_name: str, event: T) -> None:\n",
    "        for name, handler in self.handlers:\n",
    "            if name == event_name:\n",
    "                handler(event)\n",
    "\n",
    "\n",
    "class EventHandlers:\n",
    "    \"\"\"A simple event system to allow for custom callbacks.\"\"\"\n",
    "\n",
    "    phase_start: EventBinding[Event]\n",
    "    pre_step: EventBinding[Event]\n",
    "    action: EventBinding[Event]\n",
    "    phase_end: EventBinding[PhaseEndEvent]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.phase_start = EventBinding[Event]('phase-start')\n",
    "        self.pre_step = EventBinding[Event]('pre-step')\n",
    "        self.action = EventBinding[Event]('action')\n",
    "        self.phase_end = EventBinding[PhaseEndEvent]('phase-end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "from utils.progress import RichProgress\n",
    "\n",
    "\n",
    "def reiterate[T](it: Iterable[T]) -> Iterator[T]:\n",
    "    \"\"\"\n",
    "    Iterates over an iterable indefinitely.\n",
    "\n",
    "    When the iterable is exhausted, it starts over from the beginning. Unlike\n",
    "    `itertools.cycle`, yielded values are not cached — so each iteration may be\n",
    "    different.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        yield from it\n",
    "\n",
    "\n",
    "def train_color_model(  # noqa: C901\n",
    "    model: ColorMLP,\n",
    "    datasets: dict[str, tuple[DataLoader, Tensor]],\n",
    "    dopesheet: Dopesheet,\n",
    "    loss_criteria: dict[str, LossCriterion | SpecialLossCriterion],\n",
    "    event_handlers: EventHandlers | None = None,\n",
    "):\n",
    "    if event_handlers is None:\n",
    "        event_handlers = EventHandlers()\n",
    "\n",
    "    # --- Validate inputs ---\n",
    "    # Check if all phases in dopesheet have corresponding data\n",
    "    dopesheet_phases = dopesheet.phases\n",
    "    missing_data = dopesheet_phases - set(datasets.keys())\n",
    "    if missing_data:\n",
    "        raise ValueError(f'Missing data for dopesheet phases: {missing_data}')\n",
    "\n",
    "    # Check if 'lr' is defined in the dopesheet properties\n",
    "    if 'lr' not in dopesheet.props:\n",
    "        raise ValueError(\"Dopesheet must define the 'lr' property column.\")\n",
    "    # --- End Validation ---\n",
    "\n",
    "    timeline = Timeline(dopesheet)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    data_iterators = {\n",
    "        phase_name: iter(reiterate(dataloader))  #\n",
    "        for phase_name, (dataloader, _) in datasets.items()\n",
    "    }\n",
    "\n",
    "    total_steps = len(timeline)\n",
    "\n",
    "    with RichProgress(total=total_steps, description='Training Steps') as pbar:\n",
    "        for step in range(total_steps):\n",
    "            # Get state *before* advancing timeline for this step's processing\n",
    "            current_state = timeline.state\n",
    "            current_phase_name = current_state.phase\n",
    "\n",
    "            # Assuming TensorDataset yields a tuple with one element\n",
    "            (batch,) = next(data_iterators[current_phase_name])\n",
    "\n",
    "            # --- Event Handling ---\n",
    "            event_template = {\n",
    "                'step': step,\n",
    "                'model': model,\n",
    "                'timeline_state': current_state,\n",
    "                'optimizer': optimizer,\n",
    "            }\n",
    "\n",
    "            if current_state.is_phase_start:\n",
    "                event = Event(name=f'phase-start:{current_phase_name}', **event_template)\n",
    "                event_handlers.phase_start.emit(event.name, event)\n",
    "                event_handlers.phase_start.emit('phase-start', event)\n",
    "\n",
    "            for action in current_state.actions:\n",
    "                event = Event(name=f'action:{action}', **event_template)\n",
    "                event_handlers.action.emit(event.name, event)\n",
    "                event_handlers.action.emit('action', event)\n",
    "\n",
    "            event = Event(name='pre-step', **event_template)\n",
    "            event_handlers.pre_step.emit('pre-step', event)\n",
    "\n",
    "            # --- Training Step ---\n",
    "            # ... (get data, update LR, zero grad, forward pass, calculate loss, backward, step) ...\n",
    "\n",
    "            current_lr = current_state.props['lr']\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs, latents = model(batch.to(device))\n",
    "            current_results = InferenceResult(outputs, latents)\n",
    "\n",
    "            total_loss = torch.tensor(0.0, device=device)\n",
    "            losses_dict: dict[str, float] = {}\n",
    "            for name, criterion in loss_criteria.items():\n",
    "                weight = current_state.props.get(name, 0.0)\n",
    "                if weight == 0:\n",
    "                    continue\n",
    "\n",
    "                if isinstance(criterion, SpecialLossCriterion):\n",
    "                    # Special criteria might run on their own data (like Anchor)\n",
    "                    # or potentially use the current batch (depends on implementation).\n",
    "                    # The forward method gets the model and the *current batch*\n",
    "                    special_results = criterion.forward(model, batch)\n",
    "                    if special_results is None:\n",
    "                        continue\n",
    "                    term_loss = criterion(batch, special_results)\n",
    "                else:\n",
    "                    term_loss = criterion(batch, current_results)\n",
    "\n",
    "                total_loss += term_loss * weight\n",
    "                losses_dict[name] = term_loss.item()\n",
    "\n",
    "            if total_loss > 0:\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "            # --- End Training Step ---\n",
    "\n",
    "            # --- Post-Step Event Handling ---\n",
    "            if current_state.is_phase_end:\n",
    "                # Trigger phase-end for the *current* phase\n",
    "                _, validation_data = datasets[current_phase_name]\n",
    "                with torch.no_grad():\n",
    "                    val_outputs, val_latents = model(validation_data.to(device))\n",
    "                event = PhaseEndEvent(\n",
    "                    name=f'phase-end:{current_phase_name}',\n",
    "                    **event_template,\n",
    "                    validation_data=validation_data,\n",
    "                    inference_result=InferenceResult(val_outputs, val_latents),\n",
    "                )\n",
    "                event_handlers.phase_end.emit(event.name, event)\n",
    "                event_handlers.phase_end.emit('phase-end', event)\n",
    "            # --- End Event Handling ---\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(\n",
    "                metrics={\n",
    "                    'PHASE': current_phase_name,\n",
    "                    'lr': f'{current_lr:.6f}',\n",
    "                    'loss': f'{total_loss.item():.4f}',\n",
    "                    **{name: f'{lt:.4f}' for name, lt in losses_dict.items()},\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Advance timeline *after* processing the current step\n",
    "            if step < total_steps:  # Avoid stepping past the end\n",
    "                timeline.step()\n",
    "\n",
    "    log.info('Training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from torch import Tensor\n",
    "\n",
    "from utils.nb import save_fig\n",
    "\n",
    "\n",
    "class PhasePlotter:\n",
    "    \"\"\"Event handler to plot latent space at the end of each phase.\"\"\"\n",
    "\n",
    "    def __init__(self, dim_pairs: list[tuple[int, int]] | None = None):\n",
    "        from utils.nb import displayer\n",
    "\n",
    "        # Store (phase_name, end_step, data, result) - data comes from event now\n",
    "        self.history: list[tuple[str, int, Tensor, InferenceResult]] = []\n",
    "        self.display = displayer()\n",
    "        self.dim_pairs = dim_pairs or [(0, 1), (0, 2)]\n",
    "\n",
    "    # Expect PhaseEndEvent specifically\n",
    "    def __call__(self, event: PhaseEndEvent):\n",
    "        \"\"\"Handle phase-end events.\"\"\"\n",
    "        if not isinstance(event, PhaseEndEvent):\n",
    "            raise TypeError(f'Expected PhaseEndEvent, got {type(event)}')\n",
    "\n",
    "        # TODO: Don't assume device = CPU\n",
    "        # TODO: Split this class so that the event handler is separate from the plotting, and so the plotting can happen locally with @run.hither\n",
    "        phase_name = event.timeline_state.phase\n",
    "        end_step = event.step\n",
    "        phase_dataset = event.validation_data\n",
    "        inference_result = event.inference_result\n",
    "\n",
    "        log.info(f'Plotting end of phase: {phase_name} at step {end_step} using provided results.')\n",
    "\n",
    "        # Append to history\n",
    "        self.history.append((phase_name, end_step, phase_dataset.cpu(), inference_result.cpu()))\n",
    "\n",
    "        # Plotting logic remains the same as it already expected CPU tensors\n",
    "        fig = self._plot_phase_history()\n",
    "        self.display(\n",
    "            HTML(\n",
    "                save_fig(\n",
    "                    fig,\n",
    "                    'assets/ex-1.5-color-phase-history.png',\n",
    "                    alt_text='Visualizations of latent space at the end of each curriculum phase.',\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _plot_phase_history(self):\n",
    "        num_phases = len(self.history)\n",
    "        plt.style.use('dark_background')\n",
    "        if num_phases == 0:\n",
    "            fig, ax = plt.subplots()\n",
    "            fig.set_facecolor('#333')\n",
    "            ax.set_facecolor('#222')\n",
    "            ax.text(0.5, 0.5, 'Waiting...', ha='center', va='center')\n",
    "            return fig\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            num_phases, len(self.dim_pairs), figsize=(5 * len(self.dim_pairs), 5 * num_phases), squeeze=False\n",
    "        )\n",
    "        fig.set_facecolor('#333')\n",
    "\n",
    "        for row_idx, (phase_name, end_step, data, res) in enumerate(self.history):\n",
    "            _latents = res.latents.numpy()\n",
    "            _colors = data.numpy()\n",
    "\n",
    "            for col_idx, (dim1, dim2) in enumerate(self.dim_pairs):\n",
    "                ax = axes[row_idx, col_idx]\n",
    "                ax.set_facecolor('#222')\n",
    "                ax.scatter(_latents[:, dim1], _latents[:, dim2], c=_colors, s=50, alpha=0.7)\n",
    "                if col_idx == 0:\n",
    "                    ax.set_ylabel(\n",
    "                        f'Phase: {phase_name}\\n(End Step: {end_step})',\n",
    "                        fontsize='medium',\n",
    "                        rotation=0,\n",
    "                        labelpad=40,\n",
    "                        verticalalignment='center',\n",
    "                    )\n",
    "                if row_idx == 0:\n",
    "                    ax.set_title(f'Dims {dim1} vs {dim2}')\n",
    "                ax.set_xlabel(f'Dim {dim1}')\n",
    "                if col_idx != 0:\n",
    "                    ax.set_ylabel(f'Dim {dim2}')\n",
    "                else:\n",
    "                    ax.yaxis.set_label_coords(-0.2, 0.5)\n",
    "                    ax.set_yticks([])\n",
    "                ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "                ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "                ax.add_patch(Circle((0, 0), 1, fill=False, linestyle='--', color='gray', alpha=0.3))\n",
    "                ax.set_aspect('equal')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum\n",
    "\n",
    "In order to get a somewhat predictable orientation in latent space, we'll use curriculum learning:\n",
    "\n",
    "| Phase | Training Data              | Regularization                       |\n",
    "| ----- | -------------------------- | ------------------------------------ |\n",
    "| 1     | Primary & secondary colors | Separation, normalization, planarity |\n",
    "| 2     | All pure hues              | Anchor, normalization                |\n",
    "| 3     | Full color space           | Anchor, normalization                |\n",
    "\n",
    "With this curriculum, we expect to see a strong, well-formed color wheel with just the hues. As we add the darker tones, it should more-or-less retain its shape, but the darker tones should appear as smaller rings further from the _hue_ plane. Viewed from the side, it should start to resemble a dome.\n",
    "\n",
    "The constraints are applied as regularization terms. These ultimately form part of the loss function, but are calculated from the latent HSV embeddings rather than the reconstructed RGB outputs.\n",
    "\n",
    "- Separation: Penalizes close points, to encourage regular spacing of the primary and secondary colors.\n",
    "- Normalization: This uses the L2 norm in two ways:\n",
    "  - First calculates the L2 norm of each latent vector\n",
    "  - Then applies mean squared error between these norms and 1.0\n",
    "  - This encourages all latent vectors to lie on a unit sphere.\n",
    "- Planar constriant: This is also L2-based:\n",
    "  - Takes the squared values (i.e. L2 norm, not absolute values) of the remaining dimensions\n",
    "  - This pushes the later dimensions toward zero, encouraging a planar representation in the first two dimensions.\n",
    "- Anchor: Penalizes _specific_ points if they move after a certain step in training. This is used to keep the primary and secondary colors in place while the curriculum continues.\n",
    "\n",
    "To reduce disruption to the embeddings learnt in the earlier phases, we'll use smoothly-varying parameters for the learning rate and the weights of each regularization term. The full curriculum is defined as keyframes in [a dopesheet](ex-1.5-dopesheet.csv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Parameter schedule\n",
       "|   STEP | PHASE               | ACTION   |      lr |   loss-recon |   reg-separate |   reg-planar |   reg-norm |   reg-anchor |   data-fraction |\n",
       "|-------:|:--------------------|:---------|--------:|-------------:|---------------:|-------------:|-----------:|-------------:|----------------:|\n",
       "|      0 | Primary & secondary |          |         |          1   |            0   |          0.2 |            |              |                 |\n",
       "|   1200 |                     |          |         |          0.8 |            0.3 |              |            |              |                 |\n",
       "|   1800 |                     |          |         |              |                |          0.4 |       0.1  |              |                 |\n",
       "|   3000 | All hues            | anchor   |         |              |            0   |              |       0.25 |         0    |            0    |\n",
       "|   3350 |                     |          |   0.01  |              |                |              |            |              |                 |\n",
       "|   6500 |                     |          |         |          0.8 |                |          0   |            |              |                 |\n",
       "|   8600 |                     |          |         |              |                |              |            |         0.3  |                 |\n",
       "|  10000 | Full color space    |          |         |              |                |              |            |              |            0.25 |\n",
       "|  10500 |                     |          |         |              |                |              |            |              |                 |\n",
       "|  13000 |                     |          |         |          1   |                |              |            |         0.1  |            1    |\n",
       "|  20000 |                     |          |   0.001 |              |                |              |            |         0.75 |                 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4.8 ut.nb:   Figure saved: 'assets/ex-1.5-color-timeline.png'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"assets/ex-1.5-color-timeline.png?v=199868\" alt=\"Line chart showing the hyperparameter schedule over time.\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from mini.temporal.vis import group_properties_by_scale, plot_timeline, realize_timeline\n",
    "from mini.temporal.dopesheet import Dopesheet\n",
    "from mini.temporal.timeline import Timeline\n",
    "\n",
    "dopesheet = Dopesheet.from_csv('ex-1.5-dopesheet.csv')\n",
    "display(\n",
    "    Markdown(f\"\"\"\n",
    "## Parameter schedule\n",
    "{dopesheet.to_markdown()}\n",
    "\"\"\")\n",
    ")\n",
    "\n",
    "timeline = Timeline(dopesheet)\n",
    "history_df = realize_timeline(timeline)\n",
    "keyframes_df = dopesheet.as_df()\n",
    "\n",
    "groups = group_properties_by_scale(keyframes_df[dopesheet.props])\n",
    "fig, ax = plot_timeline(history_df, keyframes_df, groups)\n",
    "display(\n",
    "    HTML(\n",
    "        save_fig(\n",
    "            fig,\n",
    "            'assets/ex-1.5-color-timeline.png',\n",
    "            alt_text='Line chart showing the hyperparameter schedule over time.',\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import linalg as LA\n",
    "\n",
    "from ex_color.data.color_cube import ColorCube\n",
    "from ex_color.data.cyclic import arange_cyclic\n",
    "\n",
    "\n",
    "def objective(fn):\n",
    "    \"\"\"Adapt loss function to look like a regularizer\"\"\"\n",
    "\n",
    "    def wrapper(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        return fn(data, res.outputs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def unitary(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to have unit norm (vectors of length 1)\"\"\"\n",
    "    norms = LA.vector_norm(res.latents, dim=-1)\n",
    "    return torch.mean((norms - 1.0) ** 2)\n",
    "\n",
    "\n",
    "def planarity(data: Tensor, res: InferenceResult) -> Tensor:\n",
    "    \"\"\"Regularize latents to be planar in the first two channels (so zero in other channels)\"\"\"\n",
    "    return torch.mean(res.latents[:, 2:] ** 2)\n",
    "\n",
    "\n",
    "class Separate(LossCriterion):\n",
    "    def __init__(self, channels: tuple[int, ...] = (0, 1)):\n",
    "        self.channels = channels\n",
    "\n",
    "    def __call__(self, data: Tensor, res: InferenceResult) -> Tensor:\n",
    "        \"\"\"Regularize latents to be separated from each other in first two channels\"\"\"\n",
    "        # Get pairwise differences in the first two dimensions\n",
    "        points = res.latents[:, self.channels]  # [B, C]\n",
    "        diffs = points.unsqueeze(1) - points.unsqueeze(0)  # [B, B, C]\n",
    "\n",
    "        # Calculate squared distances\n",
    "        sq_dists = torch.sum(diffs**2, dim=-1)  # [B, B]\n",
    "\n",
    "        # Remove self-distances (diagonal)\n",
    "        mask = 1.0 - torch.eye(sq_dists.shape[0], device=sq_dists.device)\n",
    "        masked_sq_dists = sq_dists * mask\n",
    "\n",
    "        # Encourage separation by minimizing inverse distances (stronger repulsion between close points)\n",
    "        epsilon = 1e-6  # Prevent division by zero\n",
    "        return torch.mean(1.0 / (masked_sq_dists + epsilon))\n",
    "\n",
    "\n",
    "class Anchor(SpecialLossCriterion):\n",
    "    \"\"\"Regularize latents to be close to their position in the reference phase\"\"\"\n",
    "\n",
    "    ref_data: Tensor\n",
    "    _ref_latents: Tensor | None = None\n",
    "\n",
    "    def __init__(self, ref_data: Tensor):\n",
    "        self.ref_data = ref_data\n",
    "        self._ref_latents = None\n",
    "        log.info(f'Anchor initialized with reference data shape: {ref_data.shape}')\n",
    "\n",
    "    def forward(self, model: ColorMLP, data: Tensor) -> InferenceResult | None:\n",
    "        \"\"\"Run the *stored reference data* through the *current* model.\"\"\"\n",
    "        # Note: The 'data' argument passed by the training loop for SpecialLossCriterion\n",
    "        # is the *current training batch*, which we IGNORE here.\n",
    "        # We only care about running our stored _ref_data through the model.\n",
    "        device = next(model.parameters()).device\n",
    "        ref_data = self.ref_data.to(device)\n",
    "\n",
    "        outputs, latents = model(ref_data)\n",
    "        return InferenceResult(outputs, latents)\n",
    "\n",
    "    def __call__(self, data: Tensor, special: InferenceResult) -> Tensor:\n",
    "        \"\"\"Calculates loss between current model's latents (for ref_data) and the stored reference latents.\"\"\"\n",
    "        if self._ref_latents is None:\n",
    "            # This means on_anchor hasn't been called yet, so the anchor loss is zero.\n",
    "            # This prevents errors during the very first phase before the anchor point is set.\n",
    "            log.debug('Anchor.__call__ invoked before reference latents captured. Returning zero loss.')\n",
    "            return torch.tensor(0.0, device=special.latents.device)\n",
    "        ref_latents = self._ref_latents.to(special.latents.device)\n",
    "        return torch.mean((special.latents - ref_latents) ** 2)\n",
    "\n",
    "    def on_anchor(self, event: Event):\n",
    "        # Called when the 'anchor' event is triggered\n",
    "        log.info(f'Capturing anchor latents via Anchor.on_anchor at step {event.step}')\n",
    "\n",
    "        device = next(event.model.parameters()).device\n",
    "        ref_data = self.ref_data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, latents = event.model(ref_data)\n",
    "        self._ref_latents = latents.detach().cpu()\n",
    "        log.info(f'Anchor state captured internally. Ref data: {ref_data.shape}, Ref latents: {latents.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 5.2 no:      Model initialized with 263 trainable parameters.\n",
      "I 5.2 no:      Anchor initialized with reference data shape: torch.Size([6, 3])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Training Steps</b>: 100.0% [<b>20001</b>/20001] [<b>00:55</b>/<00:00, 361.23 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        \n",
       "            <div style=\"\n",
       "                display: grid;\n",
       "                grid-template-columns: repeat(6, minmax(80px, 1fr));\n",
       "                gap: 5px 0px;\n",
       "                width: 100%;\n",
       "                margin-top: 10px;\n",
       "                font-size: 0.85em;\n",
       "            \"><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">PHASE</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">lr</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">loss</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">loss-recon</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-norm</div><div style=\"\n",
       "                    font-weight: bold;\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">reg-anchor</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">Full color space</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.001000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0002</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0001</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0000</div><div style=\"\n",
       "                    padding-block: 2px;\n",
       "                    padding-inline: 10px;\n",
       "                    text-align: left;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    white-space: nowrap;\n",
       "                \">0.0000</div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 10.3 no:     Plotting end of phase: Primary & secondary at step 2999 using provided results.\n",
      "I 10.5 ut.nb:  Figure saved: 'assets/ex-1.5-color-phase-history.png'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"assets/ex-1.5-color-phase-history.png?v=143459\" alt=\"Visualizations of latent space at the end of each curriculum phase.\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 10.5 no:     Capturing anchor latents via Anchor.on_anchor at step 3000\n",
      "I 10.5 no:     Anchor state captured internally. Ref data: torch.Size([6, 3]), Ref latents: torch.Size([6, 4])\n",
      "I 30.9 no:     Plotting end of phase: All hues at step 9999 using provided results.\n",
      "I 31.4 ut.nb:  Figure saved: 'assets/ex-1.5-color-phase-history.png'\n",
      "I 60.4 no:     Plotting end of phase: Full color space at step 20000 using provided results.\n",
      "I 61.3 ut.nb:  Figure saved: 'assets/ex-1.5-color-phase-history.png'\n",
      "I 61.3 no:     Training finished!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ex_color.data.cube_sampler import DynamicWeightedRandomBatchSampler, vibrancy\n",
    "from ex_color.data.filters import levels\n",
    "\n",
    "\n",
    "primary_cube = ColorCube.from_hsv(h=arange_cyclic(step_size=1 / 6), s=np.ones(1), v=np.ones(1))\n",
    "primary_tensor = torch.tensor(primary_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "primary_dataset = TensorDataset(primary_tensor)\n",
    "primary_loader = DataLoader(primary_dataset, batch_size=len(primary_tensor))\n",
    "\n",
    "full_cube = ColorCube.from_hsv(\n",
    "    h=arange_cyclic(step_size=10 / 360),\n",
    "    s=np.linspace(0, 1, 10),\n",
    "    v=np.linspace(0, 1, 10),\n",
    ")\n",
    "full_tensor = torch.tensor(full_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "full_dataset = TensorDataset(full_tensor)\n",
    "full_sampler = DynamicWeightedRandomBatchSampler(\n",
    "    bias=full_cube.bias.flatten(),\n",
    "    batch_size=256,\n",
    "    steps_per_epoch=100,\n",
    ")\n",
    "vibrancy_weights = vibrancy(full_cube).flatten()\n",
    "full_loader = DataLoader(full_dataset, batch_sampler=full_sampler)\n",
    "\n",
    "rgb_cube = ColorCube.from_rgb(\n",
    "    r=np.linspace(0, 1, 10),\n",
    "    g=np.linspace(0, 1, 10),\n",
    "    b=np.linspace(0, 1, 10),\n",
    ")\n",
    "rgb_tensor = torch.tensor(rgb_cube.rgb_grid.reshape(-1, 3), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def update_sampler_weights(event: Event):\n",
    "    frac = event.timeline_state.props['data-fraction']\n",
    "    # When the fraction is near zero, in_low is almost 1 — which means \"scale everything down to 0 except for 1\"\n",
    "    # When the fraction is 0.5, in_low and out_low are both 0, so the weights are unchanged\n",
    "    # When the fraction is 1, in_low is 0 and out_low is 1, so the weights are all scaled to 1\n",
    "    in_low = np.interp(frac, [0, 0.5], [0.99, 0])\n",
    "    out_low = np.interp(frac, [0.5, 1], [0, 1])\n",
    "    full_sampler.weights = levels(vibrancy_weights, in_low=in_low, out_low=out_low)\n",
    "\n",
    "\n",
    "class ModelRecorder(EventHandler):\n",
    "    \"\"\"Event handler to record model parameters.\"\"\"\n",
    "\n",
    "    history: list[tuple[int, dict[str, Tensor]]]\n",
    "    \"\"\"A list of tuples (step, state_dict) where state_dict is a copy of the model's state dict.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, event: Event):\n",
    "        # It's crucial to get a *copy* of the state dict and move it to the CPU\n",
    "        # so we don't hold onto GPU memory or track gradients unnecessarily.\n",
    "        model_state = {k: v.cpu().clone() for k, v in event.model.state_dict().items()}\n",
    "        self.history.append((event.step, model_state))\n",
    "        log.debug(f'Recorded model state at step {event.step}')\n",
    "\n",
    "\n",
    "recorder = ModelRecorder()\n",
    "\n",
    "# Phase -> (train loader, validation tensor)\n",
    "datasets: dict[str, tuple[DataLoader, Tensor]] = {\n",
    "    'Primary & secondary': (primary_loader, primary_tensor),\n",
    "    'All hues': (full_loader, rgb_tensor),\n",
    "    'Full color space': (full_loader, rgb_tensor),\n",
    "}\n",
    "\n",
    "model = ColorMLP(normalize_bottleneck=False)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "log.info(f'Model initialized with {total_params:,} trainable parameters.')\n",
    "\n",
    "event_handlers = EventHandlers()\n",
    "event_handlers.pre_step.add_handler('pre-step', recorder)\n",
    "event_handlers.pre_step.add_handler('pre-step', update_sampler_weights)\n",
    "\n",
    "plotter = PhasePlotter(dim_pairs=[(0, 1), (0, 2), (0, 3)])\n",
    "event_handlers.phase_end.add_handler('phase-end', plotter)\n",
    "\n",
    "reg_anchor = Anchor(ref_data=primary_tensor)\n",
    "event_handlers.action.add_handler('action:anchor', reg_anchor.on_anchor)\n",
    "\n",
    "history = train_color_model(\n",
    "    model,\n",
    "    datasets,\n",
    "    dopesheet,\n",
    "    loss_criteria={\n",
    "        'loss-recon': objective(nn.MSELoss()),\n",
    "        'reg-separate': Separate((0, 1)),\n",
    "        'reg-planar': planarity,\n",
    "        'reg-norm': unitary,\n",
    "        'reg-anchor': reg_anchor,\n",
    "    },\n",
    "    event_handlers=event_handlers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Evaluating latents</b>: 100.0% [<b>20001</b>/20001] [<b>00:35</b>/<00:00, 559.09 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def eval_latent_history(\n",
    "    recorder: ModelRecorder,\n",
    "    rgb_tensor: Tensor,\n",
    "):\n",
    "    \"\"\"Evaluate the latent space for each step in the recorder's history.\"\"\"\n",
    "    # Create a new model instance\n",
    "    from utils.progress import RichProgress\n",
    "    model = ColorMLP(normalize_bottleneck=False)\n",
    "\n",
    "    latent_history: list[tuple[int, np.ndarray]] = []\n",
    "    # Iterate over the recorded history\n",
    "    for step, state_dict in RichProgress(recorder.history, description='Evaluating latents'):\n",
    "        # Load the model state dict\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get the latents for the RGB tensor\n",
    "            _, latents = model(rgb_tensor.to(next(model.parameters()).device))\n",
    "            latents = latents.cpu().numpy()\n",
    "            latent_history.append((step, latents))\n",
    "    return latent_history\n",
    "\n",
    "\n",
    "latent_history = eval_latent_history(recorder, rgb_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 100%; padding: 5px 0; font-family: monospace;\">\n",
       "            <!-- Progress bar container -->\n",
       "            <div style=\"position: relative; height: calc(1em * 5/3); width: 100%;\">\n",
       "                <!-- Triangle indicator -->\n",
       "                <div style=\"position: absolute; bottom: -4px; left: calc(100.0% - 4px);\">\n",
       "                    <div style=\"\n",
       "                        width: 0;\n",
       "                        height: 0;\n",
       "                        border-left: 4px solid transparent;\n",
       "                        border-right: 4px solid transparent;\n",
       "                        border-bottom: 4px solid currentColor;\n",
       "                    \"></div>\n",
       "                </div>\n",
       "                <!-- Progress bar -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    height: 100%;\n",
       "                    width: 100.0%;\n",
       "                    background-color: color(from currentColor srgb r g b / 0.1);\n",
       "                    border-bottom: 1px solid currentColor;\n",
       "                \"></div>\n",
       "                <!-- Text overlay -->\n",
       "                <div style=\"\n",
       "                    position: absolute;\n",
       "                    top: 0;\n",
       "                    left: 0;\n",
       "                    width: 100%;\n",
       "                    height: 100%;\n",
       "                    text-align: center;\n",
       "                    line-height: calc(1em * 5/3);\n",
       "                    font-size: 0.9em;\n",
       "                    white-space: nowrap;\n",
       "                    overflow: hidden;\n",
       "                    text-overflow: ellipsis;\n",
       "                    border-bottom: 1px dashed color(from currentColor srgb r g b / 0.5);\n",
       "                \">\n",
       "                    <b>Rendering video</b>: 100.0% [<b>2000</b>/2000] [<b>01:23</b>/<00:00, 23.83 it/s]\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <video width=\"600\" height=\"600\" controls loop>\n",
       "        <source src=\"assets/ex-1.5-latent-evolution.mp4?v=163438\" type=\"video/mp4\">\n",
       "        Your browser does not support the video tag.\n",
       "    </video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import imageio_ffmpeg\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from utils.progress import RichProgress\n",
    "\n",
    "rcParams['animation.ffmpeg_path'] = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "\n",
    "\n",
    "def animate_latent_evolution(\n",
    "    history: list[tuple[int, np.ndarray]],\n",
    "    colors: np.ndarray,\n",
    "    dim_pair: tuple[int, int] = (0, 1),\n",
    "    interval=1 / 30,\n",
    "):\n",
    "    \"\"\"Create an animation of the latent space evolution.\"\"\"\n",
    "    plt.style.use('dark_background')\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    fig.patch.set_facecolor('#333')\n",
    "    ax.patch.set_facecolor('#222')\n",
    "\n",
    "    model = ColorMLP(normalize_bottleneck=False)\n",
    "    data = rgb_tensor.to(next(model.parameters()).device)\n",
    "    colors = data.cpu().numpy()\n",
    "\n",
    "    step, latents = history[0]\n",
    "    scatter = ax.scatter(latents[:, dim_pair[0]], latents[:, dim_pair[1]], c=colors, s=50, alpha=0.7)\n",
    "    title = ax.set_title(f'Step {step}')\n",
    "\n",
    "    def update(frame: int):\n",
    "        frame += 1  # first frame already drawn\n",
    "        step, latents = history[frame]\n",
    "        scatter.set_offsets(latents[:, dim_pair])\n",
    "        title.set_text(f'Step {step}')\n",
    "        return (scatter, title)\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(history) - 1, interval=interval * 1000, blit=True)\n",
    "    return fig, ani\n",
    "\n",
    "\n",
    "stride = 10\n",
    "fig, ani = animate_latent_evolution(latent_history[::stride], rgb_tensor.cpu().numpy(), dim_pair=(0, 1))\n",
    "\n",
    "video_file = 'assets/ex-1.5-latent-evolution.mp4'\n",
    "with RichProgress(total=len(latent_history) // stride, description='Rendering video') as pbar:\n",
    "    ani.save(\n",
    "        video_file,\n",
    "        fps=30,\n",
    "        extra_args=['-vcodec', 'libx264'],\n",
    "        progress_callback=lambda i, n: pbar.update(1),\n",
    "    )\n",
    "plt.close(fig)\n",
    "\n",
    "from random import randint\n",
    "from IPython.display import HTML\n",
    "\n",
    "cache_buster = randint(1, 1_000_000)\n",
    "\n",
    "HTML(\n",
    "    f\"\"\"\n",
    "    <video width=\"600\" height=\"600\" controls loop>\n",
    "        <source src=\"{video_file}?v={cache_buster:d}\" type=\"video/mp4\">\n",
    "        Your browser does not support the video tag.\n",
    "    </video>\n",
    "    \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
